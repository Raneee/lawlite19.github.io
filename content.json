[{"title":"致火影","date":"2017-03-24T13:57:12.000Z","path":"2017/03/24/致火影/","text":"致火影 ——只要有树叶飞舞的地方，火就会燃烧。 昨天就知道火影动漫也完结了，但是没有马上去看，想抽个正式点的时间。 漫画是700集完结，当时动漫到700的时候就有个打算想写点东西记录一下，但是没有动手。今天准备看时还在思考，看完一些回忆涌上，果断提笔。 初三的暑假，当时是在补课，一位小伙伴有火影的光盘，当时就借来看看。记得每天最多能看几十集，当时光盘里面应该是有300集左右。 暑假结束，步入高一，当时并不知道有漫画（毕竟高一才有的QQ），还在军训，班级里面有同学买的关于火影的海报，那时漫画里讲到鼬双重间谍的身份，以及多么爱他的弟弟佐助。后来有了个诺亚舟学习机（当然现在还在），有时周末就去网吧下载火影动漫看。现在来说有的一集看了不止10遍，当然我周围的小伙伴也有一块看的。 高一结束分班，我后面一排的一位小伙伴也看火影，每次周日下午回校，他都和我讨论，当时讨论的还有死神（死神、柯南都有看，但火影是我唯一看的完整的动漫（不算死亡笔记这种比较短的动漫））。 大三的时候火影漫画700完结（700之后的5话是番外），当时写了一段话，但没有发出来。当动漫700之后几集的片尾曲唱到：さようなら（再见）的时候，些许感慨，之后看的时候的片头曲和片尾曲很少跳过。 还记得岸本齐史（AB大叔）有说过，刚开始画火影的时候他还没有结婚，就像鸣人一样希望得到别人的注意，后来结婚，漫画里的鸣人也渐渐的有了朋友。最后定格在鸣人雏田结婚。 16岁到24岁，谢谢鸣人，谢谢火影! ——思念你的人所在的地方就是你的归宿！ 2017年3月24日","tags":[{"name":"随笔","slug":"随笔","permalink":"http://lawlite.com/tags/随笔/"}]},{"title":"Python机器学习","date":"2017-01-08T15:01:58.000Z","path":"2017/01/08/Python机器学习/","text":"机器学习算法Python实现说明 github地址：https://github.com/lawlite19/MachineLearning_Python 目录 机器学习算法Python实现 一、线性回归 1、代价函数 2、梯度下降算法 3、均值归一化 4、最终运行结果 5、使用scikit-learn库中的线性模型实现 二、逻辑回归 1、代价函数 2、梯度 3、正则化 4、S型函数（即） 5、映射为多项式 6、使用的优化方法 7、运行结果 8、使用scikit-learn库中的逻辑回归模型实现 * [逻辑回归_手写数字识别_OneVsAll](#逻辑回归_手写数字识别_onevsall) * [1、随机显示100个数字](#1-随机显示100个数字) * [2、OneVsAll](#2-onevsall) * [3、手写数字识别](#3-手写数字识别) * [4、预测](#4-预测) * [5、运行结果](#5-运行结果) * [6、使用scikit-learn库中的逻辑回归模型实现](#6-使用scikit-learn库中的逻辑回归模型实现) * [三、BP神经网络](#三-bp神经网络) * [1、神经网络model](#1-神经网络model) * [2、代价函数](#2-代价函数) * [3、正则化](#3-正则化) * [4、反向传播BP](#4-反向传播bp) * [5、BP可以求梯度的原因](#5-bp可以求梯度的原因) * [6、梯度检查](#6-梯度检查) * [7、权重的随机初始化](#7-权重的随机初始化) * [8、预测](#8-预测) * [9、输出结果](#9-输出结果) * [四、SVM支持向量机](#四-svm支持向量机) * [1、代价函数](#1-代价函数) * [2、Large Margin](#2-large-margin) * [3、SVM Kernel（核函数）](#3-svm-kernel核函数) * [4、使用中的模型代码](#4-使用中的模型代码) * [5、运行结果](#5-运行结果) * [五、K-Means聚类算法](#五-k-means聚类算法) * [1、聚类过程](#1-聚类过程) * [2、目标函数](#2-目标函数) * [3、聚类中心的选择](#3-聚类中心的选择) * [4、聚类个数K的选择](#4-聚类个数k的选择) * [5、应用——图片压缩](#5-应用图片压缩) * [6、使用scikit-learn库中的线性模型实现聚类](#6-使用scikit-learn库中的线性模型实现聚类) * [7、运行结果](#7-运行结果) * [六、PCA主成分分析（降维）](#六-pca主成分分析降维) * [1、用处](#1-用处) * [2、2D--&gt;1D，nD--&gt;kD](#2-2d-1dnd-kd) * [3、主成分分析PCA与线性回归的区别](#3-主成分分析pca与线性回归的区别) * [4、PCA降维过程](#4-pca降维过程) * [5、数据恢复](#5-数据恢复) * [6、主成分个数的选择（即要降的维度）](#6-主成分个数的选择即要降的维度) * [7、使用建议](#7-使用建议) * [8、运行结果](#8-运行结果) * [9、使用scikit-learn库中的PCA实现降维](#9-使用scikit-learn库中的pca实现降维) * [七、异常检测 Anomaly Detection](#七-异常检测-anomaly-detection) * [1、高斯分布（正态分布）](#1-高斯分布正态分布) * [2、异常检测算法](#2-异常检测算法) * [3、评价的好坏，以及的选取](#3-评价的好坏以及的选取) * [4、选择使用什么样的feature（单元高斯分布）](#4-选择使用什么样的feature单元高斯分布) * [5、多元高斯分布](#5-多元高斯分布) * [6、单元和多元高斯分布特点](#6-单元和多元高斯分布特点) * [7、程序运行结果](#7-程序运行结果) 一、线性回归 全部代码 1、代价函数 其中： 下面就是要求出theta，使代价最小，即代表我们拟合出来的方程距离真实值最近 共有m条数据，其中代表我们要拟合出来的方程到真实值距离的平方，平方的原因是因为可能有负值，正负可能会抵消 前面有系数2的原因是下面求梯度是对每个变量求偏导，2可以消去 实现代码： 1234567# 计算代价函数def computerCost(X,y,theta): m = len(y) J = 0 J = (np.transpose(X*theta-y))*(X*theta-y)/(2*m) #计算代价J return J 注意这里的X是真实数据前加了一列1，因为有theta(0) 2、梯度下降算法 代价函数对求偏导得到： 所以对theta的更新可以写为： 其中为学习速率，控制梯度下降的速度，一般取0.01,0.03,0.1,0.3….. 为什么梯度下降可以逐步减小代价函数 假设函数f(x) 泰勒展开：f(x+△x)=f(x)+f&#39;(x)*△x+o(△x) 令：△x=-α*f&#39;(x) ,即负梯度方向乘以一个很小的步长α 将△x代入泰勒展开式中：f(x+x)=f(x)-α*[f&#39;(x)]²+o(△x) 可以看出，α是取得很小的正数，[f&#39;(x)]²也是正数，所以可以得出：f(x+△x)&lt;=f(x) 所以沿着负梯度放下，函数在减小，多维情况一样。 实现代码1234567891011121314151617# 梯度下降算法def gradientDescent(X,y,theta,alpha,num_iters): m = len(y) n = len(theta) temp = np.matrix(np.zeros((n,num_iters))) # 暂存每次迭代计算的theta，转化为矩阵形式 J_history = np.zeros((num_iters,1)) #记录每次迭代计算的代价值 for i in range(num_iters): # 遍历迭代次数 h = np.dot(X,theta) # 计算内积，matrix可以直接乘 temp[:,i] = theta - ((alpha/m)*(np.dot(np.transpose(X),h-y))) #梯度的计算 theta = temp[:,i] J_history[i] = computerCost(X,y,theta) #调用计算代价函数 print &apos;.&apos;, return theta,J_history 3、均值归一化 目的是使数据都缩放到一个范围内，便于使用梯度下降算法 其中 为所有此feture数据的平均值 可以是最大值-最小值，也可以是这个feature对应的数据的标准差 实现代码： 12345678910111213# 归一化featuredef featureNormaliza(X): X_norm = np.array(X) #将X转化为numpy数组对象，才可以进行矩阵的运算 #定义所需变量 mu = np.zeros((1,X.shape[1])) sigma = np.zeros((1,X.shape[1])) mu = np.mean(X_norm,0) # 求每一列的平均值（0指定为列，1代表行） sigma = np.std(X_norm,0) # 求每一列的标准差 for i in range(X.shape[1]): # 遍历列 X_norm[:,i] = (X_norm[:,i]-mu[i])/sigma[i] # 归一化 return X_norm,mu,sigma 注意预测的时候也需要均值归一化数据 4、最终运行结果 代价随迭代次数的变化 5、使用scikit-learn库中的线性模型实现 导入包 12from sklearn import linear_modelfrom sklearn.preprocessing import StandardScaler #引入缩放的包 归一化 12345# 归一化操作scaler = StandardScaler() scaler.fit(X)x_train = scaler.transform(X)x_test = scaler.transform(np.array([1650,3])) 线性模型拟合 12345 # 线性模型拟合 model = linear_model.LinearRegression() model.fit(x_train, y)``` - 预测 #预测结果 result = model.predict(x_test) 123456789101112131415161718192021222324252627282930313233343536------------------- ## 二、[逻辑回归](/LogisticRegression)- [全部代码](/LogisticRegression/LogisticRegression.py)### 1、代价函数- ![\\left\\&#123; \\begin&#123;gathered&#125; J(\\theta ) = \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;\\cos t(&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;),&#123;y^&#123;(i)&#125;&#125;)&#125; \\hfill \\\\ \\cos t(&#123;h_\\theta &#125;(x),y) = \\left\\&#123; &#123;\\begin&#123;array&#125;&#123;c&#125; &#123; - \\log (&#123;h_\\theta &#125;(x))&#125; \\\\ &#123; - \\log (1 - &#123;h_\\theta &#125;(x))&#125; \\end&#123;array&#125; \\begin&#123;array&#125;&#123;c&#125; &#123;y = 1&#125; \\\\ &#123;y = 0&#125; \\end&#123;array&#125; &#125; \\right. \\hfill \\\\ \\end&#123;gathered&#125; \\right.](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%5Cbegin%7Bgathered%7D%20J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%2C%7By%5E%7B%28i%29%7D%7D%29%7D%20%5Chfill%20%5C%5C%20%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%7By%20%3D%201%7D%20%5C%5C%20%7By%20%3D%200%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%20%5Chfill%20%5C%5C%20%5Cend%7Bgathered%7D%20%5Cright.)- 可以综合起来为：![J(\\theta ) = - \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\log (&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\\log (1 - &#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)其中：![&#123;h_\\theta &#125;(x) = \\frac&#123;1&#125;11 + &#123;e^&#123; - x&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20x%7D%7D%7D%7D)- 为什么不用线性回归的代价函数表示，因为线性回归的代价函数可能是非凸的，对于分类问题，使用梯度下降很难得到最小值，上面的代价函数是凸函数- ![&#123; - \\log (&#123;h_\\theta &#125;(x))&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下，即`y=1`时：![enter description here][2]可以看出，当![1h_\\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`1`，`y=1`,与预测值一致，此时付出的代价`cost`趋于`0`，若![1h_\\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`0`，`y=1`,此时的代价`cost`值非常大，我们最终的目的是最小化代价值- 同理![&#123; - \\log (1 - &#123;h_\\theta &#125;(x))&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下（`y=0`）： ![enter description here][3]### 2、梯度- 同样对代价函数求偏导：![\\frac1\\partial J(\\theta )&#125;&#125;1\\partial &#123;\\theta _j&#125;&#125;&#125; = \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;[(&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) - &#123;y^&#123;(i)&#125;&#125;)x_j^&#123;(i)&#125;]&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20) 可以看出与线性回归的偏导数一致- 推到过程![enter description here][4]### 3、正则化- 目的是为了防止过拟合- 在代价函数中加上一项![J(\\theta ) = - \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\log (&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\\log (1 - &#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)] + \\frac&#123;\\lambda &#125;12m&#125;&#125;\\sum\\limits_&#123;j = 1&#125;^n &#123;\\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)- 注意j是重1开始的，因为theta(0)为一个常数项，X中最前面一列会加上1列1，所以乘积还是theta(0),feature没有关系，没有必要正则化- 正则化后的代价： 代价函数def costFunction(initial_theta,X,y,inital_lambda): m = len(y) J = 0 h = sigmoid(np.dot(X,initial_theta)) # 计算h(z) theta1 = initial_theta.copy() # 因为正则化j=1从1开始，不包含0，所以复制一份，前theta(0)值为0 theta1[0] = 0 temp = np.dot(np.transpose(theta1),theta1) J = (-np.dot(np.transpose(y),np.log(h))-np.dot(np.transpose(1-y),np.log(1-h))+temp*inital_lambda/2)/m # 正则化的代价方程 return J 1- 正则化后的代价的梯度 计算梯度def gradient(initial_theta,X,y,inital_lambda): m = len(y) grad = np.zeros((initial_theta.shape[0])) h = sigmoid(np.dot(X,initial_theta))# 计算h(z) theta1 = initial_theta.copy() theta1[0] = 0 grad = np.dot(np.transpose(X),h-y)/m+inital_lambda/m*theta1 #正则化的梯度 return grad 123### 4、S型函数（即![1h_\\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)）- 实现代码： S型函数def sigmoid(z): h = np.zeros((len(z),1)) # 初始化，与z的长度一置 h = 1.0/(1.0+np.exp(-z)) return h 12345### 5、映射为多项式- 因为数据的feture可能很少，导致偏差大，所以创造出一些feture结合- eg:映射为2次方的形式:![1 + &#123;x_1&#125; + &#123;x_2&#125; + x_1^2 + &#123;x_1&#125;&#123;x_2&#125; + x_2^2](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)- 实现代码： 映射为多项式def mapFeature(X1,X2): degree = 3; # 映射的最高次方 out = np.ones((X1.shape[0],1)) # 映射后的结果数组（取代X） ‘’’ 这里以degree=2为例，映射为1,x1,x2,x1^2,x1,x2,x2^2 ‘’’ for i in np.arange(1,degree+1): for j in range(i+1): temp = X1(i-j)*(X2j) #矩阵直接乘相当于matlab中的点乘.* out = np.hstack((out, temp.reshape(-1,1))) return out12345678### 6、使用`scipy`的优化方法- 梯度下降使用`scipy`中`optimize`中的`fmin_bfgs`函数- 调用scipy中的优化算法fmin_bfgs（拟牛顿法Broyden-Fletcher-Goldfarb-Shanno - costFunction是自己实现的一个求代价的函数， - initial_theta表示初始化的值, - fprime指定costFunction的梯度 - args是其余测参数，以元组的形式传入，最后会将最小化costFunction的theta返回 result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,y,initial_lambda)) 1234567891011### 7、运行结果- data1决策边界和准确度 ![enter description here][5]![enter description here][6]- data2决策边界和准确度 ![enter description here][7]![enter description here][8]### 8、[使用scikit-learn库中的逻辑回归模型实现](/LogisticRegression/LogisticRegression_scikit-learn.py)- 导入包 from sklearn.linear_model import LogisticRegressionfrom sklearn.preprocessing import StandardScalerfrom sklearn.cross_validation import train_test_splitimport numpy as np1- 划分训练集和测试集 # 划分为训练集和测试集 x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2) 1- 归一化 # 归一化 scaler = StandardScaler() scaler.fit(x_train) x_train = scaler.fit_transform(x_train) x_test = scaler.fit_transform(x_test) 1- 逻辑回归 #逻辑回归 model = LogisticRegression() model.fit(x_train,y_train) 1- 预测 # 预测 predict = model.predict(x_test) right = sum(predict == y_test) predict = np.hstack((predict.reshape(-1,1),y_test.reshape(-1,1))) # 将预测值和真实值放在一块，好观察 print predict print (&apos;测试集准确率：%f%%&apos;%(right*100.0/predict.shape[0])) #计算在测试集上的准确度 12345678910111213-------------## [逻辑回归_手写数字识别_OneVsAll](/LogisticRegression)- [全部代码](/LogisticRegression/LogisticRegression_OneVsAll.py)### 1、随机显示100个数字- 我没有使用scikit-learn中的数据集，像素是20*20px，彩色图如下![enter description here][9]灰度图：![enter description here][10]- 实现代码： 显示100个数字def display_data(imgData): sum = 0 ‘’’ 显示100个数（若是一个一个绘制将会非常慢，可以将要画的数字整理好，放到一个矩阵中，显示这个矩阵即可） - 初始化一个二维数组 - 将每行的数据调整成图像的矩阵，放进二维数组 - 显示即可 &apos;&apos;&apos; pad = 1 display_array = -np.ones((pad+10*(20+pad),pad+10*(20+pad))) for i in range(10): for j in range(10): display_array[pad+i*(20+pad):pad+i*(20+pad)+20,pad+j*(20+pad):pad+j*(20+pad)+20] = (imgData[sum,:].reshape(20,20,order=&quot;F&quot;)) # order=F指定以列优先，在matlab中是这样的，python中需要指定，默认以行 sum += 1 plt.imshow(display_array,cmap=&apos;gray&apos;) #显示灰度图像 plt.axis(&apos;off&apos;) plt.show() 1234567891011121314### 2、OneVsAll- 如何利用逻辑回归解决多分类的问题，OneVsAll就是把当前某一类看成一类，其他所有类别看作一类，这样有成了二分类的问题了- 如下图，把途中的数据分成三类，先把红色的看成一类，把其他的看作另外一类，进行逻辑回归，然后把蓝色的看成一类，其他的再看成一类，以此类推...![enter description here][11]- 可以看出大于2类的情况下，有多少类就要进行多少次的逻辑回归分类### 3、手写数字识别- 共有0-9，10个数字，需要10次分类- 由于**数据集y**给出的是`0,1,2...9`的数字，而进行逻辑回归需要`0/1`的label标记，所以需要对y处理- 说一下数据集，前`500`个是`0`,`500-1000`是`1`,`...`,所以如下图，处理后的`y`，**前500行的第一列是1，其余都是0,500-1000行第二列是1，其余都是0....**![enter description here][12]- 然后调用**梯度下降算法**求解`theta`- 实现代码： 求每个分类的theta，最后返回所有的all_thetadef oneVsAll(X,y,num_labels,Lambda): # 初始化变量 m,n = X.shape all_theta = np.zeros((n+1,num_labels)) # 每一列对应相应分类的theta,共10列 X = np.hstack((np.ones((m,1)),X)) # X前补上一列1的偏置bias class_y = np.zeros((m,num_labels)) # 数据的y对应0-9，需要映射为0/1的关系 initial_theta = np.zeros((n+1,1)) # 初始化一个分类的theta # 映射y for i in range(num_labels): class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值 #np.savetxt(&quot;class_y.csv&quot;, class_y[0:600,:], delimiter=&apos;,&apos;) &apos;&apos;&apos;遍历每个分类，计算对应的theta值&apos;&apos;&apos; for i in range(num_labels): result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,class_y[:,i],Lambda)) # 调用梯度下降的优化方法 all_theta[:,i] = result.reshape(1,-1) # 放入all_theta中 all_theta = np.transpose(all_theta) return all_theta 1234### 4、预测- 之前说过，预测的结果是一个**概率值**，利用学习出来的`theta`代入预测的**S型函数**中，每行的最大值就是是某个数字的最大概率，所在的**列号**就是预测的数字的真实值,因为在分类时，所有为`0`的将`y`映射在第一列，为1的映射在第二列，依次类推- 实现代码： 预测def predict_oneVsAll(all_theta,X): m = X.shape[0] num_labels = all_theta.shape[0] p = np.zeros((m,1)) X = np.hstack((np.ones((m,1)),X)) #在X最前面加一列1 h = sigmoid(np.dot(X,np.transpose(all_theta))) #预测 &apos;&apos;&apos; 返回h中每一行最大值所在的列号 - np.max(h, axis=1)返回h中每一行的最大值（是某个数字的最大概率） - 最后where找到的最大概率所在的列号（列号即是对应的数字） &apos;&apos;&apos; p = np.array(np.where(h[0,:] == np.max(h, axis=1)[0])) for i in np.arange(1, m): t = np.array(np.where(h[i,:] == np.max(h, axis=1)[i])) p = np.vstack((p,t)) return p 1234567### 5、运行结果- 10次分类，在训练集上的准确度： ![enter description here][13]### 6、[使用scikit-learn库中的逻辑回归模型实现](/LogisticRegression/LogisticRegression_OneVsAll_scikit-learn.py)- 1、导入包 from scipy import io as spioimport numpy as npfrom sklearn import svmfrom sklearn.linear_model import LogisticRegression1- 2、加载数据 data = loadmat_data(&quot;data_digits.mat&quot;) X = data[&apos;X&apos;] # 获取X数据，每一行对应一个数字20x20px y = data[&apos;y&apos;] # 这里读取mat文件y的shape=(5000, 1) y = np.ravel(y) # 调用sklearn需要转化成一维的(5000,) 1- 3、拟合模型 model = LogisticRegression() model.fit(X, y) # 拟合 1- 4、预测 predict = model.predict(X) #预测 print u&quot;预测准确度为：%f%%&quot;%np.mean(np.float64(predict == y)*100) 1234567891011121314151617181920212223242526272829303132333435363738394041- 5、输出结果（在训练集上的准确度）![enter description here][14]----------## 三、BP神经网络- [全部代码](/NeuralNetwok/NeuralNetwork.py)### 1、神经网络model- 先介绍个三层的神经网络，如下图所示 - 输入层（input layer）有三个units（![&#123;x_0&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx_0%7D)为补上的bias，通常设为`1`） - ![a_i^&#123;(j)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_i%5E%7B%28j%29%7D)表示第`j`层的第`i`个激励，也称为为单元unit - ![&#123;\\theta ^&#123;(j)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D)为第`j`层到第`j+1`层映射的权重矩阵，就是每条边的权重![enter description here][15]- 所以可以得到： - 隐含层： ![a_1^&#123;(2)&#125; = g(\\theta _&#123;10&#125;^&#123;(1)&#125;&#123;x_0&#125; + \\theta _&#123;11&#125;^&#123;(1)&#125;&#123;x_1&#125; + \\theta _&#123;12&#125;^&#123;(1)&#125;&#123;x_2&#125; + \\theta _&#123;13&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_1%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%281%29%7D%7Bx_3%7D%29) ![a_2^&#123;(2)&#125; = g(\\theta _&#123;20&#125;^&#123;(1)&#125;&#123;x_0&#125; + \\theta _&#123;21&#125;^&#123;(1)&#125;&#123;x_1&#125; + \\theta _&#123;22&#125;^&#123;(1)&#125;&#123;x_2&#125; + \\theta _&#123;23&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_2%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B20%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B21%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B22%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B23%7D%5E%7B%281%29%7D%7Bx_3%7D%29) ![a_3^&#123;(2)&#125; = g(\\theta _&#123;30&#125;^&#123;(1)&#125;&#123;x_0&#125; + \\theta _&#123;31&#125;^&#123;(1)&#125;&#123;x_1&#125; + \\theta _&#123;32&#125;^&#123;(1)&#125;&#123;x_2&#125; + \\theta _&#123;33&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_3%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B30%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B31%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B32%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B33%7D%5E%7B%281%29%7D%7Bx_3%7D%29) - 输出层 ![&#123;h_\\theta &#125;(x) = a_1^&#123;(3)&#125; = g(\\theta _&#123;10&#125;^&#123;(2)&#125;a_0^&#123;(2)&#125; + \\theta _&#123;11&#125;^&#123;(2)&#125;a_1^&#123;(2)&#125; + \\theta _&#123;12&#125;^&#123;(2)&#125;a_2^&#123;(2)&#125; + \\theta _&#123;13&#125;^&#123;(2)&#125;a_3^&#123;(2)&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20a_1%5E%7B%283%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%282%29%7Da_0%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%282%29%7Da_1%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%282%29%7Da_2%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%282%29%7Da_3%5E%7B%282%29%7D%29) 其中，**S型函数**![g(z) = \\frac&#123;1&#125;11 + &#123;e^&#123; - z&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)，也成为**激励函数**- 可以看出![&#123;\\theta ^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%281%29%7D%7D) 为3x4的矩阵，![&#123;\\theta ^&#123;(2)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%282%29%7D%7D)为1x4的矩阵 - ![&#123;\\theta ^&#123;(j)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D) ==》`j+1`的单元数x（`j`层的单元数+1）### 2、代价函数- 假设最后输出的![&#123;h_\\Theta &#125;(x) \\in &#123;R^K&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5CTheta%20%7D%28x%29%20%5Cin%20%7BR%5EK%7D)，即代表输出层有K个单元- ![J(\\Theta ) = - \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;\\sum\\limits_&#123;k = 1&#125;^K &#123;[y_k^&#123;(i)&#125;\\log 1(&#123;h_\\Theta &#125;(&#123;x^&#123;(i)&#125;&#125;))&#125;_k&#125;&#125; &#125; + (1 - y_k^&#123;(i)&#125;)\\log &#123;(1 - &#123;h_\\Theta &#125;(&#123;x^&#123;(i)&#125;&#125;))_k&#125;]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5CTheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Csum%5Climits_%7Bk%20%3D%201%7D%5EK%20%7B%5By_k%5E%7B%28i%29%7D%5Clog%20%7B%7B%28%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29%7D_k%7D%7D%20%7D%20%20%2B%20%281%20-%20y_k%5E%7B%28i%29%7D%29%5Clog%20%7B%281%20-%20%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29_k%7D%5D) 其中，![&#123;(&#123;h_\\Theta &#125;(x))_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%28%7Bh_%5CTheta%20%7D%28x%29%29_i%7D)代表第`i`个单元输出- 与逻辑回归的代价函数![J(\\theta ) = - \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\log (&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\\log (1 - &#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)差不多，就是累加上每个输出（共有K个输出）### 3、正则化- `L`--&gt;所有层的个数- ![&#123;S_l&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7BS_l%7D)--&gt;第`l`层unit的个数- 正则化后的**代价函数**为 ![enter description here][16] - ![\\theta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ctheta%20)共有`L-1`层， - 然后是累加对应每一层的theta矩阵，注意不包含加上偏置项对应的theta(0)- 正则化后的代价函数实现代码： 代价函数def nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda): length = nn_params.shape[0] # theta的中长度 # 还原theta1和theta2 Theta1 = nn_params[0:hidden_layer_size*(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1) Theta2 = nn_params[hidden_layer_size*(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1) # np.savetxt(&quot;Theta1.csv&quot;,Theta1,delimiter=&apos;,&apos;) m = X.shape[0] class_y = np.zeros((m,num_labels)) # 数据的y对应0-9，需要映射为0/1的关系 # 映射y for i in range(num_labels): class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值 &apos;&apos;&apos;去掉theta1和theta2的第一列，因为正则化时从1开始&apos;&apos;&apos; Theta1_colCount = Theta1.shape[1] Theta1_x = Theta1[:,1:Theta1_colCount] Theta2_colCount = Theta2.shape[1] Theta2_x = Theta2[:,1:Theta2_colCount] # 正则化向theta^2 term = np.dot(np.transpose(np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1)))),np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1)))) &apos;&apos;&apos;正向传播,每次需要补上一列1的偏置bias&apos;&apos;&apos; a1 = np.hstack((np.ones((m,1)),X)) z2 = np.dot(a1,np.transpose(Theta1)) a2 = sigmoid(z2) a2 = np.hstack((np.ones((m,1)),a2)) z3 = np.dot(a2,np.transpose(Theta2)) h = sigmoid(z3) &apos;&apos;&apos;代价&apos;&apos;&apos; J = -(np.dot(np.transpose(class_y.reshape(-1,1)),np.log(h.reshape(-1,1)))+np.dot(np.transpose(1-class_y.reshape(-1,1)),np.log(1-h.reshape(-1,1)))-Lambda*term/2)/m return np.ravel(J) 1234567891011121314151617181920212223### 4、反向传播BP- 上面正向传播可以计算得到`J(θ)`,使用梯度下降法还需要求它的梯度- BP反向传播的目的就是求代价函数的梯度- 假设4层的神经网络,![\\delta _&#123;\\text&#123;j&#125;&#125;^&#123;(l)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%28l%29%7D)记为--&gt;`l`层第`j`个单元的误差 - ![\\delta _&#123;\\text&#123;j&#125;&#125;^&#123;(4)&#125; = a_j^&#123;(4)&#125; - &#123;y_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%284%29%7D%20%3D%20a_j%5E%7B%284%29%7D%20-%20%7By_i%7D)《===》![&#123;\\delta ^&#123;(4)&#125;&#125; = &#123;a^&#123;(4)&#125;&#125; - y](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%284%29%7D%7D%20%3D%20%7Ba%5E%7B%284%29%7D%7D%20-%20y)（向量化） - ![&#123;\\delta ^&#123;(3)&#125;&#125; = &#123;(&#123;\\theta ^&#123;(3)&#125;&#125;)^T&#125;&#123;\\delta ^&#123;(4)&#125;&#125;.*&#123;g^&#125;(&#123;a^&#123;(3)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%283%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%283%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%284%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29) - ![&#123;\\delta ^&#123;(2)&#125;&#125; = &#123;(&#123;\\theta ^&#123;(2)&#125;&#125;)^T&#125;&#123;\\delta ^&#123;(3)&#125;&#125;.*&#123;g^&#125;(&#123;a^&#123;(2)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%282%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%283%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29) - 没有![&#123;\\delta ^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%281%29%7D%7D)，因为对于输入没有误差- 因为S型函数![&#123;\\text&#123;g(z)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctext%7Bg%28z%29%7D%7D)的倒数为：![&#123;g^&#125;(z)&#123;\\text&#123; = g(z)(1 - g(z))&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28z%29%7B%5Ctext%7B%20%3D%20g%28z%29%281%20-%20g%28z%29%29%7D%7D)，所以上面的![&#123;g^&#125;(&#123;a^&#123;(3)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29)和![&#123;g^&#125;(&#123;a^&#123;(2)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29)可以在前向传播中计算出来- 反向传播计算梯度的过程为： - ![\\Delta _&#123;ij&#125;^&#123;(l)&#125; = 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%200)（![\\Delta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20)是大写的![\\delta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20)） - for i=1-m: -![&#123;a^&#123;(1)&#125;&#125; = &#123;x^&#123;(i)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Ba%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%28i%29%7D%7D) -正向传播计算![&#123;a^&#123;(l)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Ba%5E%7B%28l%29%7D%7D)（l=2,3,4...L） -反向计算![&#123;\\delta ^&#123;(L)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%28L%29%7D%7D)、![&#123;\\delta ^&#123;(L - 1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%28L%20-%201%29%7D%7D)...![&#123;\\delta ^&#123;(2)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D)； -![\\Delta _&#123;ij&#125;^&#123;(l)&#125; = \\Delta _&#123;ij&#125;^&#123;(l)&#125; + a_j^&#123;(l)&#125;&#123;\\delta ^&#123;(l + 1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20a_j%5E%7B%28l%29%7D%7B%5Cdelta%20%5E%7B%28l%20%2B%201%29%7D%7D) -![D_&#123;ij&#125;^&#123;(l)&#125; = \\frac&#123;1&#125;&#123;m&#125;\\Delta _&#123;ij&#125;^&#123;(l)&#125; + \\lambda \\theta _&#123;ij&#125;^l\\begin&#123;array&#125;&#123;c&#125; &#123;&#125;&amp;amp; &#123;(j \\ne 0)&#125; \\end&#123;array&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5El%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7B%28j%20%5Cne%200%29%7D%20%20%5Cend%7Barray%7D%20) ![D_&#123;ij&#125;^&#123;(l)&#125; = \\frac&#123;1&#125;&#123;m&#125;\\Delta _&#123;ij&#125;^&#123;(l)&#125; + \\lambda \\theta _&#123;ij&#125;^lj = 0\\begin&#123;array&#125;&#123;c&#125; &#123;&#125;&amp;amp; &#123;j = 0&#125; \\end&#123;array&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5Elj%20%3D%200%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7Bj%20%3D%200%7D%20%20%5Cend%7Barray%7D%20) - 最后![\\frac1\\partial J(\\Theta )&#125;&#125;1\\partial \\Theta _&#123;ij&#125;^&#123;(l)&#125;&#125;&#125; = D_&#123;ij&#125;^&#123;(l)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5CTheta%20%29%7D%7D%7B%7B%5Cpartial%20%5CTheta%20_%7Bij%7D%5E%7B%28l%29%7D%7D%7D%20%3D%20D_%7Bij%7D%5E%7B%28l%29%7D)，即得到代价函数的梯度- 实现代码： 梯度def nnGradient(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda): length = nn_params.shape[0] Theta1 = nn_params[0:hidden_layer_size(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1) Theta2 = nn_params[hidden_layer_size(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1) m = X.shape[0] class_y = np.zeros((m,num_labels)) # 数据的y对应0-9，需要映射为0/1的关系 # 映射y for i in range(num_labels): class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值 &apos;&apos;&apos;去掉theta1和theta2的第一列，因为正则化时从1开始&apos;&apos;&apos; Theta1_colCount = Theta1.shape[1] Theta1_x = Theta1[:,1:Theta1_colCount] Theta2_colCount = Theta2.shape[1] Theta2_x = Theta2[:,1:Theta2_colCount] Theta1_grad = np.zeros((Theta1.shape)) #第一层到第二层的权重 Theta2_grad = np.zeros((Theta2.shape)) #第二层到第三层的权重 Theta1[:,0] = 0; Theta2[:,0] = 0; &apos;&apos;&apos;正向传播，每次需要补上一列1的偏置bias&apos;&apos;&apos; a1 = np.hstack((np.ones((m,1)),X)) z2 = np.dot(a1,np.transpose(Theta1)) a2 = sigmoid(z2) a2 = np.hstack((np.ones((m,1)),a2)) z3 = np.dot(a2,np.transpose(Theta2)) h = sigmoid(z3) &apos;&apos;&apos;反向传播，delta为误差，&apos;&apos;&apos; delta3 = np.zeros((m,num_labels)) delta2 = np.zeros((m,hidden_layer_size)) for i in range(m): delta3[i,:] = h[i,:]-class_y[i,:] Theta2_grad = Theta2_grad+np.dot(np.transpose(delta3[i,:].reshape(1,-1)),a2[i,:].reshape(1,-1)) delta2[i,:] = np.dot(delta3[i,:].reshape(1,-1),Theta2_x)*sigmoidGradient(z2[i,:]) Theta1_grad = Theta1_grad+np.dot(np.transpose(delta2[i,:].reshape(1,-1)),a1[i,:].reshape(1,-1)) &apos;&apos;&apos;梯度&apos;&apos;&apos; grad = (np.vstack((Theta1_grad.reshape(-1,1),Theta2_grad.reshape(-1,1)))+Lambda*np.vstack((Theta1.reshape(-1,1),Theta2.reshape(-1,1))))/m return np.ravel(grad) 12345678910111213141516### 5、BP可以求梯度的原因- 实际是利用了`链式求导`法则- 因为下一层的单元利用上一层的单元作为输入进行计算- 大体的推导过程如下，最终我们是想预测函数与已知的`y`非常接近，求均方差的梯度沿着此梯度方向可使代价函数最小化。可对照上面求梯度的过程。![enter description here][17]- 求误差更详细的推导过程：![enter description here][18]### 6、梯度检查- 检查利用`BP`求的梯度是否正确- 利用导数的定义验证：![\\frac1dJ(\\theta )&#125;&#125;1d\\theta &#125;&#125; \\approx \\frac1J(\\theta + \\varepsilon ) - J(\\theta - \\varepsilon )&#125;&#125;12\\varepsilon &#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7BdJ%28%5Ctheta%20%29%7D%7D%7B%7Bd%5Ctheta%20%7D%7D%20%5Capprox%20%5Cfrac%7B%7BJ%28%5Ctheta%20%20%2B%20%5Cvarepsilon%20%29%20-%20J%28%5Ctheta%20%20-%20%5Cvarepsilon%20%29%7D%7D%7B%7B2%5Cvarepsilon%20%7D%7D)- 求出来的数值梯度应该与BP求出的梯度非常接近- 验证BP正确后就不需要再执行验证梯度的算法了- 实现代码： 检验梯度是否计算正确检验梯度是否计算正确def checkGradient(Lambda = 0): ‘’’构造一个小型的神经网络验证，因为数值法计算梯度很浪费时间，而且验证正确后之后就不再需要验证了’’’ input_layer_size = 3 hidden_layer_size = 5 num_labels = 3 m = 5 initial_Theta1 = debugInitializeWeights(input_layer_size,hidden_layer_size); initial_Theta2 = debugInitializeWeights(hidden_layer_size,num_labels) X = debugInitializeWeights(input_layer_size-1,m) y = 1+np.transpose(np.mod(np.arange(1,m+1), num_labels))# 初始化y y = y.reshape(-1,1) nn_params = np.vstack((initial_Theta1.reshape(-1,1),initial_Theta2.reshape(-1,1))) #展开theta &apos;&apos;&apos;BP求出梯度&apos;&apos;&apos; grad = nnGradient(nn_params, input_layer_size, hidden_layer_size, num_labels, X, y, Lambda) &apos;&apos;&apos;使用数值法计算梯度&apos;&apos;&apos; num_grad = np.zeros((nn_params.shape[0])) step = np.zeros((nn_params.shape[0])) e = 1e-4 for i in range(nn_params.shape[0]): step[i] = e loss1 = nnCostFunction(nn_params-step.reshape(-1,1), input_layer_size, hidden_layer_size, num_labels, X, y, Lambda) loss2 = nnCostFunction(nn_params+step.reshape(-1,1), input_layer_size, hidden_layer_size, num_labels, X, y, Lambda) num_grad[i] = (loss2-loss1)/(2*e) step[i]=0 # 显示两列比较 res = np.hstack((num_grad.reshape(-1,1),grad.reshape(-1,1))) print res 12345### 7、权重的随机初始化- 神经网络不能像逻辑回归那样初始化`theta`为`0`,因为若是每条边的权重都为0，每个神经元都是相同的输出，在反向传播中也会得到同样的梯度，最终只会预测一种结果。- 所以应该初始化为接近0的数- 实现代码 随机初始化权重thetadef randInitializeWeights(L_in,L_out): W = np.zeros((L_out,1+L_in)) # 对应theta的权重 epsilon_init = (6.0/(L_out+L_in))*0.5 W = np.random.rand(L_out,1+L_in)2epsilon_init-epsilon_init # np.random.rand(L_out,1+L_in)产生L_out(1+L_in)大小的随机矩阵 return W1234### 8、预测- 正向传播预测结果- 实现代码 预测def predict(Theta1,Theta2,X): m = X.shape[0] num_labels = Theta2.shape[0] #p = np.zeros((m,1)) &apos;&apos;&apos;正向传播，预测结果&apos;&apos;&apos; X = np.hstack((np.ones((m,1)),X)) h1 = sigmoid(np.dot(X,np.transpose(Theta1))) h1 = np.hstack((np.ones((m,1)),h1)) h2 = sigmoid(np.dot(h1,np.transpose(Theta2))) &apos;&apos;&apos; 返回h中每一行最大值所在的列号 - np.max(h, axis=1)返回h中每一行的最大值（是某个数字的最大概率） - 最后where找到的最大概率所在的列号（列号即是对应的数字） &apos;&apos;&apos; #np.savetxt(&quot;h2.csv&quot;,h2,delimiter=&apos;,&apos;) p = np.array(np.where(h2[0,:] == np.max(h2, axis=1)[0])) for i in np.arange(1, m): t = np.array(np.where(h2[i,:] == np.max(h2, axis=1)[i])) p = np.vstack((p,t)) return p 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182### 9、输出结果- 梯度检查： ![enter description here][19]- 随机显示100个手写数字 ![enter description here][20]- 显示theta1权重 ![enter description here][21]- 训练集预测准确度 ![enter description here][22]- 归一化后训练集预测准确度 ![enter description here][23]--------------------## 四、SVM支持向量机### 1、代价函数- 在逻辑回归中，我们的代价为： ![\\cos t(&#123;h_\\theta &#125;(x),y) = \\left\\&#123; &#123;\\begin&#123;array&#125;&#123;c&#125; &#123; - \\log (&#123;h_\\theta &#125;(x))&#125; \\\\ &#123; - \\log (1 - &#123;h_\\theta &#125;(x))&#125; \\end&#123;array&#125; \\begin&#123;array&#125;&#123;c&#125; &#123;y = 1&#125; \\\\ &#123;y = 0&#125; \\end&#123;array&#125; &#125; \\right.](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%20%20%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7By%20%3D%201%7D%20%5C%5C%20%20%20%20%7By%20%3D%200%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright.)， 其中：![&#123;h_\\theta &#125;(&#123;\\text&#123;z&#125;&#125;) = \\frac&#123;1&#125;11 + &#123;e^&#123; - z&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28%7B%5Ctext%7Bz%7D%7D%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)，![z = &#123;\\theta ^T&#125;x](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=z%20%3D%20%7B%5Ctheta%20%5ET%7Dx)- 如图所示，如果`y=1`，`cost`代价函数如图所示 ![enter description here][24] 我们想让![&#123;\\theta ^T&#125;x &amp;gt; &amp;gt; 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5ET%7Dx%20%3E%20%20%3E%200)，即`z&gt;&gt;0`，这样的话`cost`代价函数才会趋于最小（这是我们想要的），所以用途中**红色**的函数![\\cos &#123;t_1&#125;(z)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20%7Bt_1%7D%28z%29)代替逻辑回归中的cost- 当`y=0`时同样，用![\\cos &#123;t_0&#125;(z)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20%7Bt_0%7D%28z%29)代替![enter description here][25]- 最终得到的代价函数为： ![J(\\theta ) = C\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\cos &#123;t_1&#125;(&#123;\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\\cos &#123;t_0&#125;(&#123;\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;)&#125; ] + \\frac&#123;1&#125;&#123;2&#125;\\sum\\limits_&#123;j = 1&#125;^&#123;\\text&#123;n&#125;&#125; &#123;\\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20) 最后我们想要![\\mathop &#123;\\min &#125;\\limits_\\theta J(\\theta )](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cmathop%20%7B%5Cmin%20%7D%5Climits_%5Ctheta%20J%28%5Ctheta%20%29)- 之前我们逻辑回归中的代价函数为： ![J(\\theta ) = - \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\log (&#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\\log (1 - &#123;h_\\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)] + \\frac&#123;\\lambda &#125;12m&#125;&#125;\\sum\\limits_&#123;j = 1&#125;^n &#123;\\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20) 可以认为这里的![C = \\frac&#123;m&#125;&#123;\\lambda &#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=C%20%3D%20%5Cfrac%7Bm%7D%7B%5Clambda%20%7D)，只是表达形式问题，这里`C`的值越大，SVM的决策边界的`margin`也越大，下面会说明### 2、Large Margin- 如下图所示,SVM分类会使用最大的`margin`将其分开 ![enter description here][26]- 先说一下向量内积 - ![u = \\left[ &#123;\\begin&#123;array&#125;&#123;c&#125; 1u_1&#125;&#125; \\\\ 1u_2&#125;&#125; \\end&#123;array&#125; &#125; \\right]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=u%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bu_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bu_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D)，![v = \\left[ &#123;\\begin&#123;array&#125;&#123;c&#125; 1v_1&#125;&#125; \\\\ 1v_2&#125;&#125; \\end&#123;array&#125; &#125; \\right]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=v%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bv_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bv_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D) - ![||u||](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7Cu%7C%7C)表示`u`的**欧几里得范数**（欧式范数），![||u||&#123;\\text&#123; = &#125;&#125;\\sqrt 1\\text&#123;u&#125;&#125;_1^2 + u_2^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7Cu%7C%7C%7B%5Ctext%7B%20%3D%20%7D%7D%5Csqrt%20%7B%7B%5Ctext%7Bu%7D%7D_1%5E2%20%2B%20u_2%5E2%7D%20) - `向量V`在`向量u`上的投影的长度记为`p`，则：向量内积： ![1\\text&#123;u&#125;&#125;^T&#125;v = p||u|| = &#123;u_1&#125;&#123;v_1&#125; + &#123;u_2&#125;&#123;v_2&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7B%5Ctext%7Bu%7D%7D%5ET%7Dv%20%3D%20p%7C%7Cu%7C%7C%20%3D%20%7Bu_1%7D%7Bv_1%7D%20%2B%20%7Bu_2%7D%7Bv_2%7D) ![enter description here][27] 根据向量夹角公式推导一下即可，![\\cos \\theta = \\frac1\\overrightarrow &#123;\\text&#123;u&#125;&#125; \\overrightarrow v &#125;&#125;1|\\overrightarrow &#123;\\text&#123;u&#125;&#125; ||\\overrightarrow v |&#125;&#125;](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Ccos%20%5Ctheta%20%3D%20%5Cfrac%7B%7B%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%5Coverrightarrow%20v%20%7D%7D%7B%7B%7C%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%7C%7C%5Coverrightarrow%20v%20%7C%7D%7D)- 前面说过，当`C`越大时，`margin`也就越大，我们的目的是最小化代价函数`J(θ)`,当`margin`最大时，`C`的乘积项![\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\cos &#123;t_1&#125;(&#123;\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\\cos &#123;t_0&#125;(&#123;\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;)&#125; ]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D)要很小，所以近似为： ![J(\\theta ) = C0 + \\frac&#123;1&#125;&#123;2&#125;\\sum\\limits_&#123;j = 1&#125;^&#123;\\text&#123;n&#125;&#125; &#123;\\theta _j^2&#125; = \\frac&#123;1&#125;&#123;2&#125;\\sum\\limits_&#123;j = 1&#125;^&#123;\\text&#123;n&#125;&#125; &#123;\\theta _j^2&#125; = \\frac&#123;1&#125;&#123;2&#125;(\\theta _1^2 + \\theta _2^2) = \\frac&#123;1&#125;&#123;2&#125;&#123;\\sqrt &#123;\\theta _1^2 + \\theta _2^2&#125; ^2&#125;](http://latex.codecogs.com/gif.latex?%5Clarge%20J%28%5Ctheta%20%29%20%3D%20C0%20&amp;plus;%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%7B%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%7D%20%5E2%7D)， 我们最后的目的就是求使代价最小的`θ`- 由 ![\\left\\&#123; &#123;\\begin&#123;array&#125;&#123;c&#125; 1\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125; \\geqslant 1&#125; \\\\ 1\\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125; \\leqslant - 1&#125; \\end&#123;array&#125; &#125; \\right.\\begin&#123;array&#125;&#123;c&#125; &#123;(&#123;y^&#123;(i)&#125;&#125; = 1)&#125; \\\\ &#123;(&#123;y^&#123;(i)&#125;&#125; = 0)&#125; \\end&#123;array&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)可以得到： ![\\left\\&#123; &#123;\\begin&#123;array&#125;&#123;c&#125; 1p^&#123;(i)&#125;&#125;||\\theta || \\geqslant 1&#125; \\\\ 1p^&#123;(i)&#125;&#125;||\\theta || \\leqslant - 1&#125; \\end&#123;array&#125; &#125; \\right.\\begin&#123;array&#125;&#123;c&#125; &#123;(&#123;y^&#123;(i)&#125;&#125; = 1)&#125; \\\\ &#123;(&#123;y^&#123;(i)&#125;&#125; = 0)&#125; \\end&#123;array&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)，`p`即为`x`在`θ`上的投影- 如下图所示，假设决策边界如图，找其中的一个点，到`θ`上的投影为`p`,则![p||\\theta || \\geqslant 1](http://latex.codecogs.com/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201)或者![p||\\theta || \\leqslant - 1](http://latex.codecogs.com/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201)，若是`p`很小，则需要![||\\theta ||](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7C%5Ctheta%20%7C%7C)很大，这与我们要求的`θ`使![||\\theta || = \\frac&#123;1&#125;&#123;2&#125;\\sqrt &#123;\\theta _1^2 + \\theta _2^2&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%7C%7C%5Ctheta%20%7C%7C%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%7D)最小相违背，**所以**最后求的是`large margin` ![enter description here][28]### 3、SVM Kernel（核函数）- 对于线性可分的问题，使用**线性核函数**即可- 对于线性不可分的问题，在逻辑回归中，我们是将`feature`映射为使用多项式的形式![1 + &#123;x_1&#125; + &#123;x_2&#125; + x_1^2 + &#123;x_1&#125;&#123;x_2&#125; + x_2^2](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)，`SVM`中也有**多项式核函数**，但是更常用的是**高斯核函数**，也称为**RBF核**- 高斯核函数为：![f(x) = &#123;e^&#123; - \\frac1||x - u|&#123;|^2&#125;&#125;&#125;12&#123;\\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=f%28x%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20u%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D) 假设如图几个点，![enter description here][29]令： ![&#123;f_1&#125; = similarity(x,&#123;l^&#123;(1)&#125;&#125;) = &#123;e^&#123; - \\frac1||x - &#123;l^&#123;(1)&#125;&#125;|&#123;|^2&#125;&#125;&#125;12&#123;\\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_1%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%281%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%281%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D) ![&#123;f_2&#125; = similarity(x,&#123;l^&#123;(2)&#125;&#125;) = &#123;e^&#123; - \\frac1||x - &#123;l^&#123;(2)&#125;&#125;|&#123;|^2&#125;&#125;&#125;12&#123;\\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_2%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%282%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%282%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)...- 可以看出，若是`x`与![&#123;l^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D)距离较近，==》![&#123;f_1&#125; \\approx &#123;e^0&#125; = 1](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_1%7D%20%5Capprox%20%7Be%5E0%7D%20%3D%201)，（即相似度较大） 若是`x`与![&#123;l^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D)距离较远，==》![&#123;f_2&#125; \\approx &#123;e^&#123; - \\infty &#125;&#125; = 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_2%7D%20%5Capprox%20%7Be%5E%7B%20-%20%5Cinfty%20%7D%7D%20%3D%200)，（即相似度较低）- 高斯核函数的`σ`越小，`f`下降的越快 ![enter description here][30]![enter description here][31]- 如何选择初始的![&#123;l^&#123;(1)&#125;&#125;&#123;l^&#123;(2)&#125;&#125;&#123;l^&#123;(3)&#125;&#125;...](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D%7Bl%5E%7B%282%29%7D%7D%7Bl%5E%7B%283%29%7D%7D...) - 训练集：![((&#123;x^&#123;(1)&#125;&#125;,&#123;y^&#123;(1)&#125;&#125;),(&#123;x^&#123;(2)&#125;&#125;,&#123;y^&#123;(2)&#125;&#125;),...(&#123;x^&#123;(m)&#125;&#125;,&#123;y^&#123;(m)&#125;&#125;))](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%28%28%7Bx%5E%7B%281%29%7D%7D%2C%7By%5E%7B%281%29%7D%7D%29%2C%28%7Bx%5E%7B%282%29%7D%7D%2C%7By%5E%7B%282%29%7D%7D%29%2C...%28%7Bx%5E%7B%28m%29%7D%7D%2C%7By%5E%7B%28m%29%7D%7D%29%29) - 选择：![&#123;l^&#123;(1)&#125;&#125; = &#123;x^&#123;(1)&#125;&#125;,&#123;l^&#123;(2)&#125;&#125; = &#123;x^&#123;(2)&#125;&#125;...&#123;l^&#123;(m)&#125;&#125; = &#123;x^&#123;(m)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bl%5E%7B%282%29%7D%7D%20%3D%20%7Bx%5E%7B%282%29%7D%7D...%7Bl%5E%7B%28m%29%7D%7D%20%3D%20%7Bx%5E%7B%28m%29%7D%7D) - 对于给出的`x`，计算`f`,令：![f_0^&#123;(i)&#125; = 1](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=f_0%5E%7B%28i%29%7D%20%3D%201)所以：![&#123;f^&#123;(i)&#125;&#125; \\in &#123;R^&#123;m + 1&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf%5E%7B%28i%29%7D%7D%20%5Cin%20%7BR%5E%7Bm%20%2B%201%7D%7D) - 最小化`J`求出`θ`， ![J(\\theta ) = C\\sum\\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\\cos &#123;t_1&#125;(&#123;\\theta ^T&#125;&#123;f^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\\cos &#123;t_0&#125;(&#123;\\theta ^T&#125;&#123;f^&#123;(i)&#125;&#125;)&#125; ] + \\frac&#123;1&#125;&#123;2&#125;\\sum\\limits_&#123;j = 1&#125;^&#123;\\text&#123;n&#125;&#125; &#123;\\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20) - 如果![&#123;\\theta ^T&#125;f \\geqslant 0](http://latex.codecogs.com/gif.latex?%5Clarge%20%7B%5Ctheta%20%5ET%7Df%20%5Cgeqslant%200)，==》预测`y=1`### 4、使用`scikit-learn`中的`SVM`模型代码- [全部代码](/SVM/SVM_scikit-learn.py)- 线性可分的,指定核函数为`linear`： &apos;&apos;&apos;data1——线性分类&apos;&apos;&apos; data1 = spio.loadmat(&apos;data1.mat&apos;) X = data1[&apos;X&apos;] y = data1[&apos;y&apos;] y = np.ravel(y) plot_data(X,y) model = svm.SVC(C=1.0,kernel=&apos;linear&apos;).fit(X,y) # 指定核函数为线性核函数 1- 非线性可分的，默认核函数为`rbf` &apos;&apos;&apos;data2——非线性分类&apos;&apos;&apos; data2 = spio.loadmat(&apos;data2.mat&apos;) X = data2[&apos;X&apos;] y = data2[&apos;y&apos;] y = np.ravel(y) plt = plot_data(X,y) plt.show() model = svm.SVC(gamma=100).fit(X,y) # gamma为核函数的系数，值越大拟合的越好 12345678910111213141516171819202122232425### 5、运行结果- 线性可分的决策边界： ![enter description here][32]- 线性不可分的决策边界： ![enter description here][33]--------------------------## 五、K-Means聚类算法- [全部代码](/K-Means/K-Menas.py)### 1、聚类过程- 聚类属于无监督学习，不知道y的标记分为K类- K-Means算法分为两个步骤 - 第一步：簇分配，随机选`K`个点作为中心，计算到这`K`个点的距离，分为`K`个簇 - 第二步：移动聚类中心：重新计算每个**簇**的中心，移动中心，重复以上步骤。- 如下图所示： - 随机分配的聚类中心 ![enter description here][34] - 重新计算聚类中心，移动一次 ![enter description here][35] - 最后`10`步之后的聚类中心 ![enter description here][36]- 计算每条数据到哪个中心最近实现代码： 找到每条数据距离哪个类中心最近def findClosestCentroids(X,initial_centroids): m = X.shape[0] # 数据条数 K = initial_centroids.shape[0] # 类的总数 dis = np.zeros((m,K)) # 存储计算每个点分别到K个类的距离 idx = np.zeros((m,1)) # 要返回的每条数据属于哪个类 &apos;&apos;&apos;计算每个点到每个类中心的距离&apos;&apos;&apos; for i in range(m): for j in range(K): dis[i,j] = np.dot((X[i,:]-initial_centroids[j,:]).reshape(1,-1),(X[i,:]-initial_centroids[j,:]).reshape(-1,1)) &apos;&apos;&apos;返回dis每一行的最小值对应的列号，即为对应的类别 - np.min(dis, axis=1)返回每一行的最小值 - np.where(dis == np.min(dis, axis=1).reshape(-1,1)) 返回对应最小值的坐标 - 注意：可能最小值对应的坐标有多个，where都会找出来，所以返回时返回前m个需要的即可（因为对于多个最小值，属于哪个类别都可以） &apos;&apos;&apos; dummy,idx = np.where(dis == np.min(dis, axis=1).reshape(-1,1)) return idx[0:dis.shape[0]] # 注意截取一下 1- 计算类中心实现代码： 计算类中心def computerCentroids(X,idx,K): n = X.shape1 centroids = np.zeros((K,n)) for i in range(K): centroids[i,:] = np.mean(X[np.ravel(idx==i),:], axis=0).reshape(1,-1) # 索引要是一维的,axis=0为每一列，idx==i一次找出属于哪一类的，然后计算均值 return centroids12345678910111213### 2、目标函数- 也叫做**失真代价函数**- ![J(&#123;c^&#123;(1)&#125;&#125;, \\cdots ,&#123;c^&#123;(m)&#125;&#125;,&#123;u_1&#125;, \\cdots ,&#123;u_k&#125;) = \\frac&#123;1&#125;&#123;m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - &#123;u_1c^&#123;(i)&#125;&#125;&#125;&#125;|&#123;|^2&#125;&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%7Bc%5E%7B%281%29%7D%7D%2C%20%5Ccdots%20%2C%7Bc%5E%7B%28m%29%7D%7D%2C%7Bu_1%7D%2C%20%5Ccdots%20%2C%7Bu_k%7D%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20%7Bu_%7B%7Bc%5E%7B%28i%29%7D%7D%7D%7D%7C%7B%7C%5E2%7D%7D%20)- 最后我们想得到： ![enter description here][37]- 其中![&#123;c^&#123;(i)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%5E%7B%28i%29%7D%7D)表示第`i`条数据距离哪个类中心最近，- 其中![&#123;u_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bu_i%7D)即为聚类的中心### 3、聚类中心的选择- 随机初始化，从给定的数据中随机抽取K个作为聚类中心- 随机一次的结果可能不好，可以随机多次，最后取使代价函数最小的作为中心- 实现代码：(这里随机一次) 初始化类中心–随机取K个点作为聚类中心def kMeansInitCentroids(X,K): m = X.shape[0] m_arr = np.arange(0,m) # 生成0-m-1 centroids = np.zeros((K,X.shape1)) np.random.shuffle(m_arr) # 打乱m_arr顺序 rand_indices = m_arr[:K] # 取前K个 centroids = X[rand_indices,:] return centroids123456789101112### 4、聚类个数K的选择- 聚类是不知道y的label的，所以不知道真正的聚类个数- 肘部法则（Elbow method） - 作代价函数`J`和`K`的图，若是出现一个拐点，如下图所示，`K`就取拐点处的值，下图此时`K=3` ![enter description here][38] - 若是很平滑就不明确，人为选择。- 第二种就是人为观察选择### 5、应用——图片压缩- 将图片的像素分为若干类，然后用这个类代替原来的像素值- 执行聚类的算法代码： 聚类算法def runKMeans(X,initial_centroids,max_iters,plot_process): m,n = X.shape # 数据条数和维度 K = initial_centroids.shape[0] # 类数 centroids = initial_centroids # 记录当前类中心 previous_centroids = centroids # 记录上一次类中心 idx = np.zeros((m,1)) # 每条数据属于哪个类 for i in range(max_iters): # 迭代次数 print u&apos;迭代计算次数：%d&apos;%(i+1) idx = findClosestCentroids(X, centroids) if plot_process: # 如果绘制图像 plt = plotProcessKMeans(X,centroids,previous_centroids) # 画聚类中心的移动过程 previous_centroids = centroids # 重置 centroids = computerCentroids(X, idx, K) # 重新计算类中心 if plot_process: # 显示最终的绘制结果 plt.show() return centroids,idx # 返回聚类中心和数据属于哪个类 1234### 6、[使用scikit-learn库中的线性模型实现聚类](/K-Means/K-Means_scikit-learn.py)- 导入包 from sklearn.cluster import KMeans 1- 使用模型拟合数据 model = KMeans(n_clusters=3).fit(X) # n_clusters指定3类，拟合数据 1- 聚类中心 centroids = model.cluster_centers_ # 聚类中心 1234567891011121314151617181920212223242526272829303132333435### 7、运行结果- 二维数据类中心的移动 ![enter description here][39]- 图片压缩 ![enter description here][40]----------------------## 六、PCA主成分分析（降维）- [全部代码](/PCA/PCA.py)### 1、用处- 数据压缩（Data Compression）,使程序运行更快- 可视化数据，例如`3D--&gt;2D`等- ......### 2、2D--&gt;1D，nD--&gt;kD- 如下图所示，所有数据点可以投影到一条直线，是**投影距离的平方和**（投影误差）最小![enter description here][41]- 注意数据需要`归一化`处理- 思路是找`1`个`向量u`,所有数据投影到上面使投影距离最小- 那么`nD--&gt;kD`就是找`k`个向量![$$&#123;u^&#123;(1)&#125;&#125;,&#123;u^&#123;(2)&#125;&#125; \\ldots &#123;u^&#123;(k)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%20%5Cldots%20%7Bu%5E%7B%28k%29%7D%7D%24%24)，所有数据投影到上面使投影误差最小 - eg:3D--&gt;2D,2个向量![$$&#123;u^&#123;(1)&#125;&#125;,&#123;u^&#123;(2)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%24%24)就代表一个平面了，所有点投影到这个平面的投影误差最小即可### 3、主成分分析PCA与线性回归的区别- 线性回归是找`x`与`y`的关系，然后用于预测`y`- `PCA`是找一个投影面，最小化data到这个投影面的投影误差### 4、PCA降维过程- 数据预处理（均值归一化） - 公式：![$$&#123;\\rm&#123;x&#125;&#125;_j^&#123;(i)&#125; = 1&#123;\\rm&#123;x&#125;&#125;_j^&#123;(i)&#125; - &#123;u_j&#125;&#125; \\over 1s_j&#125;&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20%3D%20%7B%7B%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20-%20%7Bu_j%7D%7D%20%5Cover%20%7B%7Bs_j%7D%7D%7D%24%24) - 就是减去对应feature的均值，然后除以对应特征的标准差（也可以是最大值-最小值） - 实现代码： # 归一化数据 def featureNormalize(X): &apos;&apos;&apos;（每一个数据-当前列的均值）/当前列的标准差&apos;&apos;&apos; n = X.shape[1] mu = np.zeros((1,n)); sigma = np.zeros((1,n)) mu = np.mean(X,axis=0) sigma = np.std(X,axis=0) for i in range(n): X[:,i] = (X[:,i]-mu[i])/sigma[i] return X,mu,sigma 12345- 计算`协方差矩阵Σ`（Covariance Matrix）：![$$\\Sigma = &#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^n 1x^&#123;(i)&#125;&#125;1(&#123;x^&#123;(i)&#125;&#125;)&#125;^T&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%24%24%5CSigma%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7Bx%5E%7B%28i%29%7D%7D%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%29%7D%5ET%7D%7D%20%24%24) - 注意这里的`Σ`和求和符号不同 - 协方差矩阵`对称正定`（不理解正定的看看线代） - 大小为`nxn`,`n`为`feature`的维度 - 实现代码： Sigma = np.dot(np.transpose(X_norm),X_norm)/m # 求Sigma 1234567891011- 计算`Σ`的特征值和特征向量 - 可以是用`svd`奇异值分解函数：`U,S,V = svd(Σ)` - 返回的是与`Σ`同样大小的对角阵`S`（由`Σ`的特征值组成）[**注意**：`matlab`中函数返回的是对角阵，在`python`中返回的是一个向量，节省空间] - 还有两个**酉矩阵**U和V，且![$$\\Sigma = US&#123;V^T&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%5CSigma%20%3D%20US%7BV%5ET%7D%24%24) - ![enter description here][42] - **注意**：`svd`函数求出的`S`是按特征值降序排列的，若不是使用`svd`,需要按**特征值**大小重新排列`U`- 降维 - 选取`U`中的前`K`列（假设要降为`K`维） - ![enter description here][43] - `Z`就是对应降维之后的数据 - 实现代码： # 映射数据 def projectData(X_norm,U,K): Z = np.zeros((X_norm.shape[0],K)) U_reduce = U[:,0:K] # 取前K个 Z = np.dot(X_norm,U_reduce) return Z 123456789101112- 过程总结： - `Sigma = X&apos;*X/m` - `U,S,V = svd(Sigma)` - `Ureduce = U[:,0:k]` - `Z = Ureduce&apos;*x`### 5、数据恢复 - 因为：![$$&#123;Z^&#123;(i)&#125;&#125; = U_&#123;reduce&#125;^T*&#123;X^&#123;(i)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BZ%5E%7B%28i%29%7D%7D%20%3D%20U_%7Breduce%7D%5ET*%7BX%5E%7B%28i%29%7D%7D%24%24) - 所以：![$$&#123;X_&#123;approx&#125;&#125; = &#123;(U_&#123;reduce&#125;^T)^&#123; - 1&#125;&#125;Z$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5ET%29%5E%7B%20-%201%7D%7DZ%24%24) （注意这里是X的近似值） - 又因为`Ureduce`为正定矩阵，【正定矩阵满足：![$$A&#123;A^T&#125; = &#123;A^T&#125;A = E$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24A%7BA%5ET%7D%20%3D%20%7BA%5ET%7DA%20%3D%20E%24%24)，所以：![$$&#123;A^&#123; - 1&#125;&#125; = &#123;A^T&#125;$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BA%5E%7B%20-%201%7D%7D%20%3D%20%7BA%5ET%7D%24%24)】，所以这里： - ![$$&#123;X_&#123;approx&#125;&#125; = &#123;(U_&#123;reduce&#125;^&#123; - 1&#125;)^&#123; - 1&#125;&#125;Z = &#123;U_&#123;reduce&#125;&#125;Z$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5E%7B%20-%201%7D%29%5E%7B%20-%201%7D%7DZ%20%3D%20%7BU_%7Breduce%7D%7DZ%24%24) - 实现代码： # 恢复数据 def recoverData(Z,U,K): X_rec = np.zeros((Z.shape[0],U.shape[0])) U_recude = U[:,0:K] X_rec = np.dot(Z,np.transpose(U_recude)) # 还原数据（近似） return X_rec 123456789101112131415161718192021222324252627282930313233### 6、主成分个数的选择（即要降的维度）- 如何选择 - **投影误差**（project error）：![$$&#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - x_&#123;approx&#125;^&#123;(i)&#125;|&#123;|^2&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%24%24) - **总变差**（total variation）:![$$&#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125;|&#123;|^2&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%24%24) - 若**误差率**（error ratio）：![$$1&#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - x_&#123;approx&#125;^&#123;(i)&#125;|&#123;|^2&#125;&#125; &#125; \\over 11 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125;|&#123;|^2&#125;&#125; &#125;&#125; \\le 0.01$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%7D%20%5Cover%20%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%7D%7D%20%5Cle%200.01%24%24)，则称`99%`保留差异性 - 误差率一般取`1%，5%，10%`等- 如何实现 - 若是一个个试的话代价太大 - 之前`U,S,V = svd(Sigma)`,我们得到了`S`，这里误差率error ratio: ![$$error&#123;\\kern 1pt&#125; \\;ratio = 1 - 1\\sum\\limits_&#123;i = 1&#125;^k 1S_&#123;ii&#125;&#125;&#125; &#125; \\over &#123;\\sum\\limits_&#123;i = 1&#125;^n 1S_&#123;ii&#125;&#125;&#125; &#125;&#125; \\le threshold$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24error%7B%5Ckern%201pt%7D%20%5C%3Bratio%20%3D%201%20-%20%7B%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5Ek%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%20%5Cover%20%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%7D%20%5Cle%20threshold%24%24) - 可以一点点增加`K`尝试。### 7、使用建议- 不要使用PCA去解决过拟合问题`Overfitting`，还是使用正则化的方法（如果保留了很高的差异性还是可以的）- 只有在原数据上有好的结果，但是运行很慢，才考虑使用PCA### 8、运行结果- 2维数据降为1维 - 要投影的方向 ![enter description here][44] - 2D降为1D及对应关系 ![enter description here][45]- 人脸数据降维 - 原始数据 ![enter description here][46] - 可视化部分`U`矩阵信息 ![enter description here][47] - 恢复数据 ![enter description here][48]### 9、[使用scikit-learn库中的PCA实现降维](/PCA/PCA.py_scikit-learn.py)- 导入需要的包： #-- coding: utf-8 -- Author:bobDate:2016.12.22import numpy as npfrom matplotlib import pyplot as pltfrom scipy import io as spiofrom sklearn.decomposition import pcafrom sklearn.preprocessing import StandardScaler1- 归一化数据 &apos;&apos;&apos;归一化数据并作图&apos;&apos;&apos; scaler = StandardScaler() scaler.fit(X) x_train = scaler.transform(X) 12- 使用PCA模型拟合数据，并降维 - `n_components`对应要将的维度 &apos;&apos;&apos;拟合数据&apos;&apos;&apos; K=1 # 要降的维度 model = pca.PCA(n_components=K).fit(x_train) # 拟合数据，n_components定义要降的维度 Z = model.transform(x_train) # transform就会执行降维操作 123- 数据恢复 - `model.components_`会得到降维使用的`U`矩阵 &apos;&apos;&apos;数据恢复并作图&apos;&apos;&apos; Ureduce = model.components_ # 得到降维用的Ureduce x_rec = np.dot(Z,Ureduce) # 数据恢复 123456789101112131415161718192021222324252627---------------------------------------------------------------## 七、异常检测 Anomaly Detection- [全部代码](/AnomalyDetection/AnomalyDetection.py)### 1、高斯分布（正态分布）`Gaussian distribution` - 分布函数：![$$p(x) = &#123;1 \\over &#123;\\sqrt &#123;2\\pi &#125; \\sigma &#125;&#125;&#123;e^&#123; - 11(x - u)&#125;^2&#125;&#125; \\over &#123;2&#123;\\sigma ^2&#125;&#125;&#125;&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%5Csqrt%20%7B2%5Cpi%20%7D%20%5Csigma%20%7D%7D%7Be%5E%7B%20-%20%7B%7B%7B%7B%28x%20-%20u%29%7D%5E2%7D%7D%20%5Cover%20%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D%24%24) - 其中，`u`为数据的**均值**，`σ`为数据的**标准差** - `σ`越**小**，对应的图像越**尖**- 参数估计（`parameter estimation`） - ![$$u = &#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m 1x^&#123;(i)&#125;&#125;&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24u%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7Bx%5E%7B%28i%29%7D%7D%7D%20%24%24) - ![$$&#123;\\sigma ^2&#125; = &#123;1 \\over m&#125;\\sum\\limits_&#123;i = 1&#125;^m 1&#123;(&#123;x^&#123;(i)&#125;&#125; - u)&#125;^2&#125;&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7B%5Csigma%20%5E2%7D%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%20-%20u%29%7D%5E2%7D%7D%20%24%24)### 2、异常检测算法- 例子 - 训练集：![$$\\&#123; &#123;x^&#123;(1)&#125;&#125;,&#123;x^&#123;(2)&#125;&#125;, \\cdots &#123;x^&#123;(m)&#125;&#125;\\&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5C%7B%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bx%5E%7B%282%29%7D%7D%2C%20%5Ccdots%20%7Bx%5E%7B%28m%29%7D%7D%5C%7D%20%24%24),其中![$$x \\in &#123;R^n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24) - 假设![$$&#123;x_1&#125;,&#123;x_2&#125; \\cdots &#123;x_n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7Bx_1%7D%2C%7Bx_2%7D%20%5Ccdots%20%7Bx_n%7D%24%24)相互独立，建立model模型：![$$p(x) = p(&#123;x_1&#125;;&#123;u_1&#125;,\\sigma _1^2)p(&#123;x_2&#125;;&#123;u_2&#125;,\\sigma _2^2) \\cdots p(&#123;x_n&#125;;&#123;u_n&#125;,\\sigma _n^2) = \\prod\\limits_&#123;j = 1&#125;^n &#123;p(&#123;x_j&#125;;&#123;u_j&#125;,\\sigma _j^2)&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20p%28%7Bx_1%7D%3B%7Bu_1%7D%2C%5Csigma%20_1%5E2%29p%28%7Bx_2%7D%3B%7Bu_2%7D%2C%5Csigma%20_2%5E2%29%20%5Ccdots%20p%28%7Bx_n%7D%3B%7Bu_n%7D%2C%5Csigma%20_n%5E2%29%20%3D%20%5Cprod%5Climits_%7Bj%20%3D%201%7D%5En%20%7Bp%28%7Bx_j%7D%3B%7Bu_j%7D%2C%5Csigma%20_j%5E2%29%7D%20%24%24)- 过程 - 选择具有代表异常的`feature`:xi - 参数估计：![$$&#123;u_1&#125;,&#123;u_2&#125;, \\cdots ,&#123;u_n&#125;;\\sigma _1^2,\\sigma _2^2 \\cdots ,\\sigma _n^2$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7Bu_1%7D%2C%7Bu_2%7D%2C%20%5Ccdots%20%2C%7Bu_n%7D%3B%5Csigma%20_1%5E2%2C%5Csigma%20_2%5E2%20%5Ccdots%20%2C%5Csigma%20_n%5E2%24%24) - 计算`p(x)`,若是`P(x)&lt;ε`则认为异常，其中`ε`为我们要求的概率的临界值`threshold`- 这里只是**单元高斯分布**，假设了`feature`之间是独立的，下面会讲到**多元高斯分布**，会自动捕捉到`feature`之间的关系- **参数估计**实现代码 参数估计函数（就是求均值和方差）def estimateGaussian(X): m,n = X.shape mu = np.zeros((n,1)) sigma2 = np.zeros((n,1)) mu = np.mean(X, axis=0) # axis=0表示列，每列的均值 sigma2 = np.var(X,axis=0) # 求每列的方差 return mu,sigma2 12345678910111213141516### 3、评价`p(x)`的好坏，以及`ε`的选取- 对**偏斜数据**的错误度量 - 因为数据可能是非常**偏斜**的（就是`y=1`的个数非常少，(`y=1`表示异常)），所以可以使用`Precision/Recall`，计算`F1Score`(在**CV交叉验证集**上) - 例如：预测癌症，假设模型可以得到`99%`能够预测正确，`1%`的错误率，但是实际癌症的概率很小，只有`0.5%`，那么我们始终预测没有癌症y=0反而可以得到更小的错误率。使用`error rate`来评估就不科学了。 - 如下图记录： ![enter description here][49] - ![$$\\Pr ecision = 1TP&#125; \\over &#123;TP + FP&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5CPr%20ecision%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&amp;plus;%20FP%7D%7D%24%24) ，即：**正确预测正样本/所有预测正样本** - ![$$&#123;\\mathop&#123;\\rm Re&#125;\\nolimits&#125; &#123;\\rm&#123;call&#125;&#125; = 1TP&#125; \\over &#123;TP + FN&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7B%5Cmathop%7B%5Crm%20Re%7D%5Cnolimits%7D%20%7B%5Crm%7Bcall%7D%7D%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&amp;plus;%20FN%7D%7D%24%24) ，即：**正确预测正样本/真实值为正样本** - 总是让`y=1`(较少的类)，计算`Precision`和`Recall` - ![$$&#123;F_1&#125;Score = 21PR&#125; \\over &#123;P + R&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7BF_1%7DScore%20%3D%202%7B%7BPR%7D%20%5Cover%20%7BP%20&amp;plus;%20R%7D%7D%24%24) - 还是以癌症预测为例，假设预测都是no-cancer，TN=199，FN=1，TP=0，FP=0，所以：Precision=0/0，Recall=0/1=0，尽管accuracy=199/200=99.5%，但是不可信。- `ε`的选取 - 尝试多个`ε`值，使`F1Score`的值高- 实现代码 选择最优的epsilon，即：使F1Score最大def selectThreshold(yval,pval): ‘’’初始化所需变量’’’ bestEpsilon = 0. bestF1 = 0. F1 = 0. step = (np.max(pval)-np.min(pval))/1000 ‘’’计算’’’ for epsilon in np.arange(np.min(pval),np.max(pval),step): cvPrecision = pval bestF1: # 修改最优的F1 Score bestF1 = F1 bestEpsilon = epsilon return bestEpsilon,bestF112345678910111213141516171819202122232425262728### 4、选择使用什么样的feature（单元高斯分布）- 如果一些数据不是满足高斯分布的，可以变化一下数据，例如`log(x+C),x^(1/2)`等- 如果`p(x)`的值无论异常与否都很大，可以尝试组合多个`feature`,(因为feature之间可能是有关系的)### 5、多元高斯分布- 单元高斯分布存在的问题 - 如下图，红色的点为异常点，其他的都是正常点（比如CPU和memory的变化） ![enter description here][50] - x1对应的高斯分布如下： ![enter description here][51] - x2对应的高斯分布如下： ![enter description here][52] - 可以看出对应的p(x1)和p(x2)的值变化并不大，就不会认为异常 - 因为我们认为feature之间是相互独立的，所以如上图是以**正圆**的方式扩展- 多元高斯分布 - ![$$x \\in &#123;R^n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24)，并不是建立`p(x1),p(x2)...p(xn)`，而是统一建立`p(x)` - 其中参数：![$$\\mu \\in &#123;R^n&#125;,\\Sigma \\in &#123;R^&#123;n \\times &#123;\\rm&#123;n&#125;&#125;&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5Cmu%20%5Cin%20%7BR%5En%7D%2C%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24),`Σ`为**协方差矩阵** - ![$$p(x) = &#123;1 \\over 1&#123;(2\\pi )&#125;^1n \\over 2&#125;&#125;&#125;|\\Sigma &#123;|^11 \\over 2&#125;&#125;&#125;&#125;&#125;&#123;e^&#123; - &#123;1 \\over 2&#125;1(x - u)&#125;^T&#125;&#123;\\Sigma ^&#123; - 1&#125;&#125;(x - u)&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%7B%7B%282%5Cpi%20%29%7D%5E%7B%7Bn%20%5Cover%202%7D%7D%7D%7C%5CSigma%20%7B%7C%5E%7B%7B1%20%5Cover%202%7D%7D%7D%7D%7D%7Be%5E%7B%20-%20%7B1%20%5Cover%202%7D%7B%7B%28x%20-%20u%29%7D%5ET%7D%7B%5CSigma%20%5E%7B%20-%201%7D%7D%28x%20-%20u%29%7D%7D%24%24) - 同样，`|Σ|`越小，`p(x)`越尖 - 例如： ![enter description here][53]， 表示x1,x2**正相关**，即x1越大，x2也就越大，如下图，也就可以将红色的异常点检查出了 ![enter description here][54] 若： ![enter description here][55]， 表示x1,x2**负相关**- 实现代码： 多元高斯分布函数def multivariateGaussian(X,mu,Sigma2): k = len(mu) if (Sigma2.shape[0]&gt;1): Sigma2 = np.diag(Sigma2) ‘’’多元高斯分布函数’’’ X = X-mu argu = (2np.pi)**(-k/2)np.linalg.det(Sigma2)*(-0.5) p = argunp.exp(-0.5np.sum(np.dot(X,np.linalg.inv(Sigma2))X,axis=1)) # axis表示每行 return p``` 6、单元和多元高斯分布特点 单元高斯分布 人为可以捕捉到feature之间的关系时可以使用 计算量小 多元高斯分布 自动捕捉到相关的feature 计算量大，因为： m&gt;n或Σ可逆时可以使用。（若不可逆，可能有冗余的x，因为线性相关，不可逆，或者就是m&lt;n） 7、程序运行结果 显示数据 等高线 异常点标注","tags":[{"name":"Python","slug":"Python","permalink":"http://lawlite.com/tags/Python/"},{"name":"机器学习","slug":"机器学习","permalink":"http://lawlite.com/tags/机器学习/"}]},{"title":"Python科学计算","date":"2016-11-09T14:25:43.000Z","path":"2016/11/09/Python科学计算/","text":"一、Numpy1、Numpy特征和导入 （1）用于多维数组的第三方Python包 （2）更接近于底层和硬件 (高效) （3）专注于科学计算 (方便) （4）导入包：import numpy as np 2、list转为数组 （1）a = np.array([0,1,2,3]) （2）输出为：[0 1 2 3] （3）数据类型：&lt;type &#39;numpy.ndarray&#39;&gt; 3、一维数组 （1）a = np.array([1,2,3,4])属性a.ndim–&gt;维度为1a.shape–&gt;形状，返回(4,)len(a)–&gt;长度，4 （2）访问数组a[1:5:2]下标1-5，下标关系+2 （3）逆序 a[::-1] 4、多维数组 （1）二维：a = np.array([[0,1,2,3],[1,2,3,4]])输出为： [[0 1 2 3] [1 2 3 4]]a.ndm –&gt;2a.shape –&gt;(2,4)–&gt;行数，列数len(a) –&gt;2–&gt;第一维大小 （2）三维：a = np.array([[[0],[1]],[[2],[4]]])a.shape–&gt;(2,2,1) 5、用函数创建数组 （1）np.arange() a = np.arange(0, 10)b = np.arange(10)c = np.arange(0,10,2)输出： [0 1 2 3 4 5 6 7 8 9][0 1 2 3 4 5 6 7 8 9][0 2 4 6 8] （2）np.linspace(start, stop, num=50, endpoint=True, retstep=False, dtype=None)等距离产生num个数 （3）np.logspace(start, stop, num=50, endpoint=True, base=10.0, dtype=None)以log函数取 6、常用数组 （1）a = np.ones((3,3))输出： [[ 1. 1. 1.][ 1. 1. 1.][ 1. 1. 1.]] （2）np.zeros((3,3)) （3）np.eye(2)单位矩阵 （4）np.diag([1,2,3],k=0)对角矩阵，k为对角线的偏移 7、随机数矩阵 （1）a = np.random.rand(4)输出：[ 0.99890402 0.41171695 0.40725671 0.42501804]范围在[0,1]之间 （2）a = np.random.randn(4) Gaussian函数， （3）生成100个0-m的随机数: [t for t in [np.random.randint(x-x, m) for x in range(100)]] 也可以12m_arr = np.arange(0,m) # 生成0-m-1np.random.shuffle(m_arr) # 打乱m_arr顺序 然后取前100个即可 8、查看数据类型 （1）a.dtype 9、数组复制 （1）共享内存123a = np.array([1,2,3,4,5])b = aprint np.may_share_memory(a,b) 输出：True说明使用的同一个存储区域，修改一个数组同时另外的也会修改 （2）不共享内存b = a.copy() 10、布尔型 （1）1234a = np.random.random_integers(0,20,5)print aprint a%3==0print a[a % 3 == 0] 输出： [14 3 6 15 4] [False True True True False] [ 3 6 15] 11、中间数、平均值 （1）中间数np.median(a) （2）平均值np.mean(a), 若是矩阵，不指定axis默认求所有元素的均值 axis=0,求列的均值 axis=1，求行的均值 12、矩阵操作 （1）乘积np.dot(a,b)123a = np.array([[1,2,3],[2,3,4]])b = np.array([[1,2],[2,3],[2,2]])print np.dot(a,b) 或者使用np.matrix()生成矩阵，相乘需要满足矩阵相乘的条件 （2）内积np.inner(a,b)行相乘 （3）逆矩阵np.linalg.inv(a) （4）列的最大值np.max(a[:,0])–&gt;返回第一列的最大值 （5）每列的和np.sum(a,0) （6）每行的平均数np.mean(a,1) （7）求交集p.intersect1d(a,b)，返回一维数组 （8）转置：np.transpose(a) （9）两个矩阵对应对应元素相乘（点乘）：a*b 13、文件操作 （1）保存：tofile()123a = np.arange(10)a.shape=2,5a.tofile(&quot;test.bin&quot;) 读取：（需要注意指定保存的数据类型）12a = np.fromfile(&quot;test.bin&quot;,dtype=np.int32)print a （2）保存：np.save(&quot;test&quot;,a)–&gt;会保存成test.npy文件读取：a = np.load(&quot;test&quot;) 14、组合两个数组 （1）垂直组合 1234a = np.array([1,2,3])b = np.array([[1,2,3],[4,5,6]])c = np.vstack((b,a)) （2）水平组合 1234a = np.array([[1,2],[3,4]])b = np.array([[1,2,3],[4,5,6]])c = np.hstack((a,b)) 15、读声音Wave文件 （1）wave 1234567891011121314151617181920212223242526272829import wavefrom matplotlib import pyplot as pltimport numpy as np# 打开WAV文档f = wave.open(r&quot;c:\\WINDOWS\\Media\\ding.wav&quot;, &quot;rb&quot;)# 读取格式信息# (nchannels, sampwidth, framerate, nframes, comptype, compname)params = f.getparams()nchannels, sampwidth, framerate, nframes = params[:4]# 读取波形数据str_data = f.readframes(nframes)f.close()#将波形数据转换为数组wave_data = np.fromstring(str_data, dtype=np.short)wave_data.shape = -1, 2wave_data = wave_data.Ttime = np.arange(0, nframes) * (1.0 / framerate)# 绘制波形plt.subplot(211) plt.plot(time, wave_data[0])plt.subplot(212) plt.plot(time, wave_data[1], c=&quot;g&quot;)plt.xlabel(&quot;time (seconds)&quot;)plt.show() 16、where （1）找到y数组中=1的位置：np.where(y==1) 17、np.ravel(y) 将二维的转化为一维的，eg:(5000,1)--&gt;(5000,) 18、ndarray.flat函数 将数据展开对应的数组，可以进行访问 应用：0/1映射123456def dense_to_one_hot(label_dense,num_classes): num_labels = label_dense.shape[0] index_offset = np.arange(num_labels)*num_classes labels_one_hot = numpy.zeros((num_labels, num_classes)) labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1 return labels_one_hot 19、数组访问 X = np.array([[1,2],[3,4]]) X[0:1]和X[0:1,:]等价，都是系那是第一行数据 20、np.c_() 按照第二维度，即列拼接数据 np.c_[np.array([[1,2,3]]), 0, 0, np.array([[4,5,6]])]输出：array([[1, 2, 3, 0, 0, 4, 5, 6]]) 两个列表list拼接，长度要一致 np.c_[[1,2,3],[2,3,4]] np.c_[range(1,5),range(2,6)] 二、Matplotlib1、关于pyplot （1）matplotlib的pyplot子库提供了和matlab类似的绘图API，方便用户快速绘制2D图表。 （2）导入包：from matplotlib import pyplot as plt 2、绘图基础 （1）sin和cos 1234567x = np.linspace(-np.pi, np.pi,256,endpoint=True)C,S = np.cos(x),np.sin(x)plt.plot(x,C)plt.plot(x,S)plt.xlabel(&quot;x&quot;)plt.ylabel(&quot;y&quot;)plt.show() （2）指定绘图的大小 plt.figure(figsize=(8,6), dpi=80) （3）指定线的颜色、粗细和类型 plt.plot(x,C,color=”blue”,linewidth=2.0,linestyle=”-“,label=”cos”) 蓝色、宽度、连续、label（使用legend会显示这个label） （4）指定x坐标轴范围 plt.xlim(-4.0,4.0) （5）设置y抽刻度间隔plt.yticks(np.linspace(-1, 1, 15, endpoint=True)) （6）显示图例 plt.legend(loc=”upper left”) 显示在左上方 （7）一个figure上画多个图subplot方式 plt.subplot(1, 2, 1) plt.subplot(1, 2, 2) 例如：plt.subplot(m, n, p) 代表图共有的m行，n列，第p个图 p是指第几个图，横向数 上面代表有一行，两个图 [更详细解释]：231,232,233表示第一行的1,2,3个位置，接着的223表示把整个矩形分成4分，所以第3个位置就是第二行的第一个位置，但是相比第一行占了1.5列（每次subplot划分都是整个图重新划分） （8）一个figure上画多个图，axes方式 plt.axes([.1, .1, .8, .8]) plt.axes([.2, .2, .3, .3]) （9）填充 plt.fill_between(x, y1, y2=0, where=None, interpolate=False, step=None, hold=None, data=None) eg: 12plt.fill_between(X, 1, C+1, C+1&gt;1,color=&quot;red&quot;)plt.fill_between(X, 1, C+1, C+1&lt;1,color=&quot;blue&quot;) 3、散点图 （1） plt.scatter(X,Y) 4、条形图 （1） plt.bar(X, Y, facecolor=&quot;red&quot;, edgecolor=&quot;blue&quot; ) 填充颜色为facecolor,边界颜色为edgecolor 5、等高线图 （1）只显示等高线contour （2）显示表面contourf （3）注意三维图要用到meshgrid转化为网格12345678910111213def f(x,y): return (1 - x / 2 + x**5 + y**3) * np.exp(-x**2 -y**2)n = 256x = np.linspace(-3,3,n)y = np.linspace(-3,3,n)X,Y = np.meshgrid(x,y)plt.contourf(X,Y,f(X,Y),alpha=.5)C = plt.contour(X,Y,f(X, Y),colors=&quot;black&quot;,linewidth=.5)plt.clabel(C)plt.show() 6、显示图片imshow （1）123456789def f(x,y):return (1 - x / 2 + x ** 5 + y ** 3 ) * np.exp(-x ** 2 - y ** 2)n = 10x = np.linspace(-3, 3, 3.5 * n)y = np.linspace(-3, 3, 3.0 * n)X, Y = np.meshgrid(x, y)z = f(X,Y)plt.imshow(z)plt.show() 7、饼图pie （1）传入一个序列12345plt.figure(figsize=(8,8))n = 20Z = np.arange(10)plt.pie(Z)plt.show() 8、三维表面图* （1）需要导入包：from mpl_toolkits.mplot3d import Axes3D （2）1234567891011fig = plt.figure()ax = Axes3D(fig)X = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y)R = np.sqrt(X ** 2 + Y ** 2)Z = np.sin(R)ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=plt.cm.hot)ax.contourf(X, Y, Z, zdir=&apos;z&apos;, offset=-2, cmap=plt.cm.hot)ax.set_zlim(-2, 2)plt.show() 9、legend显示问题 （1）12345p1, = plt.plot(np.ravel(X[pos,0]),np.ravel(X[pos,1]),&apos;ro&apos;,markersize=8)p2, = plt.plot(np.ravel(X[neg,0]),np.ravel(X[neg,1]),&apos;g^&apos;,markersize=8)plt.xlabel(&quot;X1&quot;)plt.ylabel(&quot;X2&quot;)plt.legend([p1,p2],[&quot;y==1&quot;,&quot;y==0&quot;]) 注意 p1后要加上,逗号，里面的数据要是一维的，使用np.ravel()转化一下 10、显示网格 （1）plt.grid(True, linestyle = &quot;-.&quot;, color = &quot;b&quot;, linewidth = &quot;1&quot;) 11、显示正方形的坐标区域 （1）plt.axis(&#39;square&#39;) 三、Scipy1、 Scipy特征 （1）内置了图像处理， 优化，统计等等相关问题的子模块 （2）scipy 是Python科学计算环境的核心。 它被设计为利用 numpy 数组进行高效的运行。从这个角度来讲，scipy和numpy是密不可分的。 2、文件操作io （1）导包：from scipy import io as spio （2）保存mat格式文件 spio.savemat(&quot;test.mat&quot;, {&#39;a&#39;:a}) （3）加载mat文件 data = spio.loadmat(&quot;test.mat&quot;) 访问值：data[‘a’]–&gt;相当于map （4）读取图片文件导包：from scipy import misc读取：data = misc.imread(&quot;123.png&quot;)[注1]：与matplotlib中plt.imread(&#39;fname.png&#39;)类似[注2]：执行misc.imread时可能提醒不存在这个模块，那就安装pillow的包 3、线性代数操作linalg （1）求行列式det res = linalg.det(a) （2）求逆矩阵inv res = linalg.inv(a) 若是矩阵不可逆，则会抛异常LinAlgError: singular matrix （3）奇异值分解svd u,s,v = linalg.svd(a) [注1]：s为a的特征值（一维），降序排列， [注2]：a = u*s*v’（需要将s转换一下才能相乘）12t = np.diag(s)print u.dot(t).dot(v) 4、梯度下降优化算法 （1）fmin_bfgs 1234def f(x): return x**2-2*xinitial_x = 0optimize.fmin_bfgs(f,initial_x) [注]：initial_x为初始点（此方法可能会得到局部最小值） （2）fmin()、fmin_cg等等方法 5、拟合（最小二乘法） （1）curve_fit123456789101112131415161718#产生数据def f(x): return x**2 + 10*np.sin(x)xdata = np.linspace(-10, 10, num=20)ydata = f(xdata)+np.random.randn(xdata.size)plt.scatter(xdata, ydata, linewidths=3.0, edgecolors=&quot;red&quot;)#plt.show()#拟合def f2(x,a,b): return a*x**2 + b*np.sin(x)guess = [2,2]params, params_covariance = optimize.curve_fit(f2, xdata, ydata, guess)#画出拟合的曲线x1 = np.linspace(-10,10,256)y1 = f2(x1,params[0],params[1])plt.plot(x1,y1)plt.show() 6、统计检验 （1）T-检验stats.ttest_ind123a = np.random.normal(0, 1, size=10)b = np.random.normal(1, 1, size=10)print stats.ttest_ind(a, b) 输出：(-2.6694785119868358, 0.015631342180817954)后面的是概率p: 两个过程相同的概率。如果其值接近1，那么两个过程几乎可以确定是相同的，如果其值接近0，那么它们很可能拥有不同的均值。 7、插值 （1）导入包：from scipy.interpolate import interp1d12345678910111213141516#产生一些数据x = np.linspace(0, 1, 10)y = np.sin(2 * np.pi * x)computed_time = np.linspace(0, 1, 50)#线性插值linear_interp = interp1d(x, y)linear_results = linear_interp(computed_time)#三次方插值cubic_interp = interp1d(x, y, kind=&apos;cubic&apos;)cubic_results = cubic_interp(computed_time)#作图plt.plot(x, y, &apos;o&apos;, ms=6, label=&apos;y&apos;)plt.plot(computed_time, linear_results, label=&apos;linear interp&apos;)plt.plot(computed_time, cubic_results, label=&apos;cubic interp&apos;)plt.legend()plt.show() 8、求解非线性方程组 （1）optimize中的fsolve1234567from scipy.optimize import fsolvedef func(x): x0,x1,x2 = x.tolist() return [5*x1-25,5*x0*x0-x1*x2,x2*x0-27]initial_x = [1,1,1]result = fsolve(func, initial_x)print result 四、pandas1、pandas特征与导入 （1）包含高级的数据结构和精巧的工具 （2）pandas建造在NumPy之上 （3）导入：12from pandas import Series, DataFrameimport pandas as pd 2、pandas数据结构（1）Series 一维的类似的数组对象 包含一个数组的数据（任何NumPy的数据类型）和一个与数组关联的索引 不指定索引：a = Series([1,2,3]) ，输出为 1230 11 22 3 包含属性a.index,a.values，对应索引和值 指定索引：a = Series([1,2,3],index=[&#39;a&#39;,&#39;b&#39;,&#39;c&#39;])可以通过索引访问a[&#39;b&#39;] 判断某个索引是否存在：&#39;b&#39; in a 通过字典建立Series12dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;print Series(dict) 输出：1234america 30china 10indian 20dtype: int64 判断哪个索引值缺失：1234dict = &#123;&apos;china&apos;:10,&apos;america&apos;:30,&apos;indian&apos;:20&#125;state = [&apos;china&apos;,&apos;america&apos;,&apos;test&apos;]a = Series(dict,state)print a.isnull() 输出：（test索引没有对应值）1234china Falseamerica Falsetest Truedtype: bool 在算术运算中它会自动对齐不同索引的数据123a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])b = Series([10,20],[&apos;test&apos;,&apos;china&apos;])print a+b 输出：123china 30test 30dtype: int64 指定Series对象的name和index的name属性1234a = Series([10,20],[&apos;china&apos;,&apos;test&apos;])a.index.name = &apos;state&apos;a.name = &apos;number&apos;print a 输出：1234statechina 10test 20Name: number, dtype: int64 （2）DataFrame Datarame表示一个表格，类似电子表格的数据结构 包含一个经过排序的列表集（按列名排序） 每一个都可以有不同的类型值（数字，字符串，布尔等等） DataFrame在内部把数据存储为一个二维数组的格式，因此你可以采用分层索引以表格格式来表示高维的数据 创建： 通过字典12345data = &#123;&apos;state&apos;: [&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;d&apos;], &apos;year&apos;: [2000, 2001, 2002, 2001, 2002], &apos;pop&apos;: [1.5, 1.7, 3.6, 2.4, 2.9]&#125;frame = DataFrame(data)print frame 输出：(按照列名排好序的[若是手动分配列名，会按照你设定的]，并且索引会自动分配) 123456 pop state year0 1.5 a 20001 1.7 b 20012 3.6 c 20023 2.4 d 20014 2.9 d 2002 访问 列：与Series一样，通过列名访问：frame[&#39;state&#39;]或者frame.state 行：ix 索引成员（field），frame.ix[2]，返回每一列的第3行数据 赋值：frame2[&#39;debt&#39;] = np.arange(5.)，若没有debt列名，则会新增一列 删除某一列：del frame2[&#39;eastern&#39;] 像Series一样， values 属性返回一个包含在DataFrame中的数据的二维ndarray 返回所有的列信息：frame.columns 转置：frame2.T （3）索引对象 pandas的索引对象用来保存坐标轴标签和其它元数据（如坐标轴名或名称） 索引对象是不可变的，因此不能由用户改变 创建index = pd.Index([1,2,3]) 常用操作 append–&gt;链接额外的索引对象，产生一个新的索引 diff –&gt;计算索引的差集 intersection –&gt;计算交集 union –&gt;计算并集 isin –&gt;计算出一个布尔数组表示每一个值是否包含在所传递的集合里 delete –&gt;计算删除位置i的元素的索引 drop –&gt;计算删除所传递的值后的索引 insert –&gt;计算在位置i插入元素后的索引 is_monotonic –&gt;返回True，如果每一个元素都比它前面的元素大或相等 is_unique –&gt;返回True，如果索引没有重复的值 unique –&gt;计算索引的唯一值数组 3、重新索引reindex（1）Series （1）重新排列 123a = Series([2,3,1],index=[&apos;b&apos;,&apos;a&apos;,&apos;c&apos;])b = a.reindex([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;])print b （2）重新排列，没有的索引补充为0,b=a.reindex([&#39;a&#39;,&#39;b&#39;,&#39;c&#39;,&#39;d&#39;],fill_value=0) （3）重建索引时对值进行内插或填充123a = Series([&apos;a&apos;,&apos;b&apos;,&apos;c&apos;],index=[0,2,4])b = a.reindex(range(6),method=&apos;ffill&apos;)print b 输出：12345670 a1 a2 b3 b4 c5 cdata_linkdtype: object method的参数ffill或pad—-&gt;前向（或进位）填充bfill或backfill—-&gt;后向（或进位）填充 （3）DataFrame 与Series一样，reindex index 还可以reindex column列，frame.reindex(columns=[&#39;a&#39;,&#39;b&#39;]) 4、从一个坐标轴删除条目（1）Series a.drop([&#39;a&#39;,&#39;b&#39;]) 删除a，b索引项（2）DataFrame 索引项的删除与Series一样 删除column—&gt;a.drop([&#39;one&#39;], axis=1) 删除column名为one的一列 5、索引，挑选和过滤（1）Series 可以通过index值或者整数值来访问数据，eg：对于a = Series(np.arange(4.), index=[&#39;a&#39;, &#39;b&#39;, &#39;c&#39;, &#39;d&#39;])，a[&#39;b&#39;]和a[1]是一样的 使用标签来切片和正常的Python切片并不一样，它会把结束点也包括在内12a = Series(np.arange(4.), index=[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;])print a[&apos;b&apos;:&apos;c&apos;] 输出包含c索引对应的值 （2）DataFrame 显示前两行：a[:2] 布尔值访问：a[a[&#39;two&#39;]&gt;5] 索引字段 ix 的使用 index为2，column为’one’和’two’—&gt;a.ix[[2],[&#39;one&#39;,&#39;two&#39;]] index为2的一行：a.ix[2] 6、DataFrame和Series运算 （1）DataFrame每一行都减去一个Series12345a = pd.DataFrame(np.arange(16).reshape(4,4),index=[0,1,2,3],columns=[&apos;one&apos;, &apos;two&apos;,&apos;three&apos;,&apos;four&apos;])print ab = Series([0,1,2,3],index=[&apos;one&apos;,&apos;two&apos;,&apos;three&apos;,&apos;four&apos;])print bprint a-b 输出：123456789101112131415 one two three four0 0 1 2 31 4 5 6 72 8 9 10 113 12 13 14 15one 0two 1three 2four 3dtype: int64 one two three four0 0 0 0 01 4 4 4 42 8 8 8 83 12 12 12 12 7、读取文件 （1）csv文件pd.read_csv(r&quot;data/train.csv&quot;)，返回的数据类型是DataFrame类型 8、查看DataFrame的信息 （1）train_data.describe()eg:123456789 PassengerId Survived Pclass Age SibSp \\count 891.000000 891.000000 891.000000 714.000000 891.000000 mean 446.000000 0.383838 2.308642 29.699118 0.523008 std 257.353842 0.486592 0.836071 14.526497 1.102743 min 1.000000 0.000000 1.000000 0.420000 0.000000 25% 223.500000 0.000000 2.000000 20.125000 0.000000 50% 446.000000 0.000000 3.000000 28.000000 0.000000 75% 668.500000 1.000000 3.000000 38.000000 1.000000 max 891.000000 1.000000 3.000000 80.000000 8.000000 9、定位到一列并替换 df.loc[df.Age.isnull(),&#39;Age&#39;] = 23 #&#39;Age&#39;列为空的内容补上数字23 10、将分类变量转化为指示变量get_dummies() 12s = pd.Series(list(&apos;abca&apos;))pd.get_dummies(s) 12345 a b c0 1 0 01 0 1 02 0 0 13 1 0 0 11、list和string互相转化 string转list 1234&gt;&gt;&gt; str = &apos;abcde&apos;&gt;&gt;&gt; list = list(str)&gt;&gt;&gt; list[&apos;a&apos;, &apos;b&apos;, &apos;c&apos;, &apos;d&apos;, &apos;e&apos;] list转string 123&gt;&gt;&gt; str_convert = &apos;,&apos;.join(list)&gt;&gt;&gt; str_convert&apos;a,b,c,d,e&apos; 12、删除原来的索引，重新从0-n索引 x = x.reset_index(drop=True) 13、apply函数 DataFrame.apply(func, axis=0, broadcast=False, raw=False, reduce=None, ….. df.apply(numpy.sqrt) # returns DataFrame 等价==》df.apply(lambda x : numpy.sqrt(x))==&gt;使用更灵活 df.apply(numpy.sum, axis=0) # equiv to df.sum(0) df.apply(numpy.sum, axis=1) # equiv to df.sum(1) 13、re.search().group()函数 re.search(pattern, string, flags=0) group(num=0)函数返回匹配的字符，默认num=0,可以指定多个组号，例如group(0,1) 14、pandas.cut()函数 pandas.cut(x, bins, right=True, labels=None, retbins=False, precision=3, include_lowest=False) x为以为数组 bins可以是int值或者序列 若是int值就根据x分为bins个数的区间 若是序列就是自己指定的区间 right包含最右边的区间，默认为True labels 数组或者一个布尔值 若是数组，需要与对应bins的结果一致 若是布尔值False，返回bin中的一个值 eg:pd.cut(full[“FamilySize”], bins=[0,1,4,20], labels=[0,1,2]) 15、添加一行数据 定义空的dataframe: data_process = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;]) 定义一行新的数据，new = pd.DataFrame(columns=[&#39;route&#39;,&#39;date&#39;,&#39;1&#39;,&#39;2&#39;,&#39;3&#39;,&#39;4&#39;,&#39;5&#39;,&#39;6&#39;,&#39;7&#39;,&#39;8&#39;,&#39;9&#39;,&#39;10&#39;,&#39;11&#39;,&#39;12&#39;],index=[j]) 这里index可以随意设置，若是想指定就指定 添加：data_process = data_process.append(new, ignore_index=True)， 注意这里是data_process = data_process....... 五、scikit-learn1、手写数字识别（SVM）1234567891011121314151617181920212223242526272829from sklearn import datasetsfrom sklearn import svmimport numpy as npfrom matplotlib import pyplot as plt&apos;&apos;&apos;使用sciki-learn中的数据集，一般有data,target,DESCR等属性属性&apos;&apos;&apos;digits = datasets.load_digits() #加载scikit-learn中的数据集clf = svm.SVC(gamma=0.001,C=100) #使用支持向量机进行分类，gamma为核函数的系数clf.fit(digits.data[:-4],digits.target[:-4]) #将除最后4组的数据输入进行训练predict = clf.predict(digits.data[-4:]) #预测最后4组的数据，[-4:]表示最后4行所有数据，而[-4,:]表示倒数第4行数据print &quot;预测值为：&quot;,predictprint &quot;真实值：&quot;,digits.target[-4:]#显示最后四个图像plt.subplot(2,2,1)plt.imshow(digits.data[-4,:].reshape(8,8))plt.subplot(2,2,2)plt.imshow(digits.data[-3,:].reshape(8,8))plt.subplot(2,2,3)plt.imshow(digits.data[-2,:].reshape(8,8))plt.subplot(2,2,4)plt.imshow(digits.data[-1,:].reshape(8,8))plt.show() svm的参数参数解释： （1）C: 目标函数的惩罚系数C，用来平衡分类间隔margin和错分样本的，default C = 1.0； （2）kernel：参数选择有RBF, Linear, Poly, Sigmoid, 默认的是”RBF”; （3）degree：if you choose ‘Poly’ in param 2, this is effective, degree决定了多项式的最高次幂； （4）gamma：核函数的系数(‘Poly’, ‘RBF’ and ‘Sigmoid’), 默认是gamma = 1 / n_features; （5）coef0：核函数中的独立项，’RBF’ and ‘Poly’有效； （6）probablity: 可能性估计是否使用(true or false)； （7）shrinking：是否进行启发式； （8）tol（default = 1e - 3）: svm结束标准的精度; （9）cache_size: 制定训练所需要的内存（以MB为单位）； （10）class_weight:每个类所占据的权重，不同的类设置不同的惩罚参数C,缺省的话自适应； （11）verbose: 跟多线程有关，不大明白啥意思具体； （12）max_iter: 最大迭代次数，default = 1000， if max_iter = -1, no limited; （13）decision_function_shape ： ‘ovo’ 一对一, ‘ovr’ 多对多 or None 无, default=None （14）random_state ：用于概率估计的数据重排时的伪随机数生成器的种子。 2、保存训练过的模型 from sklearn.externals import joblib joblib.dump(clf, &quot;digits.pkl&quot;) #将训练的模型保存成digits.pkl文件 加载模型：clf = joblib.load(&quot;digits.pkl&quot;)其余操作数据即可，预测 3、鸢尾花分类（svm，分离出测试集）1234567891011121314151617181920from sklearn import datasetsfrom sklearn.cross_validation import train_test_splitfrom sklearn.svm import SVCimport numpy as np&apos;&apos;&apos;加载scikit-learn中的鸢尾花数据集&apos;&apos;&apos;#加载鸢尾花数据集iris = datasets.load_iris()iris_data = iris.data; #相当于Xiris_target = iris.target; #对应的label种类，相当于yx_train,x_test,y_train,y_test = train_test_split(iris_data,iris_target,test_size=0.2) #将数据分成训练集x_train和测试集x_test，测试集占总数据的0.2model = SVC().fit(x_train,y_train); #使用svm在训练集上拟合predict = model.predict(x_test) #在测试集上预测right = sum(predict == y_test) #求预测正确的个数print (&apos;测试集准确率：%f%%&apos;%(right*100.0/predict.shape[0])) #求在测试集上预测的正确率，shape[0]返回第一维的长度，即数据个数 [另：留一验证法]：–&gt;每次取一条数据作为测试集，其余作为训练集123456789101112131415161718192021222324from sklearn import datasetsfrom sklearn.svm import SVCimport numpy as npdef data_svc_test(data,target,index): x_train = np.vstack((data[0:index],data[index+1:-1]))#除第index号之外的 数据为训练集 x_test = data[index].reshape(1,-1) #第index号数据为测试集，reshape(1,-1)的作用是只有一条数据时，使用reshap e(1,-1)，否则有个过时方法的警告 y_train = np.hstack((target[0:index],target[index+1:-1])) y_test = target[index] model = SVC().fit(x_train,y_train) #建立SVC模型 predict = model.predict(x_test) return predict == y_test #返回结果是否预测正确#读取数据iris = datasets.load_iris()iris_data = iris.datairis_target = iris.targetm = iris_target.shape[0]right = 0;for i in range(0,m): right += data_svc_test(iris_data,iris_target,i)print (&quot;%f%%&quot;%(right*100.0/m)) 4、房价预测(SVR–&gt;支持向量回归)1234567891011121314151617181920212223from sklearn import datasetsfrom sklearn.svm import SVR #引入支持向量回归所需的SVR模型from sklearn.cross_validation import train_test_splitfrom sklearn.preprocessing import StandardScalerimport numpy as np#加载数据house_dataset = datasets.load_boston()house_data = house_dataset.datahouse_price = house_dataset.target#数据预处理--&gt;归一化x_train,x_test,y_train,y_test = train_test_split(house_data,house_price,test_size=0.2) scaler = StandardScaler()scaler.fit(x_train)x_train = scaler.transform(x_train) #训练集x_test = scaler.transform(x_test) #测试集#回归，预测model = SVR().fit(x_train,y_train) #使用SVR回归拟合predict = model.predict(x_test) #预测result = np.hstack((y_test.reshape(-1,1),predict.reshape(-1,1))) #reshape(-1,1)所有行转为1列向量print(result) 六、sk-learn模型总结0、数据处理（1）均值归一化：from sklearn.preprocessing import StandardScaler scaler = StandardScaler() scaler.fit(X_train) X_train = scaler.transform(X_train) （2）分割数据：from sklearn.cross_validation import train_test_split x_train,x_test,y_train,y_test = train_test_split(iris_data,iris_target,test_size=0.2) 1、线性模型from sklearn import linear_model（1）逻辑回归模型 linear_model.LogisticRegression() 重要参数 C：正则化作用，默认值1.0，值越小，正则化作用越强 max_iter：最大梯度下降执行次数，默认值100 tol：停止执行的容忍度，默认值1e-4 重要返回值 coef_：对应feature的系数 2、svm模型from sklearn import svm（1）分类模型 svm.SVC() 重要参数 kernel：使用的核函数，默认是rbf径向基函数，还有linear，poly，sigmoid ，precomputed核函数 C：正则化作用，默认值1.0，值越大，margin越大 tol：停止执行的容忍度，默认值1e-4 gamma：为核函数的系数，值越大拟合的越好，默认是1/feature的个数 degree：对应poly核函数 重要返回值","tags":[{"name":"Python","slug":"Python","permalink":"http://lawlite.com/tags/Python/"},{"name":"机器学习","slug":"机器学习","permalink":"http://lawlite.com/tags/机器学习/"}]},{"title":"搭建自己的VPN","date":"2016-11-05T09:33:50.000Z","path":"2016/11/05/搭建自己的VPN/","text":"一、首先租一个服务器 1、租一个香港的服务器，这里我选的按量付费，如果不使用了释放就可以了，按小时收费的，不过要求你账户上要多于100块钱。 2、操作系统选择的64位CentOS6.5，CentOS7以上下面的命令会有所不同。 3、创建成功后管理控制台会有公网和私网两个ip地址 二、配置VPN 1、安装ppp和pptpd: 1yum install ppp pptpd 2、配置DNS/etc/ppp/options.pptpd文件中的ms-dns配置为： 12ms-dns 8.8.8.8ms-dns 8.8.4.4 3、配置IP/etc/pptpd.conf文件中最后加入： 12localip 192.168.0.1remoteip 192.168.0.2-254 4、配置VPN用户名和密码/etc/ppp/chap-secrets文件中加入： 1userName pptpd password * 就是userName位置写上你的用户名，password位置写上你的密码 5、配置IP转发/etc/sysctl.conf文件中net.ipv4.ip_forward = 0改为1net.ipv4.ip_forward = 1 然后执行：sysctl -p使其生效 三、配置防火墙 1、加入防火墙规则 123iptables -A INPUT -p TCP -i eth1 --dport 1723 --sport 1024:65534 -j ACCEPTiptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADEiptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356 注意这里指定的网卡是eth1，其对应外网的网卡，否则能够连上VPN，但是是访问不了外网的。 VPN默认的端口是1723 2、保存防火墙配置，启动pptpd，让其开机自启动 1234service iptables saveservice iptables restartservice pptpd start chkconfig pptpd on 四、测试1、window或手机等连接 对应外网IP，设置的用户名和密码 速度是可以的， 我也测试了一下国外的服务器，速度非常慢，还不如免费的VPN软件， 五、shell脚本 1、我写了一个简单的shell脚本放在了github上，github地址：https://github.com/lawlite19/Script 2、运行步骤如下： 下载脚本：wget https://raw.githubusercontent.com/lawlite19/Script/master/shell/vpn_setup.sh 添加执行权限：chmod +x vpn_setup.sh 执行即可：./vpn_setup.sh3、完整代码：123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106#!/bin/bash# Author: Wang Yongzhi(bob)# Date: 2016.11.16echo -e &quot;-----------------------------------------------&quot;echo -e &quot;| Setup VPN... |&quot;echo -e &quot;-----------------------------------------------\\n&quot;# Step 1:install ppp and pptpdyum install -y pppyum install -y pptpdif [ $? -eq 0 ]then echo -e &quot;install ppp and pptpd Success!\\n&quot;else echo -e &quot;Sorry! install ppp and pptpd Failed!\\n&quot; exit 0fi# Step 2:configure pptpd DNSsed -i -e &apos;/#ms-dns 10.0.0.1/a\\ms-dns 8.8.8.8&apos; /etc/ppp/options.pptpdsed -i -e &apos;/#ms-dns 10.0.0.2/a\\ms-dns 8.8.4.4&apos; /etc/ppp/options.pptpdif [ $? -eq 0 ]then echo -e &quot;Configure DNS Success!\\n&quot;else echo -e &quot;Configure DNS Failed!\\n&quot; exit 0fi# Step 3:configure pptpd IPecho localip 192.168.0.1 &gt;&gt; /etc/pptpd.confecho remoteip 192.168.0.2-254 &gt;&gt; /etc/pptpd.confif [ $? -eq 0 ]then echo -e &quot;Configure pptpd IP Success!\\n&quot;else echo -e &quot;Configure pptpd IP Failed!\\n&quot; exit 0fi# Step 4: configure VPN userName and passwordwhile truedo read -p &quot;Please input userName:&quot; userName read -p &quot;Please input passwd: &quot; Passwd echo $userName pptpd $Passwd \\* &gt;&gt; /etc/ppp/chap-secrets read -p &quot;continue?y/N: &quot; flag if [ $flag = &quot;n&quot; -o $flag = &quot;N&quot; ] then break fidone# Step 5: configure forwardingsed -i &apos;s/net.ipv4.ip_forward = 0/net.ipv4.ip_forward = 1/g&apos; /etc/sysctl.confif [ $? -eq 0 ]then echo -e &quot;Configure forwarding Success!\\n&quot;else echo -e &quot;Configure forwarding Failed\\n&quot; exit 0fisysctl -p# Step 6: configure iptables#EXTIF=$(ifconfig | head -n 1 | grep -v lo | cut -d &apos; &apos; -f 1)iptables -A INPUT -p TCP -i eth1 --dport 1723 --sport 1024:65534 -j ACCEPTiptables -t nat -A POSTROUTING -o eth1 -s 192.168.0.0/24 -j MASQUERADEiptables -I FORWARD -p tcp --syn -i ppp+ -j TCPMSS --set-mss 1356# Step 7: configure when start server to start pptpd and iptablesservice iptables saveservice iptables restartservice pptpd start chkconfig pptpd onecho -e &quot;Complete! Now you can connect the VPN throuth your computer or phone!\\n&quot;echo &quot; ***** *****&quot;echo &quot; ********* *********&quot;echo &quot; ************* *************&quot;echo &quot; *****************************&quot;echo &quot; *****************************&quot;echo &quot; *****************************&quot;echo &quot; ***************************&quot;echo &quot; ***********************&quot;echo &quot; *******************&quot;echo &quot; ***************&quot;echo &quot; ***********&quot;echo &quot; *******&quot;echo &quot; ***&quot;echo &quot; *&quot; 六、总结 最初是在租了一个国外的服务器测试的，没有问题，但是后来租用香港的服务器就出现的了错误，同样的系统、同样的配置，后来查看内网绑定的是网卡eth0,外网绑定的是网卡eth1，而我防火墙里设置的是内网的网卡eth0。而国外的那个服务器只要一个网卡，所以没有问题。另外练练shell脚本。","tags":[{"name":"翻墙","slug":"翻墙","permalink":"http://lawlite.com/tags/翻墙/"}]}]
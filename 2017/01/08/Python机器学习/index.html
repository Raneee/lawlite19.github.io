<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://lawlite.com">
  <title>Python机器学习 | Lawlite的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="机器学习算法Python实现说明 github地址：https://github.com/lawlite19/MachineLearning_Python  目录 机器学习算法Python实现 一、线性回归 1、代价函数 2、梯度下降算法 3、均值归一化 4、最终运行结果 5、使用scikit-learn库中的线性模型实现   二、逻辑回归 1、代价函数 2、梯度 3、正则化 4、S型函数（即）">
<meta name="keywords" content="Python,机器学习">
<meta property="og:type" content="article">
<meta property="og:title" content="Python机器学习">
<meta property="og:url" content="http://lawlite.com/2017/01/08/Python机器学习/index.html">
<meta property="og:site_name" content="Lawlite的博客">
<meta property="og:description" content="机器学习算法Python实现说明 github地址：https://github.com/lawlite19/MachineLearning_Python  目录 机器学习算法Python实现 一、线性回归 1、代价函数 2、梯度下降算法 3、均值归一化 4、最终运行结果 5、使用scikit-learn库中的线性模型实现   二、逻辑回归 1、代价函数 2、梯度 3、正则化 4、S型函数（即）">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7B%7B2%7B%5Ctext%7Bm%7D%7D%7D%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D%20">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%7B%5Ctheta%20_0%7D%20%2B%20%7B%5Ctheta%20_1%7D%7Bx_1%7D%20%2B%20%7B%5Ctheta%20_2%7D%7Bx_2%7D%20%2B%20...">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Ctheta%20_j%7D%7D">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%5Ctheta%20_j%7D%20%3D%20%7B%5Ctheta%20_j%7D%20-%20%5Calpha%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%5Calpha%20">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7Bx_i%7D%20%3D%20%5Cfrac%7B%7B%7Bx_i%7D%20-%20%7B%5Cmu%20_i%7D%7D%7D%7B%7B%7Bs_i%7D%7D%7D">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7B%5Cmu%20_i%7D%7D">
<meta property="og:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=%7B%7Bs_i%7D%7D">
<meta property="og:image" content="http://lawlite.com/assets/blog_images/ml_python_images/LinearRegression_01.png">
<meta property="og:image" content="http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24">
<meta property="og:image" content="http://lawlite.com/assets/blog_images/ml_python_images/AnomalyDetection_08.png">
<meta property="og:image" content="http://lawlite.com/assets/blog_images/ml_python_images/AnomalyDetection_09.png">
<meta property="og:image" content="http://lawlite.com/assets/blog_images/ml_python_images/AnomalyDetection_10.png">
<meta property="og:updated_time" content="2017-04-09T05:02:07.192Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Python机器学习">
<meta name="twitter:description" content="机器学习算法Python实现说明 github地址：https://github.com/lawlite19/MachineLearning_Python  目录 机器学习算法Python实现 一、线性回归 1、代价函数 2、梯度下降算法 3、均值归一化 4、最终运行结果 5、使用scikit-learn库中的线性模型实现   二、逻辑回归 1、代价函数 2、梯度 3、正则化 4、S型函数（即）">
<meta name="twitter:image" content="http://chart.apis.google.com/chart?cht=tx&chs=1x0&chf=bg,s,FFFFFF00&chco=000000&chl=J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7B%7B2%7B%5Ctext%7Bm%7D%7D%7D%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D%20">
  
    <link rel="alternative" href="/atom.xml" title="Lawlite的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/assets/img/favicon.ico">
  
  <link rel="stylesheet" type="text/css" href="/./main.234bc0.css">
  <style type="text/css">
  
    #container.show {
      background: linear-gradient(200deg,#a0cfe4,#e8c37e);
    }
  </style>
  
<!-- Google Analytics -->
<script type="text/javascript">
(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-97005936-1', 'auto');
ga('send', 'pageview');

</script>
<!-- End Google Analytics -->


  
<script>
var _hmt = _hmt || [];
(function() {
	var hm = document.createElement("script");
	hm.src = "https://hm.baidu.com/hm.js?454d1a5ba8ed297a4f64d38a344e80c7";
	var s = document.getElementsByTagName("script")[0]; 
	s.parentNode.insertBefore(hm, s);
})();
</script>


</head>

<body>
  <div id="container" q-class="show:isCtnShow">
    <canvas id="anm-canvas" class="anm-canvas"></canvas>
    <div class="left-col" q-class="show:isShow">
      
<div class="overlay" style="background: #279ADB"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			<img src="/assets/img/avatar.jpg" class="js-avatar">
		</a>
		<hgroup>
		  <h1 class="header-author"><a href="/">Lawlite</a></h1>
		</hgroup>
		
		<p class="header-subtitle">随意点</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/tags/随笔/">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
    		
    			
    			<a q-on="click: openSlider(e, 'innerArchive')" href="javascript:void(0)">所有文章</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'friends')" href="javascript:void(0)">友链</a>
    			
            
    			
    			<a q-on="click: openSlider(e, 'aboutme')" href="javascript:void(0)">关于我</a>
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/lawlite19" title="github"><i class="icon-github"></i></a>
		        
					<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/bob-75-78-98" title="zhihu"><i class="icon-zhihu"></i></a>
		        
					<a class="mail" target="_blank" href="mailto:849451478@qq.com" title="mail"><i class="icon-mail"></i></a>
		        
					<a class="facebook" target="_blank" href="https://www.facebook.com/bob.wang.1690" title="facebook"><i class="icon-facebook"></i></a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col" q-class="show:isShow,hide:isShow|isFalse">
      
<nav id="mobile-nav">
  	<div class="overlay js-overlay" style="background: #279ADB"></div>
	<div class="btnctn js-mobile-btnctn">
  		<div class="slider-trigger list" q-on="click: openSlider(e)"><i class="icon icon-sort"></i></div>
	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				<img src="/assets/img/avatar.jpg" class="js-avatar">
			</div>
			<hgroup>
			  <h1 class="header-author js-header-author">Lawlite</h1>
			</hgroup>
			
			<p class="header-subtitle"><i class="icon icon-quo-left"></i>随意点<i class="icon icon-quo-right"></i></p>
			
			
			
				
			
				
			
			
			
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/lawlite19" title="github"><i class="icon-github"></i></a>
			        
						<a class="zhihu" target="_blank" href="https://www.zhihu.com/people/bob-75-78-98" title="zhihu"><i class="icon-zhihu"></i></a>
			        
						<a class="mail" target="_blank" href="mailto:849451478@qq.com" title="mail"><i class="icon-mail"></i></a>
			        
						<a class="facebook" target="_blank" href="https://www.facebook.com/bob.wang.1690" title="facebook"><i class="icon-facebook"></i></a>
			        
				</div>
			</nav>

			<nav class="header-menu js-header-menu">
				<ul style="width: 50%">
				
				
					<li style="width: 50%"><a href="/">主页</a></li>
		        
					<li style="width: 50%"><a href="/tags/随笔/">随笔</a></li>
		        
				</ul>
			</nav>
		</header>				
	</div>
	<div class="mobile-mask" style="display:none" q-show="isShow"></div>
</nav>

      <div id="wrapper" class="body-wrap">
        <div class="menu-l">
          <div class="canvas-wrap">
            <canvas data-colors="#eaeaea" data-sectionHeight="100" data-contentId="js-content" id="myCanvas1" class="anm-canvas"></canvas>
          </div>
          <div id="js-content" class="content-ll">
            <article id="post-Python机器学习" class="article article-type-post " itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      Python机器学习
    </h1>
  

        
        <a href="/2017/01/08/Python机器学习/" class="archive-article-date">
  	<time datetime="2017-01-08T15:01:58.000Z" itemprop="datePublished"><i class="icon-calendar icon"></i>2017-01-08</time>
</a>
        
      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h1 id="机器学习算法Python实现"><a href="#机器学习算法Python实现" class="headerlink" title="机器学习算法Python实现"></a>机器学习算法Python实现</h1><h2 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h2><ul>
<li>github地址：<a href="https://github.com/lawlite19/MachineLearning_Python" target="_blank" rel="external">https://github.com/lawlite19/MachineLearning_Python</a></li>
</ul>
<h2 id="目录"><a href="#目录" class="headerlink" title="目录"></a>目录</h2><ul>
<li><a href="#机器学习算法python实现">机器学习算法Python实现</a><ul>
<li><a href="#一-线性回归">一、线性回归</a><ul>
<li><a href="#1-代价函数">1、代价函数</a></li>
<li><a href="#2-梯度下降算法">2、梯度下降算法</a></li>
<li><a href="#3-均值归一化">3、均值归一化</a></li>
<li><a href="#4-最终运行结果">4、最终运行结果</a></li>
<li><a href="#5-使用scikit-learn库中的线性模型实现">5、使用scikit-learn库中的线性模型实现</a></li>
</ul>
</li>
<li><a href="#二-逻辑回归">二、逻辑回归</a><ul>
<li><a href="#1-代价函数">1、代价函数</a></li>
<li><a href="#2-梯度">2、梯度</a></li>
<li><a href="#3-正则化">3、正则化</a></li>
<li><a href="#4-s型函数即">4、S型函数（即）</a></li>
<li><a href="#5-映射为多项式">5、映射为多项式</a></li>
<li><a href="#6-使用的优化方法">6、使用的优化方法</a></li>
<li><a href="#7-运行结果">7、运行结果</a></li>
<li><a href="#8-使用scikit-learn库中的逻辑回归模型实现">8、使用scikit-learn库中的逻辑回归模型实现</a></li>
</ul>
</li>
</ul>
</li>
</ul>
<a id="more"></a>
<pre><code>* [逻辑回归_手写数字识别_OneVsAll](#逻辑回归_手写数字识别_onevsall)
    * [1、随机显示100个数字](#1-随机显示100个数字)
    * [2、OneVsAll](#2-onevsall)
    * [3、手写数字识别](#3-手写数字识别)
    * [4、预测](#4-预测)
    * [5、运行结果](#5-运行结果)
    * [6、使用scikit-learn库中的逻辑回归模型实现](#6-使用scikit-learn库中的逻辑回归模型实现)
* [三、BP神经网络](#三-bp神经网络)
    * [1、神经网络model](#1-神经网络model)
    * [2、代价函数](#2-代价函数)
    * [3、正则化](#3-正则化)
    * [4、反向传播BP](#4-反向传播bp)
    * [5、BP可以求梯度的原因](#5-bp可以求梯度的原因)
    * [6、梯度检查](#6-梯度检查)
    * [7、权重的随机初始化](#7-权重的随机初始化)
    * [8、预测](#8-预测)
    * [9、输出结果](#9-输出结果)
* [四、SVM支持向量机](#四-svm支持向量机)
    * [1、代价函数](#1-代价函数)
    * [2、Large Margin](#2-large-margin)
    * [3、SVM Kernel（核函数）](#3-svm-kernel核函数)
    * [4、使用中的模型代码](#4-使用中的模型代码)
    * [5、运行结果](#5-运行结果)
* [五、K-Means聚类算法](#五-k-means聚类算法)
    * [1、聚类过程](#1-聚类过程)
    * [2、目标函数](#2-目标函数)
    * [3、聚类中心的选择](#3-聚类中心的选择)
    * [4、聚类个数K的选择](#4-聚类个数k的选择)
    * [5、应用——图片压缩](#5-应用图片压缩)
    * [6、使用scikit-learn库中的线性模型实现聚类](#6-使用scikit-learn库中的线性模型实现聚类)
    * [7、运行结果](#7-运行结果)
* [六、PCA主成分分析（降维）](#六-pca主成分分析降维)
    * [1、用处](#1-用处)
    * [2、2D--&gt;1D，nD--&gt;kD](#2-2d-1dnd-kd)
    * [3、主成分分析PCA与线性回归的区别](#3-主成分分析pca与线性回归的区别)
    * [4、PCA降维过程](#4-pca降维过程)
    * [5、数据恢复](#5-数据恢复)
    * [6、主成分个数的选择（即要降的维度）](#6-主成分个数的选择即要降的维度)
    * [7、使用建议](#7-使用建议)
    * [8、运行结果](#8-运行结果)
    * [9、使用scikit-learn库中的PCA实现降维](#9-使用scikit-learn库中的pca实现降维)
* [七、异常检测 Anomaly Detection](#七-异常检测-anomaly-detection)
    * [1、高斯分布（正态分布）](#1-高斯分布正态分布)
    * [2、异常检测算法](#2-异常检测算法)
    * [3、评价的好坏，以及的选取](#3-评价的好坏以及的选取)
    * [4、选择使用什么样的feature（单元高斯分布）](#4-选择使用什么样的feature单元高斯分布)
    * [5、多元高斯分布](#5-多元高斯分布)
    * [6、单元和多元高斯分布特点](#6-单元和多元高斯分布特点)
    * [7、程序运行结果](#7-程序运行结果)
</code></pre><h2 id="一、线性回归"><a href="#一、线性回归" class="headerlink" title="一、线性回归"></a>一、<a href="/LinearRegression">线性回归</a></h2><ul>
<li><a href="/LinearRegression/LinearRegression.py">全部代码</a></li>
</ul>
<h3 id="1、代价函数"><a href="#1、代价函数" class="headerlink" title="1、代价函数"></a>1、代价函数</h3><ul>
<li><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7B%7B2%7B%5Ctext%7Bm%7D%7D%7D%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D%20" alt="J(\theta ) = \frac{1}12{\text{m}}}}\sum\limits_{i = 1}^m 1{({h_\theta }({x^{(i)}}) - {y^{(i)}})}^2}} "></li>
<li><p>其中：<br><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%7B%5Ctheta%20_0%7D%20%2B%20%7B%5Ctheta%20_1%7D%7Bx_1%7D%20%2B%20%7B%5Ctheta%20_2%7D%7Bx_2%7D%20%2B%20..." alt="{h_\theta }(x) = {\theta _0} + {\theta _1}{x_1} + {\theta _2}{x_2} + ..."></p>
</li>
<li><p>下面就是要求出theta，使代价最小，即代表我们拟合出来的方程距离真实值最近</p>
</li>
<li>共有m条数据，其中<img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7B%7B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29%7D%5E2%7D%7D" alt="1{({h_\theta }({x^{(i)}}) - {y^{(i)}})}^2}}">代表我们要拟合出来的方程到真实值距离的平方，平方的原因是因为可能有负值，正负可能会抵消</li>
<li><p>前面有系数<code>2</code>的原因是下面求梯度是对每个变量求偏导，<code>2</code>可以消去</p>
</li>
<li><p>实现代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"># 计算代价函数</div><div class="line">def computerCost(X,y,theta):</div><div class="line">    m = len(y)</div><div class="line">    J = 0</div><div class="line">    </div><div class="line">    J = (np.transpose(X*theta-y))*(X*theta-y)/(2*m) #计算代价J</div><div class="line">    return J</div></pre></td></tr></table></figure>
<ul>
<li>注意这里的X是真实数据前加了一列1，因为有theta(0)</li>
</ul>
</li>
</ul>
<h3 id="2、梯度下降算法"><a href="#2、梯度下降算法" class="headerlink" title="2、梯度下降算法"></a>2、梯度下降算法</h3><ul>
<li>代价函数对<img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7B%5Ctheta%20_j%7D%7D" alt="1\theta _j}}">求偏导得到：<br><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20" alt="\frac1\partial J(\theta )}}1\partial {\theta _j}}} = \frac{1}{m}\sum\limits_{i = 1}^m {[({h_\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} "></li>
<li>所以对theta的更新可以写为：<br><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20_j%7D%20%3D%20%7B%5Ctheta%20_j%7D%20-%20%5Calpha%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20" alt="{\theta _j} = {\theta _j} - \alpha \frac{1}{m}\sum\limits_{i = 1}^m {[({h_\theta }({x^{(i)}}) - {y^{(i)}})x_j^{(i)}]} "></li>
<li>其中<img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Calpha%20" alt="\alpha ">为学习速率，控制梯度下降的速度，一般取<strong>0.01,0.03,0.1,0.3…..</strong></li>
<li>为什么梯度下降可以逐步减小代价函数<ul>
<li>假设函数<code>f(x)</code></li>
<li>泰勒展开：<code>f(x+△x)=f(x)+f&#39;(x)*△x+o(△x)</code></li>
<li>令：<code>△x=-α*f&#39;(x)</code>   ,即负梯度方向乘以一个很小的步长<code>α</code></li>
<li>将<code>△x</code>代入泰勒展开式中：<code>f(x+x)=f(x)-α*[f&#39;(x)]²+o(△x)</code></li>
<li>可以看出，<code>α</code>是取得很小的正数，<code>[f&#39;(x)]²</code>也是正数，所以可以得出：<code>f(x+△x)&lt;=f(x)</code></li>
<li>所以沿着<strong>负梯度</strong>放下，函数在减小，多维情况一样。</li>
</ul>
</li>
<li>实现代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"># 梯度下降算法</div><div class="line">def gradientDescent(X,y,theta,alpha,num_iters):</div><div class="line">    m = len(y)      </div><div class="line">    n = len(theta)</div><div class="line">    </div><div class="line">    temp = np.matrix(np.zeros((n,num_iters)))   # 暂存每次迭代计算的theta，转化为矩阵形式</div><div class="line">    </div><div class="line">    </div><div class="line">    J_history = np.zeros((num_iters,1)) #记录每次迭代计算的代价值</div><div class="line">    </div><div class="line">    for i in range(num_iters):  # 遍历迭代次数    </div><div class="line">        h = np.dot(X,theta)     # 计算内积，matrix可以直接乘</div><div class="line">        temp[:,i] = theta - ((alpha/m)*(np.dot(np.transpose(X),h-y)))   #梯度的计算</div><div class="line">        theta = temp[:,i]</div><div class="line">        J_history[i] = computerCost(X,y,theta)      #调用计算代价函数</div><div class="line">        print &apos;.&apos;,      </div><div class="line">    return theta,J_history</div></pre></td></tr></table></figure>
</li>
</ul>
<h3 id="3、均值归一化"><a href="#3、均值归一化" class="headerlink" title="3、均值归一化"></a>3、均值归一化</h3><ul>
<li>目的是使数据都缩放到一个范围内，便于使用梯度下降算法</li>
<li><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx_i%7D%20%3D%20%5Cfrac%7B%7B%7Bx_i%7D%20-%20%7B%5Cmu%20_i%7D%7D%7D%7B%7B%7Bs_i%7D%7D%7D" alt="{x_i} = \frac1{x_i} - {\mu _i}}}1{s_i}}}"></li>
<li>其中 <img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7B%5Cmu%20_i%7D%7D" alt="1\mu _i}}"> 为所有此feture数据的平均值</li>
<li><img src="http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bs_i%7D%7D" alt="1s_i}}">可以是<strong>最大值-最小值</strong>，也可以是这个feature对应的数据的<strong>标准差</strong></li>
<li><p>实现代码：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"># 归一化feature</div><div class="line">def featureNormaliza(X):</div><div class="line">    X_norm = np.array(X)            #将X转化为numpy数组对象，才可以进行矩阵的运算</div><div class="line">    #定义所需变量</div><div class="line">    mu = np.zeros((1,X.shape[1]))   </div><div class="line">    sigma = np.zeros((1,X.shape[1]))</div><div class="line">    </div><div class="line">    mu = np.mean(X_norm,0)          # 求每一列的平均值（0指定为列，1代表行）</div><div class="line">    sigma = np.std(X_norm,0)        # 求每一列的标准差</div><div class="line">    for i in range(X.shape[1]):     # 遍历列</div><div class="line">        X_norm[:,i] = (X_norm[:,i]-mu[i])/sigma[i]  # 归一化</div><div class="line">    </div><div class="line">    return X_norm,mu,sigma</div></pre></td></tr></table></figure>
</li>
<li><p>注意预测的时候也需要均值归一化数据</p>
</li>
</ul>
<h3 id="4、最终运行结果"><a href="#4、最终运行结果" class="headerlink" title="4、最终运行结果"></a>4、最终运行结果</h3><ul>
<li>代价随迭代次数的变化<br><img src="/assets/blog_images/ml_python_images/LinearRegression_01.png" alt="enter description here" title="LinearRegression_01.png"></li>
</ul>
<h3 id="5、使用scikit-learn库中的线性模型实现"><a href="#5、使用scikit-learn库中的线性模型实现" class="headerlink" title="5、使用scikit-learn库中的线性模型实现"></a>5、<a href="/LinearRegression/LinearRegression_scikit-learn.py">使用scikit-learn库中的线性模型实现</a></h3><ul>
<li><p>导入包</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">from sklearn import linear_model</div><div class="line">from sklearn.preprocessing import StandardScaler    #引入缩放的包</div></pre></td></tr></table></figure>
</li>
<li><p>归一化</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"># 归一化操作</div><div class="line">scaler = StandardScaler()   </div><div class="line">scaler.fit(X)</div><div class="line">x_train = scaler.transform(X)</div><div class="line">x_test = scaler.transform(np.array([1650,3]))</div></pre></td></tr></table></figure>
</li>
<li><p>线性模型拟合</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">    # 线性模型拟合</div><div class="line">    model = linear_model.LinearRegression()</div><div class="line">    model.fit(x_train, y)</div><div class="line">``` </div><div class="line">- 预测</div></pre></td></tr></table></figure>
<p>  #预测结果<br>  result = model.predict(x_test)</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">-------------------</div><div class="line"></div><div class="line">  </div><div class="line">## 二、[逻辑回归](/LogisticRegression)</div><div class="line">- [全部代码](/LogisticRegression/LogisticRegression.py)</div><div class="line"></div><div class="line">### 1、代价函数</div><div class="line">- ![\left\&#123; \begin&#123;gathered&#125;</div><div class="line">  J(\theta ) = \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;\cos t(&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;),&#123;y^&#123;(i)&#125;&#125;)&#125;  \hfill \\</div><div class="line">  \cos t(&#123;h_\theta &#125;(x),y) = \left\&#123; &#123;\begin&#123;array&#125;&#123;c&#125;    &#123; - \log (&#123;h_\theta &#125;(x))&#125; \\    &#123; - \log (1 - &#123;h_\theta &#125;(x))&#125;  \end&#123;array&#125; \begin&#123;array&#125;&#123;c&#125;    &#123;y = 1&#125; \\    &#123;y = 0&#125;  \end&#123;array&#125; &#125; \right. \hfill \\ </div><div class="line">\end&#123;gathered&#125;  \right.](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%5Cbegin%7Bgathered%7D%20J%28%5Ctheta%20%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%2C%7By%5E%7B%28i%29%7D%7D%29%7D%20%5Chfill%20%5C%5C%20%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%7By%20%3D%201%7D%20%5C%5C%20%7By%20%3D%200%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%20%5Chfill%20%5C%5C%20%5Cend%7Bgathered%7D%20%5Cright.)</div><div class="line">- 可以综合起来为：</div><div class="line">![J(\theta ) =  - \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\log (&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\log (1 - &#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)</div><div class="line">其中：</div><div class="line">![&#123;h_\theta &#125;(x) = \frac&#123;1&#125;11 + &#123;e^&#123; - x&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20x%7D%7D%7D%7D)</div><div class="line">- 为什么不用线性回归的代价函数表示，因为线性回归的代价函数可能是非凸的，对于分类问题，使用梯度下降很难得到最小值，上面的代价函数是凸函数</div><div class="line">- ![&#123; - \log (&#123;h_\theta &#125;(x))&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下，即`y=1`时：</div><div class="line">![enter description here][2]</div><div class="line"></div><div class="line">可以看出，当![1h_\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`1`，`y=1`,与预测值一致，此时付出的代价`cost`趋于`0`，若![1h_\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)趋于`0`，`y=1`,此时的代价`cost`值非常大，我们最终的目的是最小化代价值</div><div class="line">- 同理![&#123; - \log (1 - &#123;h_\theta &#125;(x))&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D)的图像如下（`y=0`）：   </div><div class="line">![enter description here][3]</div><div class="line"></div><div class="line">### 2、梯度</div><div class="line">- 同样对代价函数求偏导：</div><div class="line">![\frac1\partial J(\theta )&#125;&#125;1\partial &#123;\theta _j&#125;&#125;&#125; = \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;[(&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) - &#123;y^&#123;(i)&#125;&#125;)x_j^&#123;(i)&#125;]&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5Ctheta%20%29%7D%7D%7B%7B%5Cpartial%20%7B%5Ctheta%20_j%7D%7D%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20-%20%7By%5E%7B%28i%29%7D%7D%29x_j%5E%7B%28i%29%7D%5D%7D%20)   </div><div class="line">可以看出与线性回归的偏导数一致</div><div class="line">- 推到过程</div><div class="line">![enter description here][4]</div><div class="line"></div><div class="line">### 3、正则化</div><div class="line">- 目的是为了防止过拟合</div><div class="line">- 在代价函数中加上一项![J(\theta ) =  - \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\log (&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\log (1 - &#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)] + \frac&#123;\lambda &#125;12m&#125;&#125;\sum\limits_&#123;j = 1&#125;^n &#123;\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)</div><div class="line">- 注意j是重1开始的，因为theta(0)为一个常数项，X中最前面一列会加上1列1，所以乘积还是theta(0),feature没有关系，没有必要正则化</div><div class="line">- 正则化后的代价：</div></pre></td></tr></table></figure>
</li>
</ul>
<h1 id="代价函数"><a href="#代价函数" class="headerlink" title="代价函数"></a>代价函数</h1><p>def costFunction(initial_theta,X,y,inital_lambda):<br>    m = len(y)<br>    J = 0</p>
<pre><code>h = sigmoid(np.dot(X,initial_theta))    # 计算h(z)
theta1 = initial_theta.copy()           # 因为正则化j=1从1开始，不包含0，所以复制一份，前theta(0)值为0 
theta1[0] = 0   

temp = np.dot(np.transpose(theta1),theta1)
J = (-np.dot(np.transpose(y),np.log(h))-np.dot(np.transpose(1-y),np.log(1-h))+temp*inital_lambda/2)/m   # 正则化的代价方程
return J
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 正则化后的代价的梯度</div></pre></td></tr></table></figure>
<h1 id="计算梯度"><a href="#计算梯度" class="headerlink" title="计算梯度"></a>计算梯度</h1><p>def gradient(initial_theta,X,y,inital_lambda):<br>    m = len(y)<br>    grad = np.zeros((initial_theta.shape[0]))</p>
<pre><code>h = sigmoid(np.dot(X,initial_theta))# 计算h(z)
theta1 = initial_theta.copy()
theta1[0] = 0

grad = np.dot(np.transpose(X),h-y)/m+inital_lambda/m*theta1 #正则化的梯度
return grad  
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 4、S型函数（即![1h_\theta &#125;(x)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7Bh_%5Ctheta%20%7D%28x%29%7D)）</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="S型函数"><a href="#S型函数" class="headerlink" title="S型函数"></a>S型函数</h1><p>def sigmoid(z):<br>    h = np.zeros((len(z),1))    # 初始化，与z的长度一置</p>
<pre><code>h = 1.0/(1.0+np.exp(-z))
return h
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 5、映射为多项式</div><div class="line">- 因为数据的feture可能很少，导致偏差大，所以创造出一些feture结合</div><div class="line">- eg:映射为2次方的形式:![1 + &#123;x_1&#125; + &#123;x_2&#125; + x_1^2 + &#123;x_1&#125;&#123;x_2&#125; + x_2^2](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="映射为多项式"><a href="#映射为多项式" class="headerlink" title="映射为多项式"></a>映射为多项式</h1><p>def mapFeature(X1,X2):<br>    degree = 3;                     # 映射的最高次方<br>    out = np.ones((X1.shape[0],1))  # 映射后的结果数组（取代X）<br>    ‘’’<br>    这里以degree=2为例，映射为1,x1,x2,x1^2,x1,x2,x2^2<br>    ‘’’<br>    for i in np.arange(1,degree+1):<br>        for j in range(i+1):<br>            temp = X1<strong>(i-j)*(X2</strong>j)    #矩阵直接乘相当于matlab中的点乘.*<br>            out = np.hstack((out, temp.reshape(-1,1)))<br>    return out<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 6、使用`scipy`的优化方法</div><div class="line">- 梯度下降使用`scipy`中`optimize`中的`fmin_bfgs`函数</div><div class="line">- 调用scipy中的优化算法fmin_bfgs（拟牛顿法Broyden-Fletcher-Goldfarb-Shanno</div><div class="line"> - costFunction是自己实现的一个求代价的函数，</div><div class="line"> - initial_theta表示初始化的值,</div><div class="line"> - fprime指定costFunction的梯度</div><div class="line"> - args是其余测参数，以元组的形式传入，最后会将最小化costFunction的theta返回</div></pre></td></tr></table></figure></p>
<pre><code>result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,y,initial_lambda))    
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 7、运行结果</div><div class="line">- data1决策边界和准确度  </div><div class="line">![enter description here][5]</div><div class="line">![enter description here][6]</div><div class="line">- data2决策边界和准确度  </div><div class="line">![enter description here][7]</div><div class="line">![enter description here][8]</div><div class="line"></div><div class="line">### 8、[使用scikit-learn库中的逻辑回归模型实现](/LogisticRegression/LogisticRegression_scikit-learn.py)</div><div class="line">- 导入包</div></pre></td></tr></table></figure>
<p>from sklearn.linear_model import LogisticRegression<br>from sklearn.preprocessing import StandardScaler<br>from sklearn.cross_validation import train_test_split<br>import numpy as np<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 划分训练集和测试集</div></pre></td></tr></table></figure></p>
<pre><code># 划分为训练集和测试集
x_train,x_test,y_train,y_test = train_test_split(X,y,test_size=0.2)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 归一化</div></pre></td></tr></table></figure>
<pre><code># 归一化
scaler = StandardScaler()
scaler.fit(x_train)
x_train = scaler.fit_transform(x_train)
x_test = scaler.fit_transform(x_test)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 逻辑回归</div></pre></td></tr></table></figure>
<pre><code>#逻辑回归
model = LogisticRegression()
model.fit(x_train,y_train)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 预测</div></pre></td></tr></table></figure>
<pre><code># 预测
predict = model.predict(x_test)
right = sum(predict == y_test)

predict = np.hstack((predict.reshape(-1,1),y_test.reshape(-1,1)))   # 将预测值和真实值放在一块，好观察
print predict
print (&apos;测试集准确率：%f%%&apos;%(right*100.0/predict.shape[0]))          #计算在测试集上的准确度
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line">-------------</div><div class="line"></div><div class="line">## [逻辑回归_手写数字识别_OneVsAll](/LogisticRegression)</div><div class="line">- [全部代码](/LogisticRegression/LogisticRegression_OneVsAll.py)</div><div class="line"></div><div class="line">### 1、随机显示100个数字</div><div class="line">- 我没有使用scikit-learn中的数据集，像素是20*20px，彩色图如下</div><div class="line">![enter description here][9]</div><div class="line">灰度图：</div><div class="line">![enter description here][10]</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="显示100个数字"><a href="#显示100个数字" class="headerlink" title="显示100个数字"></a>显示100个数字</h1><p>def display_data(imgData):<br>    sum = 0<br>    ‘’’<br>    显示100个数（若是一个一个绘制将会非常慢，可以将要画的数字整理好，放到一个矩阵中，显示这个矩阵即可）</p>
<pre><code>- 初始化一个二维数组
- 将每行的数据调整成图像的矩阵，放进二维数组
- 显示即可
&apos;&apos;&apos;
pad = 1
display_array = -np.ones((pad+10*(20+pad),pad+10*(20+pad)))
for i in range(10):
    for j in range(10):
        display_array[pad+i*(20+pad):pad+i*(20+pad)+20,pad+j*(20+pad):pad+j*(20+pad)+20] = (imgData[sum,:].reshape(20,20,order=&quot;F&quot;))    # order=F指定以列优先，在matlab中是这样的，python中需要指定，默认以行
        sum += 1

plt.imshow(display_array,cmap=&apos;gray&apos;)   #显示灰度图像
plt.axis(&apos;off&apos;)
plt.show()
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 2、OneVsAll</div><div class="line">- 如何利用逻辑回归解决多分类的问题，OneVsAll就是把当前某一类看成一类，其他所有类别看作一类，这样有成了二分类的问题了</div><div class="line">- 如下图，把途中的数据分成三类，先把红色的看成一类，把其他的看作另外一类，进行逻辑回归，然后把蓝色的看成一类，其他的再看成一类，以此类推...</div><div class="line">![enter description here][11]</div><div class="line">- 可以看出大于2类的情况下，有多少类就要进行多少次的逻辑回归分类</div><div class="line"></div><div class="line">### 3、手写数字识别</div><div class="line">- 共有0-9，10个数字，需要10次分类</div><div class="line">- 由于**数据集y**给出的是`0,1,2...9`的数字，而进行逻辑回归需要`0/1`的label标记，所以需要对y处理</div><div class="line">- 说一下数据集，前`500`个是`0`,`500-1000`是`1`,`...`,所以如下图，处理后的`y`，**前500行的第一列是1，其余都是0,500-1000行第二列是1，其余都是0....**</div><div class="line">![enter description here][12]</div><div class="line">- 然后调用**梯度下降算法**求解`theta`</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="求每个分类的theta，最后返回所有的all-theta"><a href="#求每个分类的theta，最后返回所有的all-theta" class="headerlink" title="求每个分类的theta，最后返回所有的all_theta"></a>求每个分类的theta，最后返回所有的all_theta</h1><p>def oneVsAll(X,y,num_labels,Lambda):</p>
<pre><code># 初始化变量
m,n = X.shape
all_theta = np.zeros((n+1,num_labels))  # 每一列对应相应分类的theta,共10列
X = np.hstack((np.ones((m,1)),X))       # X前补上一列1的偏置bias
class_y = np.zeros((m,num_labels))      # 数据的y对应0-9，需要映射为0/1的关系
initial_theta = np.zeros((n+1,1))       # 初始化一个分类的theta

# 映射y
for i in range(num_labels):
    class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值

#np.savetxt(&quot;class_y.csv&quot;, class_y[0:600,:], delimiter=&apos;,&apos;)    

&apos;&apos;&apos;遍历每个分类，计算对应的theta值&apos;&apos;&apos;
for i in range(num_labels):
    result = optimize.fmin_bfgs(costFunction, initial_theta, fprime=gradient, args=(X,class_y[:,i],Lambda)) # 调用梯度下降的优化方法
    all_theta[:,i] = result.reshape(1,-1)   # 放入all_theta中

all_theta = np.transpose(all_theta) 
return all_theta
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 4、预测</div><div class="line">- 之前说过，预测的结果是一个**概率值**，利用学习出来的`theta`代入预测的**S型函数**中，每行的最大值就是是某个数字的最大概率，所在的**列号**就是预测的数字的真实值,因为在分类时，所有为`0`的将`y`映射在第一列，为1的映射在第二列，依次类推</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="预测"><a href="#预测" class="headerlink" title="预测"></a>预测</h1><p>def predict_oneVsAll(all_theta,X):<br>    m = X.shape[0]<br>    num_labels = all_theta.shape[0]<br>    p = np.zeros((m,1))<br>    X = np.hstack((np.ones((m,1)),X))   #在X最前面加一列1</p>
<pre><code>h = sigmoid(np.dot(X,np.transpose(all_theta)))  #预测

&apos;&apos;&apos;
返回h中每一行最大值所在的列号
- np.max(h, axis=1)返回h中每一行的最大值（是某个数字的最大概率）
- 最后where找到的最大概率所在的列号（列号即是对应的数字）
&apos;&apos;&apos;
p = np.array(np.where(h[0,:] == np.max(h, axis=1)[0]))  
for i in np.arange(1, m):
    t = np.array(np.where(h[i,:] == np.max(h, axis=1)[i]))
    p = np.vstack((p,t))
return p
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 5、运行结果</div><div class="line">- 10次分类，在训练集上的准确度：   </div><div class="line">![enter description here][13]</div><div class="line"></div><div class="line">### 6、[使用scikit-learn库中的逻辑回归模型实现](/LogisticRegression/LogisticRegression_OneVsAll_scikit-learn.py)</div><div class="line">- 1、导入包</div></pre></td></tr></table></figure>
<p>from scipy import io as spio<br>import numpy as np<br>from sklearn import svm<br>from sklearn.linear_model import LogisticRegression<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 2、加载数据</div></pre></td></tr></table></figure></p>
<pre><code>data = loadmat_data(&quot;data_digits.mat&quot;) 
X = data[&apos;X&apos;]   # 获取X数据，每一行对应一个数字20x20px
y = data[&apos;y&apos;]   # 这里读取mat文件y的shape=(5000, 1)
y = np.ravel(y) # 调用sklearn需要转化成一维的(5000,)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 3、拟合模型</div></pre></td></tr></table></figure>
<pre><code>model = LogisticRegression()
model.fit(X, y) # 拟合
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 4、预测</div></pre></td></tr></table></figure>
<pre><code>predict = model.predict(X) #预测

print u&quot;预测准确度为：%f%%&quot;%np.mean(np.float64(predict == y)*100)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div></pre></td><td class="code"><pre><div class="line">- 5、输出结果（在训练集上的准确度）</div><div class="line">![enter description here][14]</div><div class="line"></div><div class="line"></div><div class="line">----------</div><div class="line"></div><div class="line">## 三、BP神经网络</div><div class="line">- [全部代码](/NeuralNetwok/NeuralNetwork.py)</div><div class="line"></div><div class="line">### 1、神经网络model</div><div class="line">- 先介绍个三层的神经网络，如下图所示</div><div class="line"> - 输入层（input layer）有三个units（![&#123;x_0&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bx_0%7D)为补上的bias，通常设为`1`）</div><div class="line"> - ![a_i^&#123;(j)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_i%5E%7B%28j%29%7D)表示第`j`层的第`i`个激励，也称为为单元unit</div><div class="line"> - ![&#123;\theta ^&#123;(j)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D)为第`j`层到第`j+1`层映射的权重矩阵，就是每条边的权重</div><div class="line">![enter description here][15]</div><div class="line"></div><div class="line">- 所以可以得到：</div><div class="line"> - 隐含层：  </div><div class="line">![a_1^&#123;(2)&#125; = g(\theta _&#123;10&#125;^&#123;(1)&#125;&#123;x_0&#125; + \theta _&#123;11&#125;^&#123;(1)&#125;&#123;x_1&#125; + \theta _&#123;12&#125;^&#123;(1)&#125;&#123;x_2&#125; + \theta _&#123;13&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_1%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%281%29%7D%7Bx_3%7D%29)   </div><div class="line">![a_2^&#123;(2)&#125; = g(\theta _&#123;20&#125;^&#123;(1)&#125;&#123;x_0&#125; + \theta _&#123;21&#125;^&#123;(1)&#125;&#123;x_1&#125; + \theta _&#123;22&#125;^&#123;(1)&#125;&#123;x_2&#125; + \theta _&#123;23&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_2%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B20%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B21%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B22%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B23%7D%5E%7B%281%29%7D%7Bx_3%7D%29)   </div><div class="line">![a_3^&#123;(2)&#125; = g(\theta _&#123;30&#125;^&#123;(1)&#125;&#123;x_0&#125; + \theta _&#123;31&#125;^&#123;(1)&#125;&#123;x_1&#125; + \theta _&#123;32&#125;^&#123;(1)&#125;&#123;x_2&#125; + \theta _&#123;33&#125;^&#123;(1)&#125;&#123;x_3&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=a_3%5E%7B%282%29%7D%20%3D%20g%28%5Ctheta%20_%7B30%7D%5E%7B%281%29%7D%7Bx_0%7D%20%2B%20%5Ctheta%20_%7B31%7D%5E%7B%281%29%7D%7Bx_1%7D%20%2B%20%5Ctheta%20_%7B32%7D%5E%7B%281%29%7D%7Bx_2%7D%20%2B%20%5Ctheta%20_%7B33%7D%5E%7B%281%29%7D%7Bx_3%7D%29)</div><div class="line"> - 输出层   </div><div class="line">![&#123;h_\theta &#125;(x) = a_1^&#123;(3)&#125; = g(\theta _&#123;10&#125;^&#123;(2)&#125;a_0^&#123;(2)&#125; + \theta _&#123;11&#125;^&#123;(2)&#125;a_1^&#123;(2)&#125; + \theta _&#123;12&#125;^&#123;(2)&#125;a_2^&#123;(2)&#125; + \theta _&#123;13&#125;^&#123;(2)&#125;a_3^&#123;(2)&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28x%29%20%3D%20a_1%5E%7B%283%29%7D%20%3D%20g%28%5Ctheta%20_%7B10%7D%5E%7B%282%29%7Da_0%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B11%7D%5E%7B%282%29%7Da_1%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B12%7D%5E%7B%282%29%7Da_2%5E%7B%282%29%7D%20%2B%20%5Ctheta%20_%7B13%7D%5E%7B%282%29%7Da_3%5E%7B%282%29%7D%29) 其中，**S型函数**![g(z) = \frac&#123;1&#125;11 + &#123;e^&#123; - z&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=g%28z%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)，也成为**激励函数**</div><div class="line">- 可以看出![&#123;\theta ^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%281%29%7D%7D) 为3x4的矩阵，![&#123;\theta ^&#123;(2)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%282%29%7D%7D)为1x4的矩阵</div><div class="line"> - ![&#123;\theta ^&#123;(j)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5E%7B%28j%29%7D%7D) ==》`j+1`的单元数x（`j`层的单元数+1）</div><div class="line"></div><div class="line">### 2、代价函数</div><div class="line">- 假设最后输出的![&#123;h_\Theta &#125;(x) \in &#123;R^K&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5CTheta%20%7D%28x%29%20%5Cin%20%7BR%5EK%7D)，即代表输出层有K个单元</div><div class="line">- ![J(\Theta ) =  - \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;\sum\limits_&#123;k = 1&#125;^K &#123;[y_k^&#123;(i)&#125;\log 1(&#123;h_\Theta &#125;(&#123;x^&#123;(i)&#125;&#125;))&#125;_k&#125;&#125; &#125;  + (1 - y_k^&#123;(i)&#125;)\log &#123;(1 - &#123;h_\Theta &#125;(&#123;x^&#123;(i)&#125;&#125;))_k&#125;]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5CTheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5Csum%5Climits_%7Bk%20%3D%201%7D%5EK%20%7B%5By_k%5E%7B%28i%29%7D%5Clog%20%7B%7B%28%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29%7D_k%7D%7D%20%7D%20%20%2B%20%281%20-%20y_k%5E%7B%28i%29%7D%29%5Clog%20%7B%281%20-%20%7Bh_%5CTheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%29_k%7D%5D) 其中，![&#123;(&#123;h_\Theta &#125;(x))_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%28%7Bh_%5CTheta%20%7D%28x%29%29_i%7D)代表第`i`个单元输出</div><div class="line">- 与逻辑回归的代价函数![J(\theta ) =  - \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\log (&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\log (1 - &#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D)差不多，就是累加上每个输出（共有K个输出）</div><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">### 3、正则化</div><div class="line">- `L`--&gt;所有层的个数</div><div class="line">- ![&#123;S_l&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7BS_l%7D)--&gt;第`l`层unit的个数</div><div class="line">- 正则化后的**代价函数**为  </div><div class="line">![enter description here][16]</div><div class="line"> - ![\theta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ctheta%20)共有`L-1`层，</div><div class="line"> - 然后是累加对应每一层的theta矩阵，注意不包含加上偏置项对应的theta(0)</div><div class="line">- 正则化后的代价函数实现代码：</div></pre></td></tr></table></figure>
<h1 id="代价函数-1"><a href="#代价函数-1" class="headerlink" title="代价函数"></a>代价函数</h1><p>def nnCostFunction(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda):<br>    length = nn_params.shape[0] # theta的中长度</p>
<pre><code># 还原theta1和theta2
Theta1 = nn_params[0:hidden_layer_size*(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1)
Theta2 = nn_params[hidden_layer_size*(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1)

# np.savetxt(&quot;Theta1.csv&quot;,Theta1,delimiter=&apos;,&apos;)

m = X.shape[0]
class_y = np.zeros((m,num_labels))      # 数据的y对应0-9，需要映射为0/1的关系
# 映射y
for i in range(num_labels):
    class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值

&apos;&apos;&apos;去掉theta1和theta2的第一列，因为正则化时从1开始&apos;&apos;&apos;    
Theta1_colCount = Theta1.shape[1]    
Theta1_x = Theta1[:,1:Theta1_colCount]
Theta2_colCount = Theta2.shape[1]    
Theta2_x = Theta2[:,1:Theta2_colCount]
# 正则化向theta^2
term = np.dot(np.transpose(np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1)))),np.vstack((Theta1_x.reshape(-1,1),Theta2_x.reshape(-1,1))))

&apos;&apos;&apos;正向传播,每次需要补上一列1的偏置bias&apos;&apos;&apos;
a1 = np.hstack((np.ones((m,1)),X))      
z2 = np.dot(a1,np.transpose(Theta1))    
a2 = sigmoid(z2)
a2 = np.hstack((np.ones((m,1)),a2))
z3 = np.dot(a2,np.transpose(Theta2))
h  = sigmoid(z3)    
&apos;&apos;&apos;代价&apos;&apos;&apos;    
J = -(np.dot(np.transpose(class_y.reshape(-1,1)),np.log(h.reshape(-1,1)))+np.dot(np.transpose(1-class_y.reshape(-1,1)),np.log(1-h.reshape(-1,1)))-Lambda*term/2)/m   

return np.ravel(J)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 4、反向传播BP</div><div class="line">- 上面正向传播可以计算得到`J(θ)`,使用梯度下降法还需要求它的梯度</div><div class="line">- BP反向传播的目的就是求代价函数的梯度</div><div class="line">- 假设4层的神经网络,![\delta _&#123;\text&#123;j&#125;&#125;^&#123;(l)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%28l%29%7D)记为--&gt;`l`层第`j`个单元的误差</div><div class="line"> - ![\delta _&#123;\text&#123;j&#125;&#125;^&#123;(4)&#125; = a_j^&#123;(4)&#125; - &#123;y_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20_%7B%5Ctext%7Bj%7D%7D%5E%7B%284%29%7D%20%3D%20a_j%5E%7B%284%29%7D%20-%20%7By_i%7D)《===》![&#123;\delta ^&#123;(4)&#125;&#125; = &#123;a^&#123;(4)&#125;&#125; - y](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%284%29%7D%7D%20%3D%20%7Ba%5E%7B%284%29%7D%7D%20-%20y)（向量化）</div><div class="line"> - ![&#123;\delta ^&#123;(3)&#125;&#125; = &#123;(&#123;\theta ^&#123;(3)&#125;&#125;)^T&#125;&#123;\delta ^&#123;(4)&#125;&#125;.*&#123;g^&#125;(&#123;a^&#123;(3)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%283%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%283%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%284%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29)</div><div class="line"> - ![&#123;\delta ^&#123;(2)&#125;&#125; = &#123;(&#123;\theta ^&#123;(2)&#125;&#125;)^T&#125;&#123;\delta ^&#123;(3)&#125;&#125;.*&#123;g^&#125;(&#123;a^&#123;(2)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D%20%3D%20%7B%28%7B%5Ctheta%20%5E%7B%282%29%7D%7D%29%5ET%7D%7B%5Cdelta%20%5E%7B%283%29%7D%7D.%2A%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29)</div><div class="line"> - 没有![&#123;\delta ^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%281%29%7D%7D)，因为对于输入没有误差</div><div class="line">- 因为S型函数![&#123;\text&#123;g(z)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctext%7Bg%28z%29%7D%7D)的倒数为：![&#123;g^&#125;(z)&#123;\text&#123; = g(z)(1 - g(z))&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28z%29%7B%5Ctext%7B%20%3D%20g%28z%29%281%20-%20g%28z%29%29%7D%7D)，所以上面的![&#123;g^&#125;(&#123;a^&#123;(3)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28%7Ba%5E%7B%283%29%7D%7D%29)和![&#123;g^&#125;(&#123;a^&#123;(2)&#125;&#125;)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bg%5E%7D%28%7Ba%5E%7B%282%29%7D%7D%29)可以在前向传播中计算出来</div><div class="line"></div><div class="line">- 反向传播计算梯度的过程为：</div><div class="line"> - ![\Delta _&#123;ij&#125;^&#123;(l)&#125; = 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%200)（![\Delta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20)是大写的![\delta ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cdelta%20)）</div><div class="line"> - for i=1-m:     </div><div class="line"> -![&#123;a^&#123;(1)&#125;&#125; = &#123;x^&#123;(i)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Ba%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%28i%29%7D%7D)       </div><div class="line">-正向传播计算![&#123;a^&#123;(l)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Ba%5E%7B%28l%29%7D%7D)（l=2,3,4...L）      </div><div class="line">-反向计算![&#123;\delta ^&#123;(L)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%28L%29%7D%7D)、![&#123;\delta ^&#123;(L - 1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%28L%20-%201%29%7D%7D)...![&#123;\delta ^&#123;(2)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Cdelta%20%5E%7B%282%29%7D%7D)；       </div><div class="line">-![\Delta _&#123;ij&#125;^&#123;(l)&#125; = \Delta _&#123;ij&#125;^&#123;(l)&#125; + a_j^&#123;(l)&#125;&#123;\delta ^&#123;(l + 1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20a_j%5E%7B%28l%29%7D%7B%5Cdelta%20%5E%7B%28l%20%2B%201%29%7D%7D)          </div><div class="line">-![D_&#123;ij&#125;^&#123;(l)&#125; = \frac&#123;1&#125;&#123;m&#125;\Delta _&#123;ij&#125;^&#123;(l)&#125; + \lambda \theta _&#123;ij&#125;^l\begin&#123;array&#125;&#123;c&#125;    &#123;&#125;&amp;amp; &#123;(j \ne 0)&#125;  \end&#123;array&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5El%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7B%28j%20%5Cne%200%29%7D%20%20%5Cend%7Barray%7D%20)      </div><div class="line">![D_&#123;ij&#125;^&#123;(l)&#125; = \frac&#123;1&#125;&#123;m&#125;\Delta _&#123;ij&#125;^&#123;(l)&#125; + \lambda \theta _&#123;ij&#125;^lj = 0\begin&#123;array&#125;&#123;c&#125;    &#123;&#125;&amp;amp; &#123;j = 0&#125;  \end&#123;array&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=D_%7Bij%7D%5E%7B%28l%29%7D%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5CDelta%20_%7Bij%7D%5E%7B%28l%29%7D%20%2B%20%5Clambda%20%5Ctheta%20_%7Bij%7D%5Elj%20%3D%200%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7D%26%20%7Bj%20%3D%200%7D%20%20%5Cend%7Barray%7D%20)     </div><div class="line"></div><div class="line">- 最后![\frac1\partial J(\Theta )&#125;&#125;1\partial \Theta _&#123;ij&#125;^&#123;(l)&#125;&#125;&#125; = D_&#123;ij&#125;^&#123;(l)&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7B%5Cpartial%20J%28%5CTheta%20%29%7D%7D%7B%7B%5Cpartial%20%5CTheta%20_%7Bij%7D%5E%7B%28l%29%7D%7D%7D%20%3D%20D_%7Bij%7D%5E%7B%28l%29%7D)，即得到代价函数的梯度</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="梯度"><a href="#梯度" class="headerlink" title="梯度"></a>梯度</h1><p>def nnGradient(nn_params,input_layer_size,hidden_layer_size,num_labels,X,y,Lambda):<br>    length = nn_params.shape[0]<br>    Theta1 = nn_params[0:hidden_layer_size<em>(input_layer_size+1)].reshape(hidden_layer_size,input_layer_size+1)<br>    Theta2 = nn_params[hidden_layer_size</em>(input_layer_size+1):length].reshape(num_labels,hidden_layer_size+1)<br>    m = X.shape[0]<br>    class_y = np.zeros((m,num_labels))      # 数据的y对应0-9，需要映射为0/1的关系    </p>
<pre><code># 映射y
for i in range(num_labels):
    class_y[:,i] = np.int32(y==i).reshape(1,-1) # 注意reshape(1,-1)才可以赋值

&apos;&apos;&apos;去掉theta1和theta2的第一列，因为正则化时从1开始&apos;&apos;&apos;
Theta1_colCount = Theta1.shape[1]    
Theta1_x = Theta1[:,1:Theta1_colCount]
Theta2_colCount = Theta2.shape[1]    
Theta2_x = Theta2[:,1:Theta2_colCount]

Theta1_grad = np.zeros((Theta1.shape))  #第一层到第二层的权重
Theta2_grad = np.zeros((Theta2.shape))  #第二层到第三层的权重

Theta1[:,0] = 0;
Theta2[:,0] = 0;
&apos;&apos;&apos;正向传播，每次需要补上一列1的偏置bias&apos;&apos;&apos;
a1 = np.hstack((np.ones((m,1)),X))
z2 = np.dot(a1,np.transpose(Theta1))
a2 = sigmoid(z2)
a2 = np.hstack((np.ones((m,1)),a2))
z3 = np.dot(a2,np.transpose(Theta2))
h  = sigmoid(z3)

&apos;&apos;&apos;反向传播，delta为误差，&apos;&apos;&apos;
delta3 = np.zeros((m,num_labels))
delta2 = np.zeros((m,hidden_layer_size))
for i in range(m):
    delta3[i,:] = h[i,:]-class_y[i,:]
    Theta2_grad = Theta2_grad+np.dot(np.transpose(delta3[i,:].reshape(1,-1)),a2[i,:].reshape(1,-1))
    delta2[i,:] = np.dot(delta3[i,:].reshape(1,-1),Theta2_x)*sigmoidGradient(z2[i,:])
    Theta1_grad = Theta1_grad+np.dot(np.transpose(delta2[i,:].reshape(1,-1)),a1[i,:].reshape(1,-1))

&apos;&apos;&apos;梯度&apos;&apos;&apos;
grad = (np.vstack((Theta1_grad.reshape(-1,1),Theta2_grad.reshape(-1,1)))+Lambda*np.vstack((Theta1.reshape(-1,1),Theta2.reshape(-1,1))))/m
return np.ravel(grad)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 5、BP可以求梯度的原因</div><div class="line">- 实际是利用了`链式求导`法则</div><div class="line">- 因为下一层的单元利用上一层的单元作为输入进行计算</div><div class="line">- 大体的推导过程如下，最终我们是想预测函数与已知的`y`非常接近，求均方差的梯度沿着此梯度方向可使代价函数最小化。可对照上面求梯度的过程。</div><div class="line">![enter description here][17]</div><div class="line">- 求误差更详细的推导过程：</div><div class="line">![enter description here][18]</div><div class="line"></div><div class="line">### 6、梯度检查</div><div class="line">- 检查利用`BP`求的梯度是否正确</div><div class="line">- 利用导数的定义验证：</div><div class="line">![\frac1dJ(\theta )&#125;&#125;1d\theta &#125;&#125; \approx \frac1J(\theta  + \varepsilon ) - J(\theta  - \varepsilon )&#125;&#125;12\varepsilon &#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Cfrac%7B%7BdJ%28%5Ctheta%20%29%7D%7D%7B%7Bd%5Ctheta%20%7D%7D%20%5Capprox%20%5Cfrac%7B%7BJ%28%5Ctheta%20%20%2B%20%5Cvarepsilon%20%29%20-%20J%28%5Ctheta%20%20-%20%5Cvarepsilon%20%29%7D%7D%7B%7B2%5Cvarepsilon%20%7D%7D)</div><div class="line">- 求出来的数值梯度应该与BP求出的梯度非常接近</div><div class="line">- 验证BP正确后就不需要再执行验证梯度的算法了</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure>
<h1 id="检验梯度是否计算正确"><a href="#检验梯度是否计算正确" class="headerlink" title="检验梯度是否计算正确"></a>检验梯度是否计算正确</h1><h1 id="检验梯度是否计算正确-1"><a href="#检验梯度是否计算正确-1" class="headerlink" title="检验梯度是否计算正确"></a>检验梯度是否计算正确</h1><p>def checkGradient(Lambda = 0):<br>    ‘’’构造一个小型的神经网络验证，因为数值法计算梯度很浪费时间，而且验证正确后之后就不再需要验证了’’’<br>    input_layer_size = 3<br>    hidden_layer_size = 5<br>    num_labels = 3<br>    m = 5<br>    initial_Theta1 = debugInitializeWeights(input_layer_size,hidden_layer_size);<br>    initial_Theta2 = debugInitializeWeights(hidden_layer_size,num_labels)<br>    X = debugInitializeWeights(input_layer_size-1,m)<br>    y = 1+np.transpose(np.mod(np.arange(1,m+1), num_labels))# 初始化y</p>
<pre><code>y = y.reshape(-1,1)
nn_params = np.vstack((initial_Theta1.reshape(-1,1),initial_Theta2.reshape(-1,1)))  #展开theta 
&apos;&apos;&apos;BP求出梯度&apos;&apos;&apos;
grad = nnGradient(nn_params, input_layer_size, hidden_layer_size, 
                 num_labels, X, y, Lambda)  
&apos;&apos;&apos;使用数值法计算梯度&apos;&apos;&apos;
num_grad = np.zeros((nn_params.shape[0]))
step = np.zeros((nn_params.shape[0]))
e = 1e-4
for i in range(nn_params.shape[0]):
    step[i] = e
    loss1 = nnCostFunction(nn_params-step.reshape(-1,1), input_layer_size, hidden_layer_size, 
                          num_labels, X, y, 
                          Lambda)
    loss2 = nnCostFunction(nn_params+step.reshape(-1,1), input_layer_size, hidden_layer_size, 
                          num_labels, X, y, 
                          Lambda)
    num_grad[i] = (loss2-loss1)/(2*e)
    step[i]=0
# 显示两列比较
res = np.hstack((num_grad.reshape(-1,1),grad.reshape(-1,1)))
print res
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 7、权重的随机初始化</div><div class="line">- 神经网络不能像逻辑回归那样初始化`theta`为`0`,因为若是每条边的权重都为0，每个神经元都是相同的输出，在反向传播中也会得到同样的梯度，最终只会预测一种结果。</div><div class="line">- 所以应该初始化为接近0的数</div><div class="line">- 实现代码</div></pre></td></tr></table></figure>
<h1 id="随机初始化权重theta"><a href="#随机初始化权重theta" class="headerlink" title="随机初始化权重theta"></a>随机初始化权重theta</h1><p>def randInitializeWeights(L_in,L_out):<br>    W = np.zeros((L_out,1+L_in))    # 对应theta的权重<br>    epsilon_init = (6.0/(L_out+L_in))<em>*0.5<br>    W = np.random.rand(L_out,1+L_in)</em>2<em>epsilon_init-epsilon_init # np.random.rand(L_out,1+L_in)产生L_out</em>(1+L_in)大小的随机矩阵<br>    return W<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 8、预测</div><div class="line">- 正向传播预测结果</div><div class="line">- 实现代码</div></pre></td></tr></table></figure></p>
<h1 id="预测-1"><a href="#预测-1" class="headerlink" title="预测"></a>预测</h1><p>def predict(Theta1,Theta2,X):<br>    m = X.shape[0]<br>    num_labels = Theta2.shape[0]</p>
<pre><code>#p = np.zeros((m,1))
&apos;&apos;&apos;正向传播，预测结果&apos;&apos;&apos;
X = np.hstack((np.ones((m,1)),X))
h1 = sigmoid(np.dot(X,np.transpose(Theta1)))
h1 = np.hstack((np.ones((m,1)),h1))
h2 = sigmoid(np.dot(h1,np.transpose(Theta2)))

&apos;&apos;&apos;
返回h中每一行最大值所在的列号
- np.max(h, axis=1)返回h中每一行的最大值（是某个数字的最大概率）
- 最后where找到的最大概率所在的列号（列号即是对应的数字）
&apos;&apos;&apos;
#np.savetxt(&quot;h2.csv&quot;,h2,delimiter=&apos;,&apos;)
p = np.array(np.where(h2[0,:] == np.max(h2, axis=1)[0]))  
for i in np.arange(1, m):
    t = np.array(np.where(h2[i,:] == np.max(h2, axis=1)[i]))
    p = np.vstack((p,t))
return p 
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 9、输出结果</div><div class="line">- 梯度检查：     </div><div class="line">![enter description here][19]</div><div class="line">- 随机显示100个手写数字     </div><div class="line">![enter description here][20]</div><div class="line">- 显示theta1权重     </div><div class="line">![enter description here][21]</div><div class="line">- 训练集预测准确度     </div><div class="line">![enter description here][22]</div><div class="line">- 归一化后训练集预测准确度     </div><div class="line">![enter description here][23]</div><div class="line"></div><div class="line">--------------------</div><div class="line"></div><div class="line">## 四、SVM支持向量机</div><div class="line"></div><div class="line">### 1、代价函数</div><div class="line">- 在逻辑回归中，我们的代价为：   </div><div class="line">![\cos t(&#123;h_\theta &#125;(x),y) = \left\&#123; &#123;\begin&#123;array&#125;&#123;c&#125;    &#123; - \log (&#123;h_\theta &#125;(x))&#125; \\    &#123; - \log (1 - &#123;h_\theta &#125;(x))&#125;  \end&#123;array&#125; \begin&#123;array&#125;&#123;c&#125;    &#123;y = 1&#125; \\    &#123;y = 0&#125;  \end&#123;array&#125; &#125; \right.](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20t%28%7Bh_%5Ctheta%20%7D%28x%29%2Cy%29%20%3D%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%20-%20%5Clog%20%28%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%5C%5C%20%20%20%20%7B%20-%20%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28x%29%29%7D%20%20%5Cend%7Barray%7D%20%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7By%20%3D%201%7D%20%5C%5C%20%20%20%20%7By%20%3D%200%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright.)，    </div><div class="line">其中：![&#123;h_\theta &#125;(&#123;\text&#123;z&#125;&#125;) = \frac&#123;1&#125;11 + &#123;e^&#123; - z&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bh_%5Ctheta%20%7D%28%7B%5Ctext%7Bz%7D%7D%29%20%3D%20%5Cfrac%7B1%7D%7B%7B1%20%2B%20%7Be%5E%7B%20-%20z%7D%7D%7D%7D)，![z = &#123;\theta ^T&#125;x](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=z%20%3D%20%7B%5Ctheta%20%5ET%7Dx)</div><div class="line">- 如图所示，如果`y=1`，`cost`代价函数如图所示    </div><div class="line">![enter description here][24]    </div><div class="line">我们想让![&#123;\theta ^T&#125;x &amp;gt;  &amp;gt; 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%5Ctheta%20%5ET%7Dx%20%3E%20%20%3E%200)，即`z&gt;&gt;0`，这样的话`cost`代价函数才会趋于最小（这是我们想要的），所以用途中**红色**的函数![\cos &#123;t_1&#125;(z)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20%7Bt_1%7D%28z%29)代替逻辑回归中的cost</div><div class="line">- 当`y=0`时同样，用![\cos &#123;t_0&#125;(z)](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Ccos%20%7Bt_0%7D%28z%29)代替</div><div class="line">![enter description here][25]</div><div class="line">- 最终得到的代价函数为：    </div><div class="line">![J(\theta ) = C\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\cos &#123;t_1&#125;(&#123;\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\cos &#123;t_0&#125;(&#123;\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;)&#125; ] + \frac&#123;1&#125;&#123;2&#125;\sum\limits_&#123;j = 1&#125;^&#123;\text&#123;n&#125;&#125; &#123;\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20)   </div><div class="line">最后我们想要![\mathop &#123;\min &#125;\limits_\theta  J(\theta )](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cmathop%20%7B%5Cmin%20%7D%5Climits_%5Ctheta%20J%28%5Ctheta%20%29)</div><div class="line">- 之前我们逻辑回归中的代价函数为：   </div><div class="line">![J(\theta ) =  - \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\log (&#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;) + (1 - &#125; &#123;y^&#123;(i)&#125;&#125;)\log (1 - &#123;h_\theta &#125;(&#123;x^&#123;(i)&#125;&#125;)] + \frac&#123;\lambda &#125;12m&#125;&#125;\sum\limits_&#123;j = 1&#125;^n &#123;\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20%20-%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Clog%20%28%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7D%20%7By%5E%7B%28i%29%7D%7D%29%5Clog%20%281%20-%20%7Bh_%5Ctheta%20%7D%28%7Bx%5E%7B%28i%29%7D%7D%29%5D%20%2B%20%5Cfrac%7B%5Clambda%20%7D%7B%7B2m%7D%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5En%20%7B%5Ctheta%20_j%5E2%7D%20)   </div><div class="line">可以认为这里的![C = \frac&#123;m&#125;&#123;\lambda &#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=C%20%3D%20%5Cfrac%7Bm%7D%7B%5Clambda%20%7D)，只是表达形式问题，这里`C`的值越大，SVM的决策边界的`margin`也越大，下面会说明</div><div class="line"></div><div class="line">### 2、Large Margin</div><div class="line">- 如下图所示,SVM分类会使用最大的`margin`将其分开    </div><div class="line">![enter description here][26]</div><div class="line">- 先说一下向量内积</div><div class="line"> - ![u = \left[ &#123;\begin&#123;array&#125;&#123;c&#125;    1u_1&#125;&#125; \\    1u_2&#125;&#125;  \end&#123;array&#125; &#125; \right]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=u%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bu_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bu_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D)，![v = \left[ &#123;\begin&#123;array&#125;&#123;c&#125;    1v_1&#125;&#125; \\    1v_2&#125;&#125;  \end&#123;array&#125; &#125; \right]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=v%20%3D%20%5Cleft%5B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%20%20%20%7B%7Bv_1%7D%7D%20%5C%5C%20%20%20%20%7B%7Bv_2%7D%7D%20%20%5Cend%7Barray%7D%20%7D%20%5Cright%5D)    </div><div class="line"> - ![||u||](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7Cu%7C%7C)表示`u`的**欧几里得范数**（欧式范数），![||u||&#123;\text&#123; = &#125;&#125;\sqrt 1\text&#123;u&#125;&#125;_1^2 + u_2^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7Cu%7C%7C%7B%5Ctext%7B%20%3D%20%7D%7D%5Csqrt%20%7B%7B%5Ctext%7Bu%7D%7D_1%5E2%20%2B%20u_2%5E2%7D%20)</div><div class="line"> - `向量V`在`向量u`上的投影的长度记为`p`，则：向量内积：    </div><div class="line"> ![1\text&#123;u&#125;&#125;^T&#125;v = p||u|| = &#123;u_1&#125;&#123;v_1&#125; + &#123;u_2&#125;&#123;v_2&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7B%7B%5Ctext%7Bu%7D%7D%5ET%7Dv%20%3D%20p%7C%7Cu%7C%7C%20%3D%20%7Bu_1%7D%7Bv_1%7D%20%2B%20%7Bu_2%7D%7Bv_2%7D)      </div><div class="line"> ![enter description here][27]  </div><div class="line">根据向量夹角公式推导一下即可，![\cos \theta  = \frac1\overrightarrow &#123;\text&#123;u&#125;&#125; \overrightarrow v &#125;&#125;1|\overrightarrow &#123;\text&#123;u&#125;&#125; ||\overrightarrow v |&#125;&#125;](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Ccos%20%5Ctheta%20%3D%20%5Cfrac%7B%7B%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%5Coverrightarrow%20v%20%7D%7D%7B%7B%7C%5Coverrightarrow%20%7B%5Ctext%7Bu%7D%7D%20%7C%7C%5Coverrightarrow%20v%20%7C%7D%7D)</div><div class="line"></div><div class="line">- 前面说过，当`C`越大时，`margin`也就越大，我们的目的是最小化代价函数`J(θ)`,当`margin`最大时，`C`的乘积项![\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\cos &#123;t_1&#125;(&#123;\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\cos &#123;t_0&#125;(&#123;\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125;)&#125; ]](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%29%7D%20%5D)要很小，所以近似为：   </div><div class="line">![J(\theta ) = C0 + \frac&#123;1&#125;&#123;2&#125;\sum\limits_&#123;j = 1&#125;^&#123;\text&#123;n&#125;&#125; &#123;\theta _j^2&#125;  = \frac&#123;1&#125;&#123;2&#125;\sum\limits_&#123;j = 1&#125;^&#123;\text&#123;n&#125;&#125; &#123;\theta _j^2&#125;  = \frac&#123;1&#125;&#123;2&#125;(\theta _1^2 + \theta _2^2) = \frac&#123;1&#125;&#123;2&#125;&#123;\sqrt &#123;\theta _1^2 + \theta _2^2&#125; ^2&#125;](http://latex.codecogs.com/gif.latex?%5Clarge%20J%28%5Ctheta%20%29%20%3D%20C0%20&amp;plus;%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20%3D%20%5Cfrac%7B1%7D%7B2%7D%28%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%29%20%3D%20%5Cfrac%7B1%7D%7B2%7D%7B%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%7D%20%5E2%7D)，      </div><div class="line">我们最后的目的就是求使代价最小的`θ`</div><div class="line">- 由   </div><div class="line">![\left\&#123; &#123;\begin&#123;array&#125;&#123;c&#125;    1\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125; \geqslant 1&#125; \\    1\theta ^T&#125;&#123;x^&#123;(i)&#125;&#125; \leqslant  - 1&#125;  \end&#123;array&#125; &#125; \right.\begin&#123;array&#125;&#123;c&#125;    &#123;(&#123;y^&#123;(i)&#125;&#125; = 1)&#125; \\    &#123;(&#123;y^&#123;(i)&#125;&#125; = 0)&#125;  \end&#123;array&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7B%5Ctheta%20%5ET%7D%7Bx%5E%7B%28i%29%7D%7D%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)可以得到：    </div><div class="line">![\left\&#123; &#123;\begin&#123;array&#125;&#123;c&#125;    1p^&#123;(i)&#125;&#125;||\theta || \geqslant 1&#125; \\    1p^&#123;(i)&#125;&#125;||\theta || \leqslant  - 1&#125;  \end&#123;array&#125; &#125; \right.\begin&#123;array&#125;&#123;c&#125;    &#123;(&#123;y^&#123;(i)&#125;&#125; = 1)&#125; \\    &#123;(&#123;y^&#123;(i)&#125;&#125; = 0)&#125;  \end&#123;array&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%5Cleft%5C%7B%20%7B%5Cbegin%7Barray%7D%7Bc%7D%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201%7D%20%5C%5C%20%7B%7Bp%5E%7B%28i%29%7D%7D%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201%7D%20%5Cend%7Barray%7D%20%7D%20%5Cright.%5Cbegin%7Barray%7D%7Bc%7D%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%201%29%7D%20%5C%5C%20%7B%28%7By%5E%7B%28i%29%7D%7D%20%3D%200%29%7D%20%5Cend%7Barray%7D)，`p`即为`x`在`θ`上的投影</div><div class="line">- 如下图所示，假设决策边界如图，找其中的一个点，到`θ`上的投影为`p`,则![p||\theta || \geqslant 1](http://latex.codecogs.com/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cgeqslant%201)或者![p||\theta || \leqslant  - 1](http://latex.codecogs.com/gif.latex?%5Clarge%20p%7C%7C%5Ctheta%20%7C%7C%20%5Cleqslant%20-%201)，若是`p`很小，则需要![||\theta ||](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7C%7C%5Ctheta%20%7C%7C)很大，这与我们要求的`θ`使![||\theta || = \frac&#123;1&#125;&#123;2&#125;\sqrt &#123;\theta _1^2 + \theta _2^2&#125; ](http://latex.codecogs.com/gif.latex?%5Clarge%20%7C%7C%5Ctheta%20%7C%7C%20%3D%20%5Cfrac%7B1%7D%7B2%7D%5Csqrt%20%7B%5Ctheta%20_1%5E2%20&amp;plus;%20%5Ctheta%20_2%5E2%7D)最小相违背，**所以**最后求的是`large margin`   </div><div class="line">![enter description here][28]</div><div class="line"></div><div class="line">### 3、SVM Kernel（核函数）</div><div class="line">- 对于线性可分的问题，使用**线性核函数**即可</div><div class="line">- 对于线性不可分的问题，在逻辑回归中，我们是将`feature`映射为使用多项式的形式![1 + &#123;x_1&#125; + &#123;x_2&#125; + x_1^2 + &#123;x_1&#125;&#123;x_2&#125; + x_2^2](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=1%20%2B%20%7Bx_1%7D%20%2B%20%7Bx_2%7D%20%2B%20x_1%5E2%20%2B%20%7Bx_1%7D%7Bx_2%7D%20%2B%20x_2%5E2)，`SVM`中也有**多项式核函数**，但是更常用的是**高斯核函数**，也称为**RBF核**</div><div class="line">- 高斯核函数为：![f(x) = &#123;e^&#123; - \frac1||x - u|&#123;|^2&#125;&#125;&#125;12&#123;\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=f%28x%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20u%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)     </div><div class="line">假设如图几个点，</div><div class="line">![enter description here][29]</div><div class="line">令：   </div><div class="line">![&#123;f_1&#125; = similarity(x,&#123;l^&#123;(1)&#125;&#125;) = &#123;e^&#123; - \frac1||x - &#123;l^&#123;(1)&#125;&#125;|&#123;|^2&#125;&#125;&#125;12&#123;\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_1%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%281%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%281%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)   </div><div class="line">![&#123;f_2&#125; = similarity(x,&#123;l^&#123;(2)&#125;&#125;) = &#123;e^&#123; - \frac1||x - &#123;l^&#123;(2)&#125;&#125;|&#123;|^2&#125;&#125;&#125;12&#123;\sigma ^2&#125;&#125;&#125;&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_2%7D%20%3D%20similarity%28x%2C%7Bl%5E%7B%282%29%7D%7D%29%20%3D%20%7Be%5E%7B%20-%20%5Cfrac%7B%7B%7C%7Cx%20-%20%7Bl%5E%7B%282%29%7D%7D%7C%7B%7C%5E2%7D%7D%7D%7B%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D)</div><div class="line">.</div><div class="line">.</div><div class="line">.</div><div class="line">- 可以看出，若是`x`与![&#123;l^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D)距离较近，==》![&#123;f_1&#125; \approx &#123;e^0&#125; = 1](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_1%7D%20%5Capprox%20%7Be%5E0%7D%20%3D%201)，（即相似度较大）   </div><div class="line">若是`x`与![&#123;l^&#123;(1)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D)距离较远，==》![&#123;f_2&#125; \approx &#123;e^&#123; - \infty &#125;&#125; = 0](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf_2%7D%20%5Capprox%20%7Be%5E%7B%20-%20%5Cinfty%20%7D%7D%20%3D%200)，（即相似度较低）</div><div class="line">- 高斯核函数的`σ`越小，`f`下降的越快      </div><div class="line">![enter description here][30]</div><div class="line">![enter description here][31]</div><div class="line"></div><div class="line">- 如何选择初始的![&#123;l^&#123;(1)&#125;&#125;&#123;l^&#123;(2)&#125;&#125;&#123;l^&#123;(3)&#125;&#125;...](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D%7Bl%5E%7B%282%29%7D%7D%7Bl%5E%7B%283%29%7D%7D...)</div><div class="line"> - 训练集：![((&#123;x^&#123;(1)&#125;&#125;,&#123;y^&#123;(1)&#125;&#125;),(&#123;x^&#123;(2)&#125;&#125;,&#123;y^&#123;(2)&#125;&#125;),...(&#123;x^&#123;(m)&#125;&#125;,&#123;y^&#123;(m)&#125;&#125;))](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%28%28%7Bx%5E%7B%281%29%7D%7D%2C%7By%5E%7B%281%29%7D%7D%29%2C%28%7Bx%5E%7B%282%29%7D%7D%2C%7By%5E%7B%282%29%7D%7D%29%2C...%28%7Bx%5E%7B%28m%29%7D%7D%2C%7By%5E%7B%28m%29%7D%7D%29%29)</div><div class="line"> - 选择：![&#123;l^&#123;(1)&#125;&#125; = &#123;x^&#123;(1)&#125;&#125;,&#123;l^&#123;(2)&#125;&#125; = &#123;x^&#123;(2)&#125;&#125;...&#123;l^&#123;(m)&#125;&#125; = &#123;x^&#123;(m)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bl%5E%7B%281%29%7D%7D%20%3D%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bl%5E%7B%282%29%7D%7D%20%3D%20%7Bx%5E%7B%282%29%7D%7D...%7Bl%5E%7B%28m%29%7D%7D%20%3D%20%7Bx%5E%7B%28m%29%7D%7D)</div><div class="line"> - 对于给出的`x`，计算`f`,令：![f_0^&#123;(i)&#125; = 1](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=f_0%5E%7B%28i%29%7D%20%3D%201)所以：![&#123;f^&#123;(i)&#125;&#125; \in &#123;R^&#123;m + 1&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bf%5E%7B%28i%29%7D%7D%20%5Cin%20%7BR%5E%7Bm%20%2B%201%7D%7D)</div><div class="line"> - 最小化`J`求出`θ`，          </div><div class="line"> ![J(\theta ) = C\sum\limits_&#123;i = 1&#125;^m &#123;[&#123;y^&#123;(i)&#125;&#125;\cos &#123;t_1&#125;(&#123;\theta ^T&#125;&#123;f^&#123;(i)&#125;&#125;) + (1 - &#123;y^&#123;(i)&#125;&#125;)\cos &#123;t_0&#125;(&#123;\theta ^T&#125;&#123;f^&#123;(i)&#125;&#125;)&#125; ] + \frac&#123;1&#125;&#123;2&#125;\sum\limits_&#123;j = 1&#125;^&#123;\text&#123;n&#125;&#125; &#123;\theta _j^2&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%5Ctheta%20%29%20%3D%20C%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%5B%7By%5E%7B%28i%29%7D%7D%5Ccos%20%7Bt_1%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%20%2B%20%281%20-%20%7By%5E%7B%28i%29%7D%7D%29%5Ccos%20%7Bt_0%7D%28%7B%5Ctheta%20%5ET%7D%7Bf%5E%7B%28i%29%7D%7D%29%7D%20%5D%20%2B%20%5Cfrac%7B1%7D%7B2%7D%5Csum%5Climits_%7Bj%20%3D%201%7D%5E%7B%5Ctext%7Bn%7D%7D%20%7B%5Ctheta%20_j%5E2%7D%20)</div><div class="line"> - 如果![&#123;\theta ^T&#125;f \geqslant 0](http://latex.codecogs.com/gif.latex?%5Clarge%20%7B%5Ctheta%20%5ET%7Df%20%5Cgeqslant%200)，==》预测`y=1`</div><div class="line"></div><div class="line">### 4、使用`scikit-learn`中的`SVM`模型代码</div><div class="line">- [全部代码](/SVM/SVM_scikit-learn.py)</div><div class="line">- 线性可分的,指定核函数为`linear`：</div></pre></td></tr></table></figure>
<pre><code>&apos;&apos;&apos;data1——线性分类&apos;&apos;&apos;
data1 = spio.loadmat(&apos;data1.mat&apos;)
X = data1[&apos;X&apos;]
y = data1[&apos;y&apos;]
y = np.ravel(y)
plot_data(X,y)

model = svm.SVC(C=1.0,kernel=&apos;linear&apos;).fit(X,y) # 指定核函数为线性核函数
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 非线性可分的，默认核函数为`rbf`</div></pre></td></tr></table></figure>
<pre><code>&apos;&apos;&apos;data2——非线性分类&apos;&apos;&apos;
data2 = spio.loadmat(&apos;data2.mat&apos;)
X = data2[&apos;X&apos;]
y = data2[&apos;y&apos;]
y = np.ravel(y)
plt = plot_data(X,y)
plt.show()

model = svm.SVC(gamma=100).fit(X,y)     # gamma为核函数的系数，值越大拟合的越好
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div></pre></td><td class="code"><pre><div class="line">### 5、运行结果</div><div class="line">- 线性可分的决策边界：    </div><div class="line">![enter description here][32]</div><div class="line">- 线性不可分的决策边界：   </div><div class="line">![enter description here][33]</div><div class="line"></div><div class="line">--------------------------</div><div class="line"></div><div class="line">## 五、K-Means聚类算法</div><div class="line">- [全部代码](/K-Means/K-Menas.py)</div><div class="line"></div><div class="line">### 1、聚类过程</div><div class="line">- 聚类属于无监督学习，不知道y的标记分为K类</div><div class="line">- K-Means算法分为两个步骤</div><div class="line"> - 第一步：簇分配，随机选`K`个点作为中心，计算到这`K`个点的距离，分为`K`个簇</div><div class="line"> - 第二步：移动聚类中心：重新计算每个**簇**的中心，移动中心，重复以上步骤。</div><div class="line">- 如下图所示：</div><div class="line"> - 随机分配的聚类中心  </div><div class="line"> ![enter description here][34]</div><div class="line"> - 重新计算聚类中心，移动一次  </div><div class="line"> ![enter description here][35]</div><div class="line"> - 最后`10`步之后的聚类中心  </div><div class="line"> ![enter description here][36]</div><div class="line"></div><div class="line">- 计算每条数据到哪个中心最近实现代码：</div></pre></td></tr></table></figure>
<h1 id="找到每条数据距离哪个类中心最近"><a href="#找到每条数据距离哪个类中心最近" class="headerlink" title="找到每条数据距离哪个类中心最近"></a>找到每条数据距离哪个类中心最近</h1><p>def findClosestCentroids(X,initial_centroids):<br>    m = X.shape[0]                  # 数据条数<br>    K = initial_centroids.shape[0]  # 类的总数<br>    dis = np.zeros((m,K))           # 存储计算每个点分别到K个类的距离<br>    idx = np.zeros((m,1))           # 要返回的每条数据属于哪个类</p>
<pre><code>&apos;&apos;&apos;计算每个点到每个类中心的距离&apos;&apos;&apos;
for i in range(m):
    for j in range(K):
        dis[i,j] = np.dot((X[i,:]-initial_centroids[j,:]).reshape(1,-1),(X[i,:]-initial_centroids[j,:]).reshape(-1,1))

&apos;&apos;&apos;返回dis每一行的最小值对应的列号，即为对应的类别
- np.min(dis, axis=1)返回每一行的最小值
- np.where(dis == np.min(dis, axis=1).reshape(-1,1)) 返回对应最小值的坐标
 - 注意：可能最小值对应的坐标有多个，where都会找出来，所以返回时返回前m个需要的即可（因为对于多个最小值，属于哪个类别都可以）
&apos;&apos;&apos;  
dummy,idx = np.where(dis == np.min(dis, axis=1).reshape(-1,1))
return idx[0:dis.shape[0]]  # 注意截取一下
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 计算类中心实现代码：</div></pre></td></tr></table></figure>
<h1 id="计算类中心"><a href="#计算类中心" class="headerlink" title="计算类中心"></a>计算类中心</h1><p>def computerCentroids(X,idx,K):<br>    n = X.shape<a href="/assets/blog_images/ml_python_images/LinearRegression_01.png" title="LinearRegression_01.png">1</a><br>    centroids = np.zeros((K,n))<br>    for i in range(K):<br>        centroids[i,:] = np.mean(X[np.ravel(idx==i),:], axis=0).reshape(1,-1)   # 索引要是一维的,axis=0为每一列，idx==i一次找出属于哪一类的，然后计算均值<br>    return centroids<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 2、目标函数</div><div class="line">- 也叫做**失真代价函数**</div><div class="line">- ![J(&#123;c^&#123;(1)&#125;&#125;, \cdots ,&#123;c^&#123;(m)&#125;&#125;,&#123;u_1&#125;, \cdots ,&#123;u_k&#125;) = \frac&#123;1&#125;&#123;m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - &#123;u_1c^&#123;(i)&#125;&#125;&#125;&#125;|&#123;|^2&#125;&#125; ](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=J%28%7Bc%5E%7B%281%29%7D%7D%2C%20%5Ccdots%20%2C%7Bc%5E%7B%28m%29%7D%7D%2C%7Bu_1%7D%2C%20%5Ccdots%20%2C%7Bu_k%7D%29%20%3D%20%5Cfrac%7B1%7D%7Bm%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20%7Bu_%7B%7Bc%5E%7B%28i%29%7D%7D%7D%7D%7C%7B%7C%5E2%7D%7D%20)</div><div class="line">- 最后我们想得到：  </div><div class="line">![enter description here][37]</div><div class="line">- 其中![&#123;c^&#123;(i)&#125;&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bc%5E%7B%28i%29%7D%7D)表示第`i`条数据距离哪个类中心最近，</div><div class="line">- 其中![&#123;u_i&#125;](http://chart.apis.google.com/chart?cht=tx&amp;chs=1x0&amp;chf=bg,s,FFFFFF00&amp;chco=000000&amp;chl=%7Bu_i%7D)即为聚类的中心</div><div class="line"></div><div class="line">### 3、聚类中心的选择</div><div class="line">- 随机初始化，从给定的数据中随机抽取K个作为聚类中心</div><div class="line">- 随机一次的结果可能不好，可以随机多次，最后取使代价函数最小的作为中心</div><div class="line">- 实现代码：(这里随机一次)</div></pre></td></tr></table></figure></p>
<h1 id="初始化类中心–随机取K个点作为聚类中心"><a href="#初始化类中心–随机取K个点作为聚类中心" class="headerlink" title="初始化类中心–随机取K个点作为聚类中心"></a>初始化类中心–随机取K个点作为聚类中心</h1><p>def kMeansInitCentroids(X,K):<br>    m = X.shape[0]<br>    m_arr = np.arange(0,m)      # 生成0-m-1<br>    centroids = np.zeros((K,X.shape<a href="/assets/blog_images/ml_python_images/LinearRegression_01.png" title="LinearRegression_01.png">1</a>))<br>    np.random.shuffle(m_arr)    # 打乱m_arr顺序<br>    rand_indices = m_arr[:K]    # 取前K个<br>    centroids = X[rand_indices,:]<br>    return centroids<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 4、聚类个数K的选择</div><div class="line">- 聚类是不知道y的label的，所以不知道真正的聚类个数</div><div class="line">- 肘部法则（Elbow method）</div><div class="line"> - 作代价函数`J`和`K`的图，若是出现一个拐点，如下图所示，`K`就取拐点处的值，下图此时`K=3`</div><div class="line"> ![enter description here][38]</div><div class="line"> - 若是很平滑就不明确，人为选择。</div><div class="line">- 第二种就是人为观察选择</div><div class="line"></div><div class="line">### 5、应用——图片压缩</div><div class="line">- 将图片的像素分为若干类，然后用这个类代替原来的像素值</div><div class="line">- 执行聚类的算法代码：</div></pre></td></tr></table></figure></p>
<h1 id="聚类算法"><a href="#聚类算法" class="headerlink" title="聚类算法"></a>聚类算法</h1><p>def runKMeans(X,initial_centroids,max_iters,plot_process):<br>    m,n = X.shape                   # 数据条数和维度<br>    K = initial_centroids.shape[0]  # 类数<br>    centroids = initial_centroids   # 记录当前类中心<br>    previous_centroids = centroids  # 记录上一次类中心<br>    idx = np.zeros((m,1))           # 每条数据属于哪个类</p>
<pre><code>for i in range(max_iters):      # 迭代次数
    print u&apos;迭代计算次数：%d&apos;%(i+1)
    idx = findClosestCentroids(X, centroids)
    if plot_process:    # 如果绘制图像
        plt = plotProcessKMeans(X,centroids,previous_centroids) # 画聚类中心的移动过程
        previous_centroids = centroids  # 重置
    centroids = computerCentroids(X, idx, K)    # 重新计算类中心
if plot_process:    # 显示最终的绘制结果
    plt.show()
return centroids,idx    # 返回聚类中心和数据属于哪个类
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 6、[使用scikit-learn库中的线性模型实现聚类](/K-Means/K-Means_scikit-learn.py)</div><div class="line"></div><div class="line">- 导入包</div></pre></td></tr></table></figure>
<pre><code>from sklearn.cluster import KMeans
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 使用模型拟合数据</div></pre></td></tr></table></figure>
<pre><code>model = KMeans(n_clusters=3).fit(X) # n_clusters指定3类，拟合数据
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 聚类中心</div></pre></td></tr></table></figure>
<pre><code>centroids = model.cluster_centers_  # 聚类中心
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 7、运行结果</div><div class="line">- 二维数据类中心的移动  </div><div class="line">![enter description here][39]</div><div class="line">- 图片压缩  </div><div class="line">![enter description here][40]</div><div class="line"></div><div class="line"></div><div class="line">----------------------</div><div class="line"></div><div class="line">## 六、PCA主成分分析（降维）</div><div class="line">- [全部代码](/PCA/PCA.py)</div><div class="line"></div><div class="line">### 1、用处</div><div class="line">- 数据压缩（Data Compression）,使程序运行更快</div><div class="line">- 可视化数据，例如`3D--&gt;2D`等</div><div class="line">- ......</div><div class="line"></div><div class="line">### 2、2D--&gt;1D，nD--&gt;kD</div><div class="line">- 如下图所示，所有数据点可以投影到一条直线，是**投影距离的平方和**（投影误差）最小</div><div class="line">![enter description here][41]</div><div class="line">- 注意数据需要`归一化`处理</div><div class="line">- 思路是找`1`个`向量u`,所有数据投影到上面使投影距离最小</div><div class="line">- 那么`nD--&gt;kD`就是找`k`个向量![$$&#123;u^&#123;(1)&#125;&#125;,&#123;u^&#123;(2)&#125;&#125; \ldots &#123;u^&#123;(k)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%20%5Cldots%20%7Bu%5E%7B%28k%29%7D%7D%24%24)，所有数据投影到上面使投影误差最小</div><div class="line"> - eg:3D--&gt;2D,2个向量![$$&#123;u^&#123;(1)&#125;&#125;,&#123;u^&#123;(2)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7Bu%5E%7B%281%29%7D%7D%2C%7Bu%5E%7B%282%29%7D%7D%24%24)就代表一个平面了，所有点投影到这个平面的投影误差最小即可</div><div class="line"></div><div class="line">### 3、主成分分析PCA与线性回归的区别</div><div class="line">- 线性回归是找`x`与`y`的关系，然后用于预测`y`</div><div class="line">- `PCA`是找一个投影面，最小化data到这个投影面的投影误差</div><div class="line"></div><div class="line">### 4、PCA降维过程</div><div class="line">- 数据预处理（均值归一化）</div><div class="line"> - 公式：![$$&#123;\rm&#123;x&#125;&#125;_j^&#123;(i)&#125; = 1&#123;\rm&#123;x&#125;&#125;_j^&#123;(i)&#125; - &#123;u_j&#125;&#125; \over 1s_j&#125;&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20%3D%20%7B%7B%7B%5Crm%7Bx%7D%7D_j%5E%7B%28i%29%7D%20-%20%7Bu_j%7D%7D%20%5Cover%20%7B%7Bs_j%7D%7D%7D%24%24)</div><div class="line"> - 就是减去对应feature的均值，然后除以对应特征的标准差（也可以是最大值-最小值）</div><div class="line"> - 实现代码：</div></pre></td></tr></table></figure>
<pre><code> # 归一化数据
def featureNormalize(X):
    &apos;&apos;&apos;（每一个数据-当前列的均值）/当前列的标准差&apos;&apos;&apos;
    n = X.shape[1]
    mu = np.zeros((1,n));
    sigma = np.zeros((1,n))

    mu = np.mean(X,axis=0)
    sigma = np.std(X,axis=0)
    for i in range(n):
        X[:,i] = (X[:,i]-mu[i])/sigma[i]
    return X,mu,sigma
</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">- 计算`协方差矩阵Σ`（Covariance Matrix）：![$$\Sigma  = &#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^n 1x^&#123;(i)&#125;&#125;1(&#123;x^&#123;(i)&#125;&#125;)&#125;^T&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%24%24%5CSigma%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7Bx%5E%7B%28i%29%7D%7D%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%29%7D%5ET%7D%7D%20%24%24)</div><div class="line"> - 注意这里的`Σ`和求和符号不同</div><div class="line"> - 协方差矩阵`对称正定`（不理解正定的看看线代）</div><div class="line"> - 大小为`nxn`,`n`为`feature`的维度</div><div class="line"> - 实现代码：</div></pre></td></tr></table></figure>
<p> Sigma = np.dot(np.transpose(X_norm),X_norm)/m  # 求Sigma<br> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">- 计算`Σ`的特征值和特征向量</div><div class="line"> - 可以是用`svd`奇异值分解函数：`U,S,V = svd(Σ)`</div><div class="line"> - 返回的是与`Σ`同样大小的对角阵`S`（由`Σ`的特征值组成）[**注意**：`matlab`中函数返回的是对角阵，在`python`中返回的是一个向量，节省空间]</div><div class="line"> - 还有两个**酉矩阵**U和V，且![$$\Sigma  = US&#123;V^T&#125;$$](http://latex.codecogs.com/gif.latex?%24%24%5CSigma%20%3D%20US%7BV%5ET%7D%24%24)</div><div class="line"> - ![enter description here][42]</div><div class="line"> - **注意**：`svd`函数求出的`S`是按特征值降序排列的，若不是使用`svd`,需要按**特征值**大小重新排列`U`</div><div class="line">- 降维</div><div class="line"> - 选取`U`中的前`K`列（假设要降为`K`维）</div><div class="line"> - ![enter description here][43]</div><div class="line"> - `Z`就是对应降维之后的数据</div><div class="line"> - 实现代码：</div></pre></td></tr></table></figure></p>
<pre><code> # 映射数据
def projectData(X_norm,U,K):
    Z = np.zeros((X_norm.shape[0],K))

    U_reduce = U[:,0:K]          # 取前K个
    Z = np.dot(X_norm,U_reduce) 
    return Z
</code></pre> <figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">- 过程总结：</div><div class="line"> - `Sigma = X&apos;*X/m`</div><div class="line"> - `U,S,V = svd(Sigma)`</div><div class="line"> - `Ureduce = U[:,0:k]`</div><div class="line"> - `Z = Ureduce&apos;*x`</div><div class="line"></div><div class="line">### 5、数据恢复</div><div class="line"> - 因为：![$$&#123;Z^&#123;(i)&#125;&#125; = U_&#123;reduce&#125;^T*&#123;X^&#123;(i)&#125;&#125;$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BZ%5E%7B%28i%29%7D%7D%20%3D%20U_%7Breduce%7D%5ET*%7BX%5E%7B%28i%29%7D%7D%24%24)</div><div class="line"> - 所以：![$$&#123;X_&#123;approx&#125;&#125; = &#123;(U_&#123;reduce&#125;^T)^&#123; - 1&#125;&#125;Z$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5ET%29%5E%7B%20-%201%7D%7DZ%24%24)     （注意这里是X的近似值）</div><div class="line"> - 又因为`Ureduce`为正定矩阵，【正定矩阵满足：![$$A&#123;A^T&#125; = &#123;A^T&#125;A = E$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24A%7BA%5ET%7D%20%3D%20%7BA%5ET%7DA%20%3D%20E%24%24)，所以：![$$&#123;A^&#123; - 1&#125;&#125; = &#123;A^T&#125;$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BA%5E%7B%20-%201%7D%7D%20%3D%20%7BA%5ET%7D%24%24)】，所以这里：</div><div class="line"> - ![$$&#123;X_&#123;approx&#125;&#125; = &#123;(U_&#123;reduce&#125;^&#123; - 1&#125;)^&#123; - 1&#125;&#125;Z = &#123;U_&#123;reduce&#125;&#125;Z$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7BX_%7Bapprox%7D%7D%20%3D%20%7B%28U_%7Breduce%7D%5E%7B%20-%201%7D%29%5E%7B%20-%201%7D%7DZ%20%3D%20%7BU_%7Breduce%7D%7DZ%24%24)</div><div class="line"> - 实现代码：</div></pre></td></tr></table></figure>
<pre><code># 恢复数据 
def recoverData(Z,U,K):
    X_rec = np.zeros((Z.shape[0],U.shape[0]))
    U_recude = U[:,0:K]
    X_rec = np.dot(Z,np.transpose(U_recude))  # 还原数据（近似）
    return X_rec
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 6、主成分个数的选择（即要降的维度）</div><div class="line">- 如何选择</div><div class="line"> - **投影误差**（project error）：![$$&#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - x_&#123;approx&#125;^&#123;(i)&#125;|&#123;|^2&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%24%24)</div><div class="line"> - **总变差**（total variation）:![$$&#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125;|&#123;|^2&#125;&#125; $$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%24%24)</div><div class="line"> - 若**误差率**（error ratio）：![$$1&#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125; - x_&#123;approx&#125;^&#123;(i)&#125;|&#123;|^2&#125;&#125; &#125; \over 11 \over m&#125;\sum\limits_&#123;i = 1&#125;^m &#123;||&#123;x^&#123;(i)&#125;&#125;|&#123;|^2&#125;&#125; &#125;&#125; \le 0.01$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24%7B%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%20-%20x_%7Bapprox%7D%5E%7B%28i%29%7D%7C%7B%7C%5E2%7D%7D%20%7D%20%5Cover%20%7B%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7C%7C%7Bx%5E%7B%28i%29%7D%7D%7C%7B%7C%5E2%7D%7D%20%7D%7D%20%5Cle%200.01%24%24)，则称`99%`保留差异性</div><div class="line"> - 误差率一般取`1%，5%，10%`等</div><div class="line">- 如何实现</div><div class="line"> - 若是一个个试的话代价太大</div><div class="line"> - 之前`U,S,V = svd(Sigma)`,我们得到了`S`，这里误差率error ratio:    </div><div class="line"> ![$$error&#123;\kern 1pt&#125; \;ratio = 1 - 1\sum\limits_&#123;i = 1&#125;^k 1S_&#123;ii&#125;&#125;&#125; &#125; \over &#123;\sum\limits_&#123;i = 1&#125;^n 1S_&#123;ii&#125;&#125;&#125; &#125;&#125; \le threshold$$](http://latex.codecogs.com/gif.latex?%5Cfn_cm%20%24%24error%7B%5Ckern%201pt%7D%20%5C%3Bratio%20%3D%201%20-%20%7B%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5Ek%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%20%5Cover%20%7B%5Csum%5Climits_%7Bi%20%3D%201%7D%5En%20%7B%7BS_%7Bii%7D%7D%7D%20%7D%7D%20%5Cle%20threshold%24%24)</div><div class="line"> - 可以一点点增加`K`尝试。</div><div class="line"></div><div class="line">### 7、使用建议</div><div class="line">- 不要使用PCA去解决过拟合问题`Overfitting`，还是使用正则化的方法（如果保留了很高的差异性还是可以的）</div><div class="line">- 只有在原数据上有好的结果，但是运行很慢，才考虑使用PCA</div><div class="line"></div><div class="line">### 8、运行结果</div><div class="line">- 2维数据降为1维</div><div class="line"> - 要投影的方向     </div><div class="line">![enter description here][44]</div><div class="line"> - 2D降为1D及对应关系        </div><div class="line">![enter description here][45]</div><div class="line">- 人脸数据降维</div><div class="line"> - 原始数据         </div><div class="line"> ![enter description here][46]</div><div class="line"> - 可视化部分`U`矩阵信息    </div><div class="line"> ![enter description here][47]</div><div class="line"> - 恢复数据    </div><div class="line"> ![enter description here][48]</div><div class="line"></div><div class="line">### 9、[使用scikit-learn库中的PCA实现降维](/PCA/PCA.py_scikit-learn.py)</div><div class="line">- 导入需要的包：</div></pre></td></tr></table></figure>
<p>#-<em>- coding: utf-8 -</em>-</p>
<h1 id="Author-bob"><a href="#Author-bob" class="headerlink" title="Author:bob"></a>Author:bob</h1><h1 id="Date-2016-12-22"><a href="#Date-2016-12-22" class="headerlink" title="Date:2016.12.22"></a>Date:2016.12.22</h1><p>import numpy as np<br>from matplotlib import pyplot as plt<br>from scipy import io as spio<br>from sklearn.decomposition import pca<br>from sklearn.preprocessing import StandardScaler<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">- 归一化数据</div></pre></td></tr></table></figure></p>
<pre><code>&apos;&apos;&apos;归一化数据并作图&apos;&apos;&apos;
scaler = StandardScaler()
scaler.fit(X)
x_train = scaler.transform(X)
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">- 使用PCA模型拟合数据，并降维</div><div class="line"> - `n_components`对应要将的维度</div></pre></td></tr></table></figure>
<pre><code>&apos;&apos;&apos;拟合数据&apos;&apos;&apos;
K=1 # 要降的维度
model = pca.PCA(n_components=K).fit(x_train)   # 拟合数据，n_components定义要降的维度
Z = model.transform(x_train)    # transform就会执行降维操作
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">- 数据恢复</div><div class="line"> - `model.components_`会得到降维使用的`U`矩阵</div></pre></td></tr></table></figure>
<pre><code>&apos;&apos;&apos;数据恢复并作图&apos;&apos;&apos;
Ureduce = model.components_     # 得到降维用的Ureduce
x_rec = np.dot(Z,Ureduce)       # 数据恢复
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"></div><div class="line"></div><div class="line"></div><div class="line">---------------------------------------------------------------</div><div class="line"></div><div class="line"></div><div class="line">## 七、异常检测 Anomaly Detection</div><div class="line">- [全部代码](/AnomalyDetection/AnomalyDetection.py)</div><div class="line"></div><div class="line">### 1、高斯分布（正态分布）`Gaussian distribution` </div><div class="line">- 分布函数：![$$p(x) = &#123;1 \over &#123;\sqrt &#123;2\pi &#125; \sigma &#125;&#125;&#123;e^&#123; - 11(x - u)&#125;^2&#125;&#125; \over &#123;2&#123;\sigma ^2&#125;&#125;&#125;&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%5Csqrt%20%7B2%5Cpi%20%7D%20%5Csigma%20%7D%7D%7Be%5E%7B%20-%20%7B%7B%7B%7B%28x%20-%20u%29%7D%5E2%7D%7D%20%5Cover%20%7B2%7B%5Csigma%20%5E2%7D%7D%7D%7D%7D%24%24)</div><div class="line"> - 其中，`u`为数据的**均值**，`σ`为数据的**标准差**</div><div class="line"> - `σ`越**小**，对应的图像越**尖**</div><div class="line">- 参数估计（`parameter estimation`）</div><div class="line"> - ![$$u = &#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^m 1x^&#123;(i)&#125;&#125;&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24u%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7Bx%5E%7B%28i%29%7D%7D%7D%20%24%24)</div><div class="line"> - ![$$&#123;\sigma ^2&#125; = &#123;1 \over m&#125;\sum\limits_&#123;i = 1&#125;^m 1&#123;(&#123;x^&#123;(i)&#125;&#125; - u)&#125;^2&#125;&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7B%5Csigma%20%5E2%7D%20%3D%20%7B1%20%5Cover%20m%7D%5Csum%5Climits_%7Bi%20%3D%201%7D%5Em%20%7B%7B%7B%28%7Bx%5E%7B%28i%29%7D%7D%20-%20u%29%7D%5E2%7D%7D%20%24%24)</div><div class="line"></div><div class="line">### 2、异常检测算法</div><div class="line">- 例子</div><div class="line"> - 训练集：![$$\&#123; &#123;x^&#123;(1)&#125;&#125;,&#123;x^&#123;(2)&#125;&#125;, \cdots &#123;x^&#123;(m)&#125;&#125;\&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5C%7B%20%7Bx%5E%7B%281%29%7D%7D%2C%7Bx%5E%7B%282%29%7D%7D%2C%20%5Ccdots%20%7Bx%5E%7B%28m%29%7D%7D%5C%7D%20%24%24),其中![$$x \in &#123;R^n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24)</div><div class="line"> - 假设![$$&#123;x_1&#125;,&#123;x_2&#125; \cdots &#123;x_n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7Bx_1%7D%2C%7Bx_2%7D%20%5Ccdots%20%7Bx_n%7D%24%24)相互独立，建立model模型：![$$p(x) = p(&#123;x_1&#125;;&#123;u_1&#125;,\sigma _1^2)p(&#123;x_2&#125;;&#123;u_2&#125;,\sigma _2^2) \cdots p(&#123;x_n&#125;;&#123;u_n&#125;,\sigma _n^2) = \prod\limits_&#123;j = 1&#125;^n &#123;p(&#123;x_j&#125;;&#123;u_j&#125;,\sigma _j^2)&#125; $$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20p%28%7Bx_1%7D%3B%7Bu_1%7D%2C%5Csigma%20_1%5E2%29p%28%7Bx_2%7D%3B%7Bu_2%7D%2C%5Csigma%20_2%5E2%29%20%5Ccdots%20p%28%7Bx_n%7D%3B%7Bu_n%7D%2C%5Csigma%20_n%5E2%29%20%3D%20%5Cprod%5Climits_%7Bj%20%3D%201%7D%5En%20%7Bp%28%7Bx_j%7D%3B%7Bu_j%7D%2C%5Csigma%20_j%5E2%29%7D%20%24%24)</div><div class="line">- 过程</div><div class="line"> - 选择具有代表异常的`feature`:xi</div><div class="line"> - 参数估计：![$$&#123;u_1&#125;,&#123;u_2&#125;, \cdots ,&#123;u_n&#125;;\sigma _1^2,\sigma _2^2 \cdots ,\sigma _n^2$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7Bu_1%7D%2C%7Bu_2%7D%2C%20%5Ccdots%20%2C%7Bu_n%7D%3B%5Csigma%20_1%5E2%2C%5Csigma%20_2%5E2%20%5Ccdots%20%2C%5Csigma%20_n%5E2%24%24)</div><div class="line"> - 计算`p(x)`,若是`P(x)&lt;ε`则认为异常，其中`ε`为我们要求的概率的临界值`threshold`</div><div class="line">- 这里只是**单元高斯分布**，假设了`feature`之间是独立的，下面会讲到**多元高斯分布**，会自动捕捉到`feature`之间的关系</div><div class="line">- **参数估计**实现代码</div></pre></td></tr></table></figure>
<h1 id="参数估计函数（就是求均值和方差）"><a href="#参数估计函数（就是求均值和方差）" class="headerlink" title="参数估计函数（就是求均值和方差）"></a>参数估计函数（就是求均值和方差）</h1><p>def estimateGaussian(X):<br>    m,n = X.shape<br>    mu = np.zeros((n,1))<br>    sigma2 = np.zeros((n,1))</p>
<pre><code>mu = np.mean(X, axis=0) # axis=0表示列，每列的均值
sigma2 = np.var(X,axis=0) # 求每列的方差
return mu,sigma2
</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 3、评价`p(x)`的好坏，以及`ε`的选取</div><div class="line">- 对**偏斜数据**的错误度量</div><div class="line"> - 因为数据可能是非常**偏斜**的（就是`y=1`的个数非常少，(`y=1`表示异常)），所以可以使用`Precision/Recall`，计算`F1Score`(在**CV交叉验证集**上)</div><div class="line"> - 例如：预测癌症，假设模型可以得到`99%`能够预测正确，`1%`的错误率，但是实际癌症的概率很小，只有`0.5%`，那么我们始终预测没有癌症y=0反而可以得到更小的错误率。使用`error rate`来评估就不科学了。</div><div class="line"> - 如下图记录：    </div><div class="line"> ![enter description here][49]</div><div class="line"> - ![$$\Pr ecision = 1TP&#125; \over &#123;TP + FP&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5CPr%20ecision%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&amp;plus;%20FP%7D%7D%24%24) ，即：**正确预测正样本/所有预测正样本**</div><div class="line"> - ![$$&#123;\mathop&#123;\rm Re&#125;\nolimits&#125; &#123;\rm&#123;call&#125;&#125; = 1TP&#125; \over &#123;TP + FN&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7B%5Cmathop%7B%5Crm%20Re%7D%5Cnolimits%7D%20%7B%5Crm%7Bcall%7D%7D%20%3D%20%7B%7BTP%7D%20%5Cover%20%7BTP%20&amp;plus;%20FN%7D%7D%24%24) ，即：**正确预测正样本/真实值为正样本**</div><div class="line"> - 总是让`y=1`(较少的类)，计算`Precision`和`Recall`</div><div class="line"> - ![$$&#123;F_1&#125;Score = 21PR&#125; \over &#123;P + R&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%7BF_1%7DScore%20%3D%202%7B%7BPR%7D%20%5Cover%20%7BP%20&amp;plus;%20R%7D%7D%24%24)</div><div class="line"> - 还是以癌症预测为例，假设预测都是no-cancer，TN=199，FN=1，TP=0，FP=0，所以：Precision=0/0，Recall=0/1=0，尽管accuracy=199/200=99.5%，但是不可信。</div><div class="line"></div><div class="line">- `ε`的选取</div><div class="line"> - 尝试多个`ε`值，使`F1Score`的值高</div><div class="line">- 实现代码</div></pre></td></tr></table></figure>
<h1 id="选择最优的epsilon，即：使F1Score最大"><a href="#选择最优的epsilon，即：使F1Score最大" class="headerlink" title="选择最优的epsilon，即：使F1Score最大"></a>选择最优的epsilon，即：使F1Score最大</h1><p>def selectThreshold(yval,pval):<br>    ‘’’初始化所需变量’’’<br>    bestEpsilon = 0.<br>    bestF1 = 0.<br>    F1 = 0.<br>    step = (np.max(pval)-np.min(pval))/1000<br>    ‘’’计算’’’<br>    for epsilon in np.arange(np.min(pval),np.max(pval),step):<br>        cvPrecision = pval<epsilon tp="np.sum((cvPrecision" =="1)" &="" (yval="=" 1)).astype(float)="" #="" sum求和是int型的，需要转为float="" fp="np.sum((cvPrecision" 0)).astype(float)="" fn="np.sum((cvPrecision" precision="tp/(tp+fp)" 精准度="" recision="tp/(tp+fn)" 召回率="" f1="(2*precision*recision)/(precision+recision)" f1score计算公式="" if=""> bestF1:  # 修改最优的F1 Score<br>            bestF1 = F1<br>            bestEpsilon = epsilon<br>    return bestEpsilon,bestF1<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"></div><div class="line">### 4、选择使用什么样的feature（单元高斯分布）</div><div class="line">- 如果一些数据不是满足高斯分布的，可以变化一下数据，例如`log(x+C),x^(1/2)`等</div><div class="line">- 如果`p(x)`的值无论异常与否都很大，可以尝试组合多个`feature`,(因为feature之间可能是有关系的)</div><div class="line"></div><div class="line">### 5、多元高斯分布</div><div class="line">- 单元高斯分布存在的问题</div><div class="line"> - 如下图，红色的点为异常点，其他的都是正常点（比如CPU和memory的变化）   </div><div class="line"> ![enter description here][50]</div><div class="line"> - x1对应的高斯分布如下：   </div><div class="line"> ![enter description here][51]</div><div class="line"> - x2对应的高斯分布如下：   </div><div class="line"> ![enter description here][52]</div><div class="line"> - 可以看出对应的p(x1)和p(x2)的值变化并不大，就不会认为异常</div><div class="line"> - 因为我们认为feature之间是相互独立的，所以如上图是以**正圆**的方式扩展</div><div class="line">- 多元高斯分布</div><div class="line"> - ![$$x \in &#123;R^n&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24x%20%5Cin%20%7BR%5En%7D%24%24)，并不是建立`p(x1),p(x2)...p(xn)`，而是统一建立`p(x)`</div><div class="line"> - 其中参数：![$$\mu  \in &#123;R^n&#125;,\Sigma  \in &#123;R^&#123;n \times &#123;\rm&#123;n&#125;&#125;&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5Cmu%20%5Cin%20%7BR%5En%7D%2C%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24),`Σ`为**协方差矩阵**</div><div class="line"> - ![$$p(x) = &#123;1 \over 1&#123;(2\pi )&#125;^1n \over 2&#125;&#125;&#125;|\Sigma &#123;|^11 \over 2&#125;&#125;&#125;&#125;&#125;&#123;e^&#123; - &#123;1 \over 2&#125;1(x - u)&#125;^T&#125;&#123;\Sigma ^&#123; - 1&#125;&#125;(x - u)&#125;&#125;$$](http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24p%28x%29%20%3D%20%7B1%20%5Cover%20%7B%7B%7B%282%5Cpi%20%29%7D%5E%7B%7Bn%20%5Cover%202%7D%7D%7D%7C%5CSigma%20%7B%7C%5E%7B%7B1%20%5Cover%202%7D%7D%7D%7D%7D%7Be%5E%7B%20-%20%7B1%20%5Cover%202%7D%7B%7B%28x%20-%20u%29%7D%5ET%7D%7B%5CSigma%20%5E%7B%20-%201%7D%7D%28x%20-%20u%29%7D%7D%24%24)</div><div class="line"> - 同样，`|Σ|`越小，`p(x)`越尖</div><div class="line"> - 例如：    </div><div class="line"> ![enter description here][53]，  </div><div class="line"> 表示x1,x2**正相关**，即x1越大，x2也就越大，如下图，也就可以将红色的异常点检查出了</div><div class="line"> ![enter description here][54]      </div><div class="line"> 若：   </div><div class="line">  ![enter description here][55]，   </div><div class="line"> 表示x1,x2**负相关**</div><div class="line">- 实现代码：</div></pre></td></tr></table></figure></epsilon></p>
<h1 id="多元高斯分布函数"><a href="#多元高斯分布函数" class="headerlink" title="多元高斯分布函数"></a>多元高斯分布函数</h1><p>def multivariateGaussian(X,mu,Sigma2):<br>    k = len(mu)<br>    if (Sigma2.shape[0]&gt;1):<br>        Sigma2 = np.diag(Sigma2)<br>    ‘’’多元高斯分布函数’’’<br>    X = X-mu<br>    argu = (2<em>np.pi)**(-k/2)</em>np.linalg.det(Sigma2)<em>*(-0.5)<br>    p = argu</em>np.exp(-0.5<em>np.sum(np.dot(X,np.linalg.inv(Sigma2))</em>X,axis=1))  # axis表示每行<br>    return p<br>```</p>
<h3 id="6、单元和多元高斯分布特点"><a href="#6、单元和多元高斯分布特点" class="headerlink" title="6、单元和多元高斯分布特点"></a>6、单元和多元高斯分布特点</h3><ul>
<li>单元高斯分布<ul>
<li>人为可以捕捉到<code>feature</code>之间的关系时可以使用</li>
<li>计算量小</li>
</ul>
</li>
<li>多元高斯分布<ul>
<li>自动捕捉到相关的feature</li>
<li>计算量大，因为：<img src="http://latex.codecogs.com/png.latex?%5Cfn_cm%20%24%24%5CSigma%20%5Cin%20%7BR%5E%7Bn%20%5Ctimes%20%7B%5Crm%7Bn%7D%7D%7D%7D%24%24" alt="$$\Sigma  \in {R^{n \times {\rm{n}}}}$$"></li>
<li><code>m&gt;n</code>或<code>Σ</code>可逆时可以使用。（若不可逆，可能有冗余的x，因为线性相关，不可逆，或者就是m&lt;n）</li>
</ul>
</li>
</ul>
<h3 id="7、程序运行结果"><a href="#7、程序运行结果" class="headerlink" title="7、程序运行结果"></a>7、程序运行结果</h3><ul>
<li>显示数据<br><img src="/assets/blog_images/ml_python_images/AnomalyDetection_08.png" alt="enter description here" title="AnomalyDetection_08.png"></li>
<li>等高线<br><img src="/assets/blog_images/ml_python_images/AnomalyDetection_09.png" alt="enter description here" title="AnomalyDetection_09.png"></li>
<li>异常点标注<br><img src="/assets/blog_images/ml_python_images/AnomalyDetection_10.png" alt="enter description here" title="AnomalyDetection_10.png"></li>
</ul>
<hr>

      

      
        <div class="page-reward">
          <a href="javascript:;" class="page-reward-btn tooltip-top">
            <div class="tooltip tooltip-east">
            <span class="tooltip-item">
              赏
            </span>
            <span class="tooltip-content">
              <span class="tooltip-text">
                <span class="tooltip-inner">
                  <p class="reward-p"><i class="icon icon-quo-left"></i>谢谢你请我吃糖果<i class="icon icon-quo-right"></i></p>
                  <div class="reward-box">
                    
                    <div class="reward-box-item">
                      <img class="reward-img" src="/assets/img/alipay.jpg">
                      <span class="reward-type">支付宝</span>
                    </div>
                    
                    
                  </div>
                </span>
              </span>
            </span>
          </div>
          </a>
        </div>
      
    </div>
    <div class="article-info article-info-index">
      
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags icon"></i>
		<ul class="article-tag-list">
			 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color2">Python</a>
        		</li>
      		 
        		<li class="article-tag-list-item">
        			<a href="javascript:void(0)" class="js-tag article-tag-list-link color5">机器学习</a>
        		</li>
      		
		</ul>
	</div>

      

      

      
        
<div class="share-btn share-icons tooltip-left">
  <div class="tooltip tooltip-east">
    <span class="tooltip-item">
      <a href="javascript:;" class="share-sns share-outer">
        <i class="icon icon-share"></i>
      </a>
    </span>
    <span class="tooltip-content">
      <div class="share-wrap">
        <div class="share-icons">
          <a class="weibo share-sns" href="javascript:;" data-type="weibo">
            <i class="icon icon-weibo"></i>
          </a>
          <a class="weixin share-sns wxFab" href="javascript:;" data-type="weixin">
            <i class="icon icon-weixin"></i>
          </a>
          <a class="qq share-sns" href="javascript:;" data-type="qq">
            <i class="icon icon-qq"></i>
          </a>
          <a class="douban share-sns" href="javascript:;" data-type="douban">
            <i class="icon icon-douban"></i>
          </a>
          <a class="qzone share-sns" href="javascript:;" data-type="qzone">
            <i class="icon icon-qzone"></i>
          </a>
          <a class="facebook share-sns" href="javascript:;" data-type="facebook">
            <i class="icon icon-facebook"></i>
          </a>
          <a class="twitter share-sns" href="javascript:;" data-type="twitter">
            <i class="icon icon-twitter"></i>
          </a>
          <a class="google share-sns" href="javascript:;" data-type="google">
            <i class="icon icon-google"></i>
          </a>
        </div>
      </div>
    </span>
  </div>
</div>

<div class="page-modal wx-share js-wx-box">
    <a class="close js-modal-close" href="javascript:;"><i class="icon icon-close"></i></a>
    <p>扫一扫，分享到微信</p>
    <div class="wx-qrcode">
      <img src="http://s.jiathis.com/qrcode.php?url=http://lawlite.com/2017/01/08/Python机器学习/" alt="微信分享二维码">
    </div>
</div>

<div class="mask js-mask"></div>
      
      <div class="clearfix"></div>
    </div>
  </div>
</article>

  
<nav id="article-nav">
  
    <a href="/2016/11/09/Python科学计算/" id="article-nav-newer" class="article-nav-link-wrap">
      <i class="icon-circle-left"></i>
      <div class="article-nav-title">
        
          Python科学计算
        
      </div>
    </a>
  
  
    <a href="/2017/03/24/致火影/" id="article-nav-older" class="article-nav-link-wrap">
      <div class="article-nav-title">致火影</div>
      <i class="icon-circle-right"></i>
    </a>
  
</nav>






  
    <div class="duoshuo"></div>
  






<section class="youyan" id="comments">
<!-- 来必力City版安装代码 -->
<div id="lv-container" data-id="city" data-uid="MTAyMC8yODEwMy80Njc2">
	<script type="text/javascript">
   (function(d, s) {
       var j, e = d.getElementsByTagName(s)[0];

       if (typeof LivereTower === 'function') { return; }

       j = d.createElement(s);
       j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
       j.async = true;

       e.parentNode.insertBefore(j, e);
   })(document, 'script');
	</script>
<noscript> 为正常使用来必力评论功能请激活JavaScript</noscript>
</div>
<!-- City版安装代码已完成 -->
</section>



          </div>
        </div>
      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2017 Lawlite
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		mathjax: false,
		isHome: false,
		isPost: true,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: true,
		root: "/",
		innerArchive: true,
		showTags: true
	}
</script>

<script>!function(t){function n(r){if(e[r])return e[r].exports;var o=e[r]={exports:{},id:r,loaded:!1};return t[r].call(o.exports,o,o.exports,n),o.loaded=!0,o.exports}var e={};return n.m=t,n.c=e,n.p="./",n(0)}([function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}function o(t,n){var e=/\/|index.html/g;return t.replace(e,"")===n.replace(e,"")}function i(){for(var t=document.querySelectorAll(".js-header-menu li a"),n=window.location.pathname,e=0,r=t.length;e<r;e++){var i=t[e];o(n,i.getAttribute("href"))&&(0,d.default)(i,"active")}}function u(t){for(var n=t.offsetLeft,e=t.offsetParent;null!==e;)n+=e.offsetLeft,e=e.offsetParent;return n}function f(t){for(var n=t.offsetTop,e=t.offsetParent;null!==e;)n+=e.offsetTop,e=e.offsetParent;return n}function c(t,n,e,r,o){var i=u(t),c=f(t)-n;if(c-e<=o){var a=t.$newDom;a||(a=t.cloneNode(!0),(0,h.default)(t,a),t.$newDom=a,a.style.position="fixed",a.style.top=(e||c)+"px",a.style.left=i+"px",a.style.zIndex=r||2,a.style.width="100%",a.style.color="#fff"),a.style.visibility="visible",t.style.visibility="hidden"}else{t.style.visibility="visible";var s=t.$newDom;s&&(s.style.visibility="hidden")}}function a(){var t=document.querySelector(".js-overlay"),n=document.querySelector(".js-header-menu");c(t,document.body.scrollTop,-63,2,0),c(n,document.body.scrollTop,1,3,0)}function s(){document.querySelector("#container").addEventListener("scroll",function(t){a()}),window.addEventListener("scroll",function(t){a()}),a()}function l(){x.default.versions.mobile&&window.screen.width<800&&(i(),s())}var p=e(71),d=r(p),v=e(72),y=(r(v),e(84)),h=r(y),b=e(69),x=r(b),m=e(75),g=r(m),w=e(70);l(),(0,w.addLoadEvent)(function(){g.default.init()}),t.exports={}},function(t,n){var e=t.exports="undefined"!=typeof window&&window.Math==Math?window:"undefined"!=typeof self&&self.Math==Math?self:Function("return this")();"number"==typeof __g&&(__g=e)},function(t,n){var e={}.hasOwnProperty;t.exports=function(t,n){return e.call(t,n)}},function(t,n,e){var r=e(49),o=e(15);t.exports=function(t){return r(o(t))}},function(t,n,e){t.exports=!e(8)(function(){return 7!=Object.defineProperty({},"a",{get:function(){return 7}}).a})},function(t,n,e){var r=e(6),o=e(12);t.exports=e(4)?function(t,n,e){return r.f(t,n,o(1,e))}:function(t,n,e){return t[n]=e,t}},function(t,n,e){var r=e(10),o=e(30),i=e(24),u=Object.defineProperty;n.f=e(4)?Object.defineProperty:function(t,n,e){if(r(t),n=i(n,!0),r(e),o)try{return u(t,n,e)}catch(t){}if("get"in e||"set"in e)throw TypeError("Accessors not supported!");return"value"in e&&(t[n]=e.value),t}},function(t,n,e){var r=e(22)("wks"),o=e(13),i=e(1).Symbol,u="function"==typeof i,f=t.exports=function(t){return r[t]||(r[t]=u&&i[t]||(u?i:o)("Symbol."+t))};f.store=r},function(t,n){t.exports=function(t){try{return!!t()}catch(t){return!0}}},function(t,n,e){var r=e(35),o=e(16);t.exports=Object.keys||function(t){return r(t,o)}},function(t,n,e){var r=e(11);t.exports=function(t){if(!r(t))throw TypeError(t+" is not an object!");return t}},function(t,n){t.exports=function(t){return"object"==typeof t?null!==t:"function"==typeof t}},function(t,n){t.exports=function(t,n){return{enumerable:!(1&t),configurable:!(2&t),writable:!(4&t),value:n}}},function(t,n){var e=0,r=Math.random();t.exports=function(t){return"Symbol(".concat(void 0===t?"":t,")_",(++e+r).toString(36))}},function(t,n){var e=t.exports={version:"2.4.0"};"number"==typeof __e&&(__e=e)},function(t,n){t.exports=function(t){if(void 0==t)throw TypeError("Can't call method on  "+t);return t}},function(t,n){t.exports="constructor,hasOwnProperty,isPrototypeOf,propertyIsEnumerable,toLocaleString,toString,valueOf".split(",")},function(t,n){t.exports={}},function(t,n){t.exports=!0},function(t,n){n.f={}.propertyIsEnumerable},function(t,n,e){var r=e(6).f,o=e(2),i=e(7)("toStringTag");t.exports=function(t,n,e){t&&!o(t=e?t:t.prototype,i)&&r(t,i,{configurable:!0,value:n})}},function(t,n,e){var r=e(22)("keys"),o=e(13);t.exports=function(t){return r[t]||(r[t]=o(t))}},function(t,n,e){var r=e(1),o="__core-js_shared__",i=r[o]||(r[o]={});t.exports=function(t){return i[t]||(i[t]={})}},function(t,n){var e=Math.ceil,r=Math.floor;t.exports=function(t){return isNaN(t=+t)?0:(t>0?r:e)(t)}},function(t,n,e){var r=e(11);t.exports=function(t,n){if(!r(t))return t;var e,o;if(n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;if("function"==typeof(e=t.valueOf)&&!r(o=e.call(t)))return o;if(!n&&"function"==typeof(e=t.toString)&&!r(o=e.call(t)))return o;throw TypeError("Can't convert object to primitive value")}},function(t,n,e){var r=e(1),o=e(14),i=e(18),u=e(26),f=e(6).f;t.exports=function(t){var n=o.Symbol||(o.Symbol=i?{}:r.Symbol||{});"_"==t.charAt(0)||t in n||f(n,t,{value:u.f(t)})}},function(t,n,e){n.f=e(7)},function(t,n,e){var r=e(1),o=e(14),i=e(46),u=e(5),f="prototype",c=function(t,n,e){var a,s,l,p=t&c.F,d=t&c.G,v=t&c.S,y=t&c.P,h=t&c.B,b=t&c.W,x=d?o:o[n]||(o[n]={}),m=x[f],g=d?r:v?r[n]:(r[n]||{})[f];d&&(e=n);for(a in e)s=!p&&g&&void 0!==g[a],s&&a in x||(l=s?g[a]:e[a],x[a]=d&&"function"!=typeof g[a]?e[a]:h&&s?i(l,r):b&&g[a]==l?function(t){var n=function(n,e,r){if(this instanceof t){switch(arguments.length){case 0:return new t;case 1:return new t(n);case 2:return new t(n,e)}return new t(n,e,r)}return t.apply(this,arguments)};return n[f]=t[f],n}(l):y&&"function"==typeof l?i(Function.call,l):l,y&&((x.virtual||(x.virtual={}))[a]=l,t&c.R&&m&&!m[a]&&u(m,a,l)))};c.F=1,c.G=2,c.S=4,c.P=8,c.B=16,c.W=32,c.U=64,c.R=128,t.exports=c},function(t,n){var e={}.toString;t.exports=function(t){return e.call(t).slice(8,-1)}},function(t,n,e){var r=e(11),o=e(1).document,i=r(o)&&r(o.createElement);t.exports=function(t){return i?o.createElement(t):{}}},function(t,n,e){t.exports=!e(4)&&!e(8)(function(){return 7!=Object.defineProperty(e(29)("div"),"a",{get:function(){return 7}}).a})},function(t,n,e){"use strict";var r=e(18),o=e(27),i=e(36),u=e(5),f=e(2),c=e(17),a=e(51),s=e(20),l=e(58),p=e(7)("iterator"),d=!([].keys&&"next"in[].keys()),v="@@iterator",y="keys",h="values",b=function(){return this};t.exports=function(t,n,e,x,m,g,w){a(e,n,x);var O,S,_,j=function(t){if(!d&&t in A)return A[t];switch(t){case y:return function(){return new e(this,t)};case h:return function(){return new e(this,t)}}return function(){return new e(this,t)}},P=n+" Iterator",E=m==h,M=!1,A=t.prototype,T=A[p]||A[v]||m&&A[m],L=T||j(m),N=m?E?j("entries"):L:void 0,C="Array"==n?A.entries||T:T;if(C&&(_=l(C.call(new t)),_!==Object.prototype&&(s(_,P,!0),r||f(_,p)||u(_,p,b))),E&&T&&T.name!==h&&(M=!0,L=function(){return T.call(this)}),r&&!w||!d&&!M&&A[p]||u(A,p,L),c[n]=L,c[P]=b,m)if(O={values:E?L:j(h),keys:g?L:j(y),entries:N},w)for(S in O)S in A||i(A,S,O[S]);else o(o.P+o.F*(d||M),n,O);return O}},function(t,n,e){var r=e(10),o=e(55),i=e(16),u=e(21)("IE_PROTO"),f=function(){},c="prototype",a=function(){var t,n=e(29)("iframe"),r=i.length,o="<",u=">";for(n.style.display="none",e(48).appendChild(n),n.src="javascript:",t=n.contentWindow.document,t.open(),t.write(o+"script"+u+"document.F=Object"+o+"/script"+u),t.close(),a=t.F;r--;)delete a[c][i[r]];return a()};t.exports=Object.create||function(t,n){var e;return null!==t?(f[c]=r(t),e=new f,f[c]=null,e[u]=t):e=a(),void 0===n?e:o(e,n)}},function(t,n,e){var r=e(35),o=e(16).concat("length","prototype");n.f=Object.getOwnPropertyNames||function(t){return r(t,o)}},function(t,n){n.f=Object.getOwnPropertySymbols},function(t,n,e){var r=e(2),o=e(3),i=e(45)(!1),u=e(21)("IE_PROTO");t.exports=function(t,n){var e,f=o(t),c=0,a=[];for(e in f)e!=u&&r(f,e)&&a.push(e);for(;n.length>c;)r(f,e=n[c++])&&(~i(a,e)||a.push(e));return a}},function(t,n,e){t.exports=e(5)},function(t,n,e){var r=e(15);t.exports=function(t){return Object(r(t))}},function(t,n,e){t.exports={default:e(41),__esModule:!0}},function(t,n,e){t.exports={default:e(42),__esModule:!0}},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}n.__esModule=!0;var o=e(39),i=r(o),u=e(38),f=r(u),c="function"==typeof f.default&&"symbol"==typeof i.default?function(t){return typeof t}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":typeof t};n.default="function"==typeof f.default&&"symbol"===c(i.default)?function(t){return"undefined"==typeof t?"undefined":c(t)}:function(t){return t&&"function"==typeof f.default&&t.constructor===f.default&&t!==f.default.prototype?"symbol":"undefined"==typeof t?"undefined":c(t)}},function(t,n,e){e(65),e(63),e(66),e(67),t.exports=e(14).Symbol},function(t,n,e){e(64),e(68),t.exports=e(26).f("iterator")},function(t,n){t.exports=function(t){if("function"!=typeof t)throw TypeError(t+" is not a function!");return t}},function(t,n){t.exports=function(){}},function(t,n,e){var r=e(3),o=e(61),i=e(60);t.exports=function(t){return function(n,e,u){var f,c=r(n),a=o(c.length),s=i(u,a);if(t&&e!=e){for(;a>s;)if(f=c[s++],f!=f)return!0}else for(;a>s;s++)if((t||s in c)&&c[s]===e)return t||s||0;return!t&&-1}}},function(t,n,e){var r=e(43);t.exports=function(t,n,e){if(r(t),void 0===n)return t;switch(e){case 1:return function(e){return t.call(n,e)};case 2:return function(e,r){return t.call(n,e,r)};case 3:return function(e,r,o){return t.call(n,e,r,o)}}return function(){return t.apply(n,arguments)}}},function(t,n,e){var r=e(9),o=e(34),i=e(19);t.exports=function(t){var n=r(t),e=o.f;if(e)for(var u,f=e(t),c=i.f,a=0;f.length>a;)c.call(t,u=f[a++])&&n.push(u);return n}},function(t,n,e){t.exports=e(1).document&&document.documentElement},function(t,n,e){var r=e(28);t.exports=Object("z").propertyIsEnumerable(0)?Object:function(t){return"String"==r(t)?t.split(""):Object(t)}},function(t,n,e){var r=e(28);t.exports=Array.isArray||function(t){return"Array"==r(t)}},function(t,n,e){"use strict";var r=e(32),o=e(12),i=e(20),u={};e(5)(u,e(7)("iterator"),function(){return this}),t.exports=function(t,n,e){t.prototype=r(u,{next:o(1,e)}),i(t,n+" Iterator")}},function(t,n){t.exports=function(t,n){return{value:n,done:!!t}}},function(t,n,e){var r=e(9),o=e(3);t.exports=function(t,n){for(var e,i=o(t),u=r(i),f=u.length,c=0;f>c;)if(i[e=u[c++]]===n)return e}},function(t,n,e){var r=e(13)("meta"),o=e(11),i=e(2),u=e(6).f,f=0,c=Object.isExtensible||function(){return!0},a=!e(8)(function(){return c(Object.preventExtensions({}))}),s=function(t){u(t,r,{value:{i:"O"+ ++f,w:{}}})},l=function(t,n){if(!o(t))return"symbol"==typeof t?t:("string"==typeof t?"S":"P")+t;if(!i(t,r)){if(!c(t))return"F";if(!n)return"E";s(t)}return t[r].i},p=function(t,n){if(!i(t,r)){if(!c(t))return!0;if(!n)return!1;s(t)}return t[r].w},d=function(t){return a&&v.NEED&&c(t)&&!i(t,r)&&s(t),t},v=t.exports={KEY:r,NEED:!1,fastKey:l,getWeak:p,onFreeze:d}},function(t,n,e){var r=e(6),o=e(10),i=e(9);t.exports=e(4)?Object.defineProperties:function(t,n){o(t);for(var e,u=i(n),f=u.length,c=0;f>c;)r.f(t,e=u[c++],n[e]);return t}},function(t,n,e){var r=e(19),o=e(12),i=e(3),u=e(24),f=e(2),c=e(30),a=Object.getOwnPropertyDescriptor;n.f=e(4)?a:function(t,n){if(t=i(t),n=u(n,!0),c)try{return a(t,n)}catch(t){}if(f(t,n))return o(!r.f.call(t,n),t[n])}},function(t,n,e){var r=e(3),o=e(33).f,i={}.toString,u="object"==typeof window&&window&&Object.getOwnPropertyNames?Object.getOwnPropertyNames(window):[],f=function(t){try{return o(t)}catch(t){return u.slice()}};t.exports.f=function(t){return u&&"[object Window]"==i.call(t)?f(t):o(r(t))}},function(t,n,e){var r=e(2),o=e(37),i=e(21)("IE_PROTO"),u=Object.prototype;t.exports=Object.getPrototypeOf||function(t){return t=o(t),r(t,i)?t[i]:"function"==typeof t.constructor&&t instanceof t.constructor?t.constructor.prototype:t instanceof Object?u:null}},function(t,n,e){var r=e(23),o=e(15);t.exports=function(t){return function(n,e){var i,u,f=String(o(n)),c=r(e),a=f.length;return c<0||c>=a?t?"":void 0:(i=f.charCodeAt(c),i<55296||i>56319||c+1===a||(u=f.charCodeAt(c+1))<56320||u>57343?t?f.charAt(c):i:t?f.slice(c,c+2):(i-55296<<10)+(u-56320)+65536)}}},function(t,n,e){var r=e(23),o=Math.max,i=Math.min;t.exports=function(t,n){return t=r(t),t<0?o(t+n,0):i(t,n)}},function(t,n,e){var r=e(23),o=Math.min;t.exports=function(t){return t>0?o(r(t),9007199254740991):0}},function(t,n,e){"use strict";var r=e(44),o=e(52),i=e(17),u=e(3);t.exports=e(31)(Array,"Array",function(t,n){this._t=u(t),this._i=0,this._k=n},function(){var t=this._t,n=this._k,e=this._i++;return!t||e>=t.length?(this._t=void 0,o(1)):"keys"==n?o(0,e):"values"==n?o(0,t[e]):o(0,[e,t[e]])},"values"),i.Arguments=i.Array,r("keys"),r("values"),r("entries")},function(t,n){},function(t,n,e){"use strict";var r=e(59)(!0);e(31)(String,"String",function(t){this._t=String(t),this._i=0},function(){var t,n=this._t,e=this._i;return e>=n.length?{value:void 0,done:!0}:(t=r(n,e),this._i+=t.length,{value:t,done:!1})})},function(t,n,e){"use strict";var r=e(1),o=e(2),i=e(4),u=e(27),f=e(36),c=e(54).KEY,a=e(8),s=e(22),l=e(20),p=e(13),d=e(7),v=e(26),y=e(25),h=e(53),b=e(47),x=e(50),m=e(10),g=e(3),w=e(24),O=e(12),S=e(32),_=e(57),j=e(56),P=e(6),E=e(9),M=j.f,A=P.f,T=_.f,L=r.Symbol,N=r.JSON,C=N&&N.stringify,k="prototype",F=d("_hidden"),q=d("toPrimitive"),I={}.propertyIsEnumerable,B=s("symbol-registry"),D=s("symbols"),W=s("op-symbols"),H=Object[k],K="function"==typeof L,R=r.QObject,J=!R||!R[k]||!R[k].findChild,U=i&&a(function(){return 7!=S(A({},"a",{get:function(){return A(this,"a",{value:7}).a}})).a})?function(t,n,e){var r=M(H,n);r&&delete H[n],A(t,n,e),r&&t!==H&&A(H,n,r)}:A,G=function(t){var n=D[t]=S(L[k]);return n._k=t,n},$=K&&"symbol"==typeof L.iterator?function(t){return"symbol"==typeof t}:function(t){return t instanceof L},z=function(t,n,e){return t===H&&z(W,n,e),m(t),n=w(n,!0),m(e),o(D,n)?(e.enumerable?(o(t,F)&&t[F][n]&&(t[F][n]=!1),e=S(e,{enumerable:O(0,!1)})):(o(t,F)||A(t,F,O(1,{})),t[F][n]=!0),U(t,n,e)):A(t,n,e)},Y=function(t,n){m(t);for(var e,r=b(n=g(n)),o=0,i=r.length;i>o;)z(t,e=r[o++],n[e]);return t},Q=function(t,n){return void 0===n?S(t):Y(S(t),n)},X=function(t){var n=I.call(this,t=w(t,!0));return!(this===H&&o(D,t)&&!o(W,t))&&(!(n||!o(this,t)||!o(D,t)||o(this,F)&&this[F][t])||n)},V=function(t,n){if(t=g(t),n=w(n,!0),t!==H||!o(D,n)||o(W,n)){var e=M(t,n);return!e||!o(D,n)||o(t,F)&&t[F][n]||(e.enumerable=!0),e}},Z=function(t){for(var n,e=T(g(t)),r=[],i=0;e.length>i;)o(D,n=e[i++])||n==F||n==c||r.push(n);return r},tt=function(t){for(var n,e=t===H,r=T(e?W:g(t)),i=[],u=0;r.length>u;)!o(D,n=r[u++])||e&&!o(H,n)||i.push(D[n]);return i};K||(L=function(){if(this instanceof L)throw TypeError("Symbol is not a constructor!");var t=p(arguments.length>0?arguments[0]:void 0),n=function(e){this===H&&n.call(W,e),o(this,F)&&o(this[F],t)&&(this[F][t]=!1),U(this,t,O(1,e))};return i&&J&&U(H,t,{configurable:!0,set:n}),G(t)},f(L[k],"toString",function(){return this._k}),j.f=V,P.f=z,e(33).f=_.f=Z,e(19).f=X,e(34).f=tt,i&&!e(18)&&f(H,"propertyIsEnumerable",X,!0),v.f=function(t){return G(d(t))}),u(u.G+u.W+u.F*!K,{Symbol:L});for(var nt="hasInstance,isConcatSpreadable,iterator,match,replace,search,species,split,toPrimitive,toStringTag,unscopables".split(","),et=0;nt.length>et;)d(nt[et++]);for(var nt=E(d.store),et=0;nt.length>et;)y(nt[et++]);u(u.S+u.F*!K,"Symbol",{for:function(t){return o(B,t+="")?B[t]:B[t]=L(t)},keyFor:function(t){if($(t))return h(B,t);throw TypeError(t+" is not a symbol!")},useSetter:function(){J=!0},useSimple:function(){J=!1}}),u(u.S+u.F*!K,"Object",{create:Q,defineProperty:z,defineProperties:Y,getOwnPropertyDescriptor:V,getOwnPropertyNames:Z,getOwnPropertySymbols:tt}),N&&u(u.S+u.F*(!K||a(function(){var t=L();return"[null]"!=C([t])||"{}"!=C({a:t})||"{}"!=C(Object(t))})),"JSON",{stringify:function(t){if(void 0!==t&&!$(t)){for(var n,e,r=[t],o=1;arguments.length>o;)r.push(arguments[o++]);return n=r[1],"function"==typeof n&&(e=n),!e&&x(n)||(n=function(t,n){if(e&&(n=e.call(this,t,n)),!$(n))return n}),r[1]=n,C.apply(N,r)}}}),L[k][q]||e(5)(L[k],q,L[k].valueOf),l(L,"Symbol"),l(Math,"Math",!0),l(r.JSON,"JSON",!0)},function(t,n,e){e(25)("asyncIterator")},function(t,n,e){e(25)("observable")},function(t,n,e){e(62);for(var r=e(1),o=e(5),i=e(17),u=e(7)("toStringTag"),f=["NodeList","DOMTokenList","MediaList","StyleSheetList","CSSRuleList"],c=0;c<5;c++){var a=f[c],s=r[a],l=s&&s.prototype;l&&!l[u]&&o(l,u,a),i[a]=i.Array}},function(t,n){"use strict";var e={versions:function(){var t=window.navigator.userAgent;return{trident:t.indexOf("Trident")>-1,presto:t.indexOf("Presto")>-1,webKit:t.indexOf("AppleWebKit")>-1,gecko:t.indexOf("Gecko")>-1&&t.indexOf("KHTML")==-1,mobile:!!t.match(/AppleWebKit.*Mobile.*/),ios:!!t.match(/\(i[^;]+;( U;)? CPU.+Mac OS X/),android:t.indexOf("Android")>-1||t.indexOf("Linux")>-1,iPhone:t.indexOf("iPhone")>-1||t.indexOf("Mac")>-1,iPad:t.indexOf("iPad")>-1,webApp:t.indexOf("Safari")==-1,weixin:t.indexOf("MicroMessenger")==-1}}()};t.exports=e},function(t,n,e){"use strict";function r(t){return t&&t.__esModule?t:{default:t}}var o=e(40),i=r(o),u=function(){function t(t,n,e){return n||e?String.fromCharCode(n||e):o[t]||t}function n(t){return l[t]}var e=/&quot;|&lt;|&gt;|&amp;|&nbsp;|&apos;|&#(\d+);|&#(\d+)/g,r=/['<> "&]/g,o={"&quot;":'"',"&lt;":"<","&gt;":">","&amp;":"&","&nbsp;":" "},f=/\u00a0/g,c=/<br\s*\/?>/gi,a=/\r?\n/g,s=/\s/g,l={};for(var p in o)l[o[p]]=p;return o["&apos;"]="'",l["'"]="&#39;",{encode:function(t){return t?(""+t).replace(r,n).replace(a,"<br/>").replace(s,"&nbsp;"):""},decode:function(n){return n?(""+n).replace(c,"\n").replace(e,t).replace(f," "):""},encodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},encodeBase16forJSON:function(t){if(!t)return t;t=t.replace(/[\u4E00-\u9FBF]/gi,function(t){return escape(t).replace("%u","\\u")});for(var n=[],e=0,r=t.length;r>e;e++)n.push(t.charCodeAt(e).toString(16).toUpperCase());return n.join("")},decodeBase16:function(t){if(!t)return t;t+="";for(var n=[],e=0,r=t.length;r>e;e+=2)n.push(String.fromCharCode("0x"+t.slice(e,e+2)));return n.join("")},encodeObject:function(t){if(t instanceof Array)for(var n=0,e=t.length;e>n;n++)t[n]=u.encodeObject(t[n]);else if("object"==("undefined"==typeof t?"undefined":(0,i.default)(t)))for(var r in t)t[r]=u.encodeObject(t[r]);else if("string"==typeof t)return u.encode(t);return t},loadScript:function(t){var n=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(n),n.setAttribute("src",t)},addLoadEvent:function(t){var n=window.onload;"function"!=typeof window.onload?window.onload=t:window.onload=function(){n(),t()}}}}();t.exports=u},function(t,n){function e(t,n){t.classList?t.classList.add(n):t.className+=" "+n}t.exports=e},function(t,n){function e(t,n){if(t.classList)t.classList.remove(n);else{var e=new RegExp("(^|\\b)"+n.split(" ").join("|")+"(\\b|$)","gi");t.className=t.className.replace(e," ")}}t.exports=e},,,function(t,n){"use strict";function e(){var t=document.querySelector("#page-nav");if(t&&!document.querySelector("#page-nav .extend.prev")&&(t.innerHTML='<a class="extend prev disabled" rel="prev">&laquo; Prev</a>'+t.innerHTML),t&&!document.querySelector("#page-nav .extend.next")&&(t.innerHTML=t.innerHTML+'<a class="extend next disabled" rel="next">Next &raquo;</a>'),yiliaConfig&&yiliaConfig.open_in_new){var n=document.querySelectorAll(".article-entry a:not(.article-more-a)");n.forEach(function(t){t.setAttribute("target","_blank")})}var e=document.querySelector("#js-aboutme");e&&0!==e.length&&(e.innerHTML=e.innerText)}t.exports={init:e}},,,,,,,,,function(t,n){function e(t,n){if("string"==typeof n)return t.insertAdjacentHTML("afterend",n);var e=t.nextSibling;return e?t.parentNode.insertBefore(n,e):t.parentNode.appendChild(n)}t.exports=e}])</script><script src="/./main.234bc0.js"></script><script>!function(){var e=function(e){var t=document.createElement("script");document.getElementsByTagName("body")[0].appendChild(t),t.setAttribute("src",e)};e("/slider.885efe.js")}()</script>


    
<div class="tools-col" q-class="show:isShow,hide:isShow|isFalse" q-on="click:stop(e)">
  <div class="tools-nav header-menu">
    
    
      
      
      
    
      
      
      
    
      
      
      
    
    

    <ul style="width: 70%">
    
    
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'innerArchive')"><a href="javascript:void(0)" q-class="active:innerArchive">所有文章</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'friends')"><a href="javascript:void(0)" q-class="active:friends">友链</a></li>
      
        
      
      <li style="width: 33.333333333333336%" q-on="click: openSlider(e, 'aboutme')"><a href="javascript:void(0)" q-class="active:aboutme">关于我</a></li>
      
        
    </ul>
  </div>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all" q-show="innerArchive">
        <div class="search-wrap">
          <input class="search-ipt" q-model="search" type="text" placeholder="find something…">
          <i class="icon-search icon" q-show="search|isEmptyStr"></i>
          <i class="icon-close icon" q-show="search|isNotEmptyStr" q-on="click:clearChose(e)"></i>
        </div>
        <div class="widget tagcloud search-tag">
          <p class="search-tag-wording">tag:</p>
          <label class="search-switch">
            <input type="checkbox" q-on="click:toggleTag(e)" q-attr="checked:showTags">
          </label>
          <ul class="article-tag-list" q-show="showTags">
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color2">Python</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color5">机器学习</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">翻墙</a>
              </li>
             
              <li class="article-tag-list-item">
                <a href="javascript:void(0)" class="js-tag color3">随笔</a>
              </li>
            
            <div class="clearfix"></div>
          </ul>
        </div>
        <ul class="search-ul">
          <p q-show="jsonFail" style="padding: 20px; font-size: 12px;">
            缺失模块。<br/>1、在博客根目录（注意不是yilia根目录）执行以下命令：<br/> npm i hexo-generator-json-content --save<br/><br/>
            2、在根目录_config.yml里添加配置：
<pre style="font-size: 12px;" q-show="jsonFail">
  jsonContent:
    meta: false
    pages: false
    posts:
      title: true
      date: true
      path: true
      text: true
      raw: false
      content: false
      slug: false
      updated: false
      comments: false
      link: false
      permalink: false
      excerpt: false
      categories: false
      tags: true
</pre>
          </p>
          <li class="search-li" q-repeat="items" q-show="isShow">
            <a q-attr="href:path|urlformat" class="search-title"><i class="icon-quo-left icon"></i><span q-text="title"></span></a>
            <p class="search-time">
              <i class="icon-calendar icon"></i>
              <span q-text="date|dateformat"></span>
            </p>
            <p class="search-tag">
              <i class="icon-price-tags icon"></i>
              <span q-repeat="tags" q-on="click:choseTag(e, name)" q-text="name|tagformat"></span>
            </p>
          </li>
        </ul>
    	</section>
    

    
    	<section class="tools-section tools-section-friends" q-show="friends">
  		
        <ul class="search-ul">
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接1</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接2</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接3</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接4</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接5</a>
            </li>
          
            <li class="search-li">
              <a href="http://localhost:4000/" target="_blank" class="search-title"><i class="icon-quo-left icon"></i>友情链接6</a>
            </li>
          
        </ul>
  		
    	</section>
    

    
    	<section class="tools-section tools-section-me" q-show="aboutme">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">研究生一枚</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>